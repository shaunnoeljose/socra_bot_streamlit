{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39c888c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3c7bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MESSAGES_IN_CONTEXT = 10\n",
    "# Defining the class of the agent\n",
    "class Agent(TypedDict):\n",
    "    \"\"\"\n",
    "    Defines the state of the agent in the conversation\n",
    "    messages: A list of chat messages exchanged so far.\n",
    "    difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "    user_struggle_count: Counter for consecutive times the user struggles.\n",
    "    topic: The current Python topic being discussed.\n",
    "    sub_topic: The specific sub-topic within the main topic.\n",
    "    mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "    mcq_question: The active MCQ question text.\n",
    "    mcq_options: List of options for the active MCQ.\n",
    "    mcq_correct_answer: The correct answer for the active MCQ.\n",
    "    agent_thought: The last thought process articulated by the Socratic agent.\n",
    "    next_node: str # The next node the supervisor has decided to route to\n",
    "    tool_input: dict # Input arguments for the tool if a tool is routed to        \n",
    "    \"\"\"\n",
    "    \n",
    "    messages: Annotated[List[BaseMessage], add_messages] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node: str\n",
    "    tool_input: dict\n",
    "\n",
    "# Initialize the Gemini LLM for the Socratic Agent\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac41b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tools performing the tasks\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code:str)->str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies issues, suggests improvements, and provides feedback. \n",
    "    Use this when the user provides code and asks for review or debugging. The output will be used by the Socratic agent to ask questions.\n",
    "    \"\"\"\n",
    "    return f\"Code Analysis Result: For the code snippet '{code}', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    The output is raw explanation, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    return f\"Explanation Result: The concept of '{concept}' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it describes repetitive execution. If it's 'objects', it's about data and behavior bundling.\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    The output is the challenge, which the Socratic agent will present.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    # logger.info(f\"Executing Challenge Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    return f\"Challenge Result: For '{topic}' at '{difficulty}' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specifically for MCQ generation.\n",
    "    # logger.info(f\"Executing MCQ Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    mcq_data = {\n",
    "        \"question\": f\"Which of the following operations would lead to an `IndentationError` in Python?\",\n",
    "        \"options\": [\"A) Missing a colon after a function definition\", \"B) Inconsistent use of spaces and tabs for indentation\", \"C) Using a reserved keyword as a variable name\", \"D) Forgetting a closing parenthesis\"],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "    return json.dumps(mcq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69624cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_facing_tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent]\n",
    "\n",
    "@tool\n",
    "def route_to_socratic_question(query: str = None) -> str:\n",
    "    \"\"\"Routes the conversation to the main Socratic Questioning agent for general teaching or follow-up.\n",
    "    This is the default route for general queries, concept discussions, and after tool outputs.\n",
    "    Optionally includes a follow-up query for the Socratic agent if the intent is specific.\n",
    "    \"\"\"\n",
    "    return \"socratic_question\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_analysis(code: str) -> str:\n",
    "    \"\"\"Routes to the Code Analysis agent for debugging or code review. Requires the code snippet.\"\"\"\n",
    "    return \"code_analysis\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_explanation(concept: str) -> str:\n",
    "    \"\"\"Routes to the Code Explanation agent to explain a specific concept, keyword, or error. Requires the concept.\"\"\"\n",
    "    return \"code_explanation\"\n",
    "\n",
    "@tool\n",
    "def route_to_challenge_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the Challenge Generator agent to create a coding challenge. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"challenge_generator\"\n",
    "\n",
    "@tool\n",
    "def route_to_mcq_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the MCQ Generator agent to create a multiple-choice question. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"mcq_generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "383a8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all internal routing tools available to the Supervisor\n",
    "supervisor_routing_tools = [\n",
    "    route_to_socratic_question,\n",
    "    route_to_code_analysis,\n",
    "    route_to_code_explanation,\n",
    "    route_to_challenge_generator,\n",
    "    route_to_mcq_generator\n",
    "]\n",
    "\n",
    "# Supervisor Agent Setup\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a highly intelligent routing agent for a Socratic Python Tutor. Your task is to analyze\n",
    "the user's last message and the conversation history to determine the most appropriate next step\n",
    "in the learning process.\n",
    "\n",
    "You have access to several internal routing tools. Call exactly one of these tools to specify\n",
    "which specialized agent or flow should handle the user's request.\n",
    "\n",
    "Here are your routing rules:\n",
    "-   **Default:** For general questions, learning new topics, or continuing a Socratic dialogue, use `route_to_socratic_question`. This should be your most frequent choice.\n",
    "-   **Code Analysis:** If the user provides Python code and asks for debugging, feedback, review, or analysis, use `route_to_code_analysis` and pass the code.\n",
    "-   **Code Explanation:** If the user explicitly asks for an explanation of a specific Python concept, keyword, function, or error message, use `route_to_code_explanation` and pass the concept.\n",
    "-   **Challenge/Exercise:** If the user explicitly asks for a coding challenge, exercise, or fill-in-the-blanks, use `route_to_challenge_generator`.\n",
    "-   **MCQ:** If the user asks for a multiple-choice question or you determine an MCQ is a good way to test their understanding, use `route_to_mcq_generator`.\n",
    "\n",
    "Pay close attention to keywords and the overall intent. Your response MUST be a tool call.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "supervisor_runnable = supervisor_prompt | llm.bind_tools(supervisor_routing_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65a64dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Socratic Agent Setup (This is our main Socratic Questioning LLM)\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Interpret Tool Outputs Socratically:** If a tool provides information (e.g., Code Analysis Result, Explanation Result, Challenge Result), your task is to *process that information* and turn it into a Socratic question or guided step for the user. Do not just relay the tool's output directly.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub-topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture (Internal Thought):** Before responding, articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". This thought will be logged but not shown to the user. Then, proceed with your Socratic question.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "socratic_agent_runnable = socratic_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2426ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_supervisor(state: Agent):\n",
    "    \"\"\"\n",
    "    Node for the supervisor to determine the next action/agent based on user intent.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Supervisor node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    # Pass the full state to the prompt for contextual awareness in routing\n",
    "    response = supervisor_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"]\n",
    "    })\n",
    "\n",
    "    # The supervisor is expected to call one of its internal routing tools.\n",
    "    # Extract the tool call and set the next_node and tool_input in the state.\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0] # Assuming supervisor calls one tool\n",
    "        next_node = tool_call.name.replace(\"route_to_\", \"\") # Extract node name (e.g., \"socratic_question\")\n",
    "        tool_input = tool_call.args\n",
    "        # logger.info(f\"Supervisor decided to route to: {next_node} with input: {tool_input}\") # Uncomment when logger is ready\n",
    "        return {\"messages\": [response], \"next_node\": next_node, \"tool_input\": tool_input}\n",
    "    else:\n",
    "        # Fallback if supervisor doesn't call a tool (shouldn't happen with proper prompt)\n",
    "        # Force it to the socratic question node\n",
    "        # logger.warning(\"Supervisor did not call a tool. Defaulting to socratic_question.\") # Uncomment when logger is ready\n",
    "        return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "\n",
    "def socratic_question_node(state: Agent):\n",
    "    \"\"\"\n",
    "    Node for the main Socratic LLM to ask questions or interpret tool outputs.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Socratic Question Node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    # Extract thought (for logging)\n",
    "    thought = \"\"\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            thought = parts[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    return {\"messages\": [response], \"agent_thought\": thought}\n",
    "\n",
    "\n",
    "def call_specialized_tool_node(state: Agent):\n",
    "    \"\"\"\n",
    "    Node to execute a specialized user-facing tool based on supervisor's decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Call Specialized Tool Node activated for {state['next_node']}\") # Uncomment when logger is ready\n",
    "    tool_name = state[\"next_node\"] # This comes from the supervisor's routing decision\n",
    "    tool_args = state[\"tool_input\"]\n",
    "\n",
    "    # Manually find and execute the tool function\n",
    "    tool_function = next((t for t in user_facing_tools if t.name == tool_name), None)\n",
    "\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            tool_output = tool_function(**tool_args)\n",
    "            # logger.info(f\"Tool '{tool_name}' executed. Output: {tool_output[:100]}...\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "            # logger.error(f\"Error executing tool {tool_name}: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "        # logger.error(f\"Specialized tool '{tool_name}' not found.\") # Uncomment when logger is ready\n",
    "\n",
    "    # Add the tool output as a ToolMessage to the conversation history\n",
    "    # The socratic_question_node will then process this and ask a question.\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name)]}\n",
    "\n",
    "\n",
    "def generate_mcq_node(state: Agent):\n",
    "    \"\"\"\n",
    "    Node specifically for generating an MCQ via the mcq_agent tool.\n",
    "    This also handles setting the MCQ active state for main.py.\n",
    "    \"\"\"\n",
    "    # logger.info(\"MCQ Generation Node activated.\") # Uncomment when logger is ready\n",
    "    tool_name = \"mcq_agent\"\n",
    "    tool_args = state[\"tool_input\"] # Should contain topic and difficulty from supervisor\n",
    "\n",
    "    tool_function = next((t for t in user_facing_tools if t.name == tool_name), None)\n",
    "    mcq_raw_output = \"\"\n",
    "\n",
    "    if tool_function:\n",
    "        try:\n",
    "            mcq_raw_output = tool_function(**tool_args)\n",
    "            mcq_data = json.loads(mcq_raw_output)\n",
    "            state[\"mcq_active\"] = True\n",
    "            state[\"mcq_question\"] = mcq_data[\"question\"]\n",
    "            state[\"mcq_options\"] = mcq_data[\"options\"]\n",
    "            state[\"mcq_correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "            # logger.info(\"MCQ details updated in state.\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            mcq_raw_output = f\"Error generating MCQ: {e}\"\n",
    "            # logger.error(f\"Error generating MCQ: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    \n",
    "    # Add a ToolMessage for the MCQ generation, which the Socratic LLM can interpret\n",
    "    # or simply for logging purposes in the graph flow.\n",
    "    return {\"messages\": [ToolMessage(content=mcq_raw_output, name=tool_name)], **state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18ae615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def route_supervisor_output(state: Agent):\n",
    "    \"\"\"\n",
    "    Conditional edge from the supervisor to determine the next node based on its decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Routing supervisor output. Next node: {state['next_node']}\") # Uncomment when logger is ready\n",
    "    if state[\"next_node\"] == \"socratic_question\":\n",
    "        return \"socratic_question_node\"\n",
    "    elif state[\"next_node\"] == \"mcq_generator\":\n",
    "        return \"generate_mcq_node\"\n",
    "    # All other specialized tools go through the generic tool calling node\n",
    "    elif state[\"next_node\"] in [\"code_analysis\", \"code_explanation\", \"challenge_generator\"]:\n",
    "        return \"call_specialized_tool_node\"\n",
    "    return \"socratic_question_node\" # Fallback to socratic question if unexpected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83b58b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(Agent)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_supervisor\", call_supervisor)\n",
    "workflow.add_node(\"socratic_question_node\", socratic_question_node) # Renamed from call_llm\n",
    "workflow.add_node(\"call_specialized_tool_node\", call_specialized_tool_node)\n",
    "workflow.add_node(\"generate_mcq_node\", generate_mcq_node)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_supervisor\")\n",
    "\n",
    "# Define the edges.\n",
    "# From supervisor, route conditionally\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_supervisor\",\n",
    "    route_supervisor_output,\n",
    "    {\n",
    "        \"socratic_question_node\": \"socratic_question_node\",\n",
    "        \"call_specialized_tool_node\": \"call_specialized_tool_node\",\n",
    "        \"generate_mcq_node\": \"generate_mcq_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After a specialized tool (other than MCQ), return to the socratic_question_node\n",
    "# for the Socratic LLM to interpret the tool's output and formulate a question.\n",
    "workflow.add_edge(\"call_specialized_tool_node\", \"socratic_question_node\")\n",
    "\n",
    "# After the socratic_question_node, the run ends. The main.py loop will then take user input.\n",
    "workflow.add_edge(\"socratic_question_node\", END)\n",
    "\n",
    "# After generating an MCQ, the run ends. Main.py handles the MCQ display and user input.\n",
    "workflow.add_edge(\"generate_mcq_node\", END)\n",
    "\n",
    "# Initialize MemorySaver\n",
    "# This will save conversation state to a local file system by default.\n",
    "# checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the graph into a runnable agent with the checkpointer.\n",
    "socratic_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd8e7ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x75580f3af1d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socratic_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "377dacc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Node: call_supervisor ---\n",
      "Node 'call_supervisor' Test FAILED: 'dict' object has no attribute 'name'\n",
      "--- End Node Test: call_supervisor ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12503/4231620347.py\", line 30, in _test_node_functionality\n",
      "    result_state_update = node_runnable.invoke(initial_state)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.python/current/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 377, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12503/542529847.py\", line 20, in call_supervisor\n",
      "    next_node = tool_call.name.replace(\"route_to_\", \"\") # Extract node name (e.g., \"socratic_question\")\n",
      "                ^^^^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# --- Temporary LLM Connection Test ---\n",
    "# This function will be called once when the module is imported to test LLM connectivity.\n",
    "def _test_llm_connection():\n",
    "    print(\"\\n--- Testing LLM Connection (Temporary) ---\")\n",
    "    try:\n",
    "        test_message = HumanMessage(content=\"Say hi!\")\n",
    "        response = llm.invoke([test_message])\n",
    "        print(f\"LLM Test Response: {response.content}\")\n",
    "        print(\"LLM connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Connection Test FAILED: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and network connection.\")\n",
    "    print(\"--- End LLM Connection Test ---\\n\")\n",
    "\n",
    "# --- Temporary Node Functionality Test ---\n",
    "# This function allows testing individual nodes with a sample state.\n",
    "def _test_node_functionality(node_name: str, initial_state: Agent):\n",
    "    print(f\"\\n--- Testing Node: {node_name} ---\")\n",
    "    try:\n",
    "        # Get the node function from the workflow's nodes\n",
    "        # Access the runnable attribute to get the actual function\n",
    "        # This is the fix for 'StateNodeSpec' object is not callable\n",
    "        node_runnable = workflow.nodes[node_name].runnable\n",
    "\n",
    "        # Invoke the node's runnable with the provided initial state\n",
    "        # For nodes that are LLMChain or similar, invoke with a dict matching their input schema\n",
    "        # For simple functions, it might be direct call.\n",
    "        # Here, we assume the node_runnable expects the full state dict as input,\n",
    "        # which is how LangGraph nodes are typically defined.\n",
    "        result_state_update = node_runnable.invoke(initial_state)\n",
    "\n",
    "\n",
    "        print(f\"Node '{node_name}' executed successfully.\")\n",
    "        print(f\"Initial State (messages only): {[msg.content if hasattr(msg, 'content') else str(msg) for msg in initial_state['messages']]}\")\n",
    "        print(f\"Resulting State Update: {result_state_update}\")\n",
    "\n",
    "        # For nodes that return messages, let's print them\n",
    "        if 'messages' in result_state_update and result_state_update['messages']:\n",
    "            print(\"Messages returned by node:\")\n",
    "            for msg in result_state_update['messages']:\n",
    "                if isinstance(msg, BaseMessage):\n",
    "                    print(f\"  - Type: {msg.type}, Content: {msg.content[:50]}...\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        # Ensure tool_calls are correctly converted to ToolCall objects\n",
    "                        # This part of the debug print itself needs to be robust\n",
    "                        for tc in msg.tool_calls:\n",
    "                            if isinstance(tc, ToolCall):\n",
    "                                print(f\"    Tool Call: {tc.function.name} with args {tc.function.arguments}\")\n",
    "                            else:\n",
    "                                print(f\"    Raw Tool Call (still dict?): {tc}\")\n",
    "                else:\n",
    "                    print(f\"  - Raw: {msg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Node '{node_name}' Test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for node errors\n",
    "    print(f\"--- End Node Test: {node_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Call the LLM connection test immediately when this module is loaded\n",
    "# _test_llm_connection()\n",
    "\n",
    "# Example usage of the new node testing function (uncomment to test specific nodes)\n",
    "# Ensure your GOOGLE_API_KEY is set for LLM-based nodes.\n",
    "\n",
    "# Test call_supervisor node\n",
    "sample_supervisor_state = Agent(\n",
    "    messages=[HumanMessage(content=\"I need help with debugging this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Introduction\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"call_supervisor\", sample_supervisor_state)\n",
    "\n",
    "# Test socratic_question_node\n",
    "# sample_socratic_state = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"What are variables?\"), AIMessage(content=\"Thought: User asked about variables. I should ask a foundational question. What is your current understanding of variables in programming?\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Variables\",\n",
    "#     sub_topic=\"Definition\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\", next_node=\"\", tool_input={}\n",
    "# )\n",
    "# _test_node_functionality(\"socratic_question_node\", sample_socratic_state)\n",
    "\n",
    "# Test call_specialized_tool_node (e.g., code_analysis)\n",
    "# This requires the supervisor to have set next_node and tool_input previously.\n",
    "# For a direct test, we simulate that state.\n",
    "# sample_tool_state_code_analysis = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"debug this code: print('hello')\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Debugging\",\n",
    "#     sub_topic=\"Code Analysis\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\",\n",
    "#     next_node=\"code_analysis\", # Supervisor would set this\n",
    "#     tool_input={\"code\": \"def my_func():\\n    pass\"} # Supervisor would set this\n",
    "# )\n",
    "# _test_node_functionality(\"call_specialized_tool_node\", sample_tool_state_code_analysis)\n",
    "\n",
    "# Test generate_mcq_node\n",
    "# sample_mcq_state = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"Give me an MCQ on functions.\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Functions\",\n",
    "#     sub_topic=\"MCQ\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\",\n",
    "#     next_node=\"mcq_generator\", # Supervisor would set this\n",
    "#     tool_input={\"topic\": \"functions\", \"difficulty\": \"beginner\"} # Supervisor would set this\n",
    "# )\n",
    "# _test_node_functionality(\"generate_mcq_node\", sample_mcq_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM Connection (Temporary) ---\n",
      "LLM Test Response: Hi there! How can I help you today?\n",
      "LLM connection successful!\n",
      "--- End LLM Connection Test ---\n",
      "\n",
      "\n",
      "--- Testing Node: call_supervisor ---\n",
      "Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\n",
      "Node 'call_supervisor' executed successfully.\n",
      "Initial State (messages only): [\"I need help with debugging this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'route_to_code_analysis', 'arguments': '{\"code\": \"print(\\'hello\\')\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--c1cfbe78-9ad4-445a-8694-e1a65bb92245-0', tool_calls=[{'name': 'route_to_code_analysis', 'args': {'code': \"print('hello')\"}, 'id': 'da8b1db0-9e19-4280-92cd-9da2d59a71c1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 533, 'output_tokens': 12, 'total_tokens': 545, 'input_token_details': {'cache_read': 0}})], 'next_node': 'socratic_question'}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: ...\n",
      "Node 'call_supervisor' Test FAILED: 'dict' object has no attribute 'function'\n",
      "--- End Node Test: call_supervisor ---\n",
      "\n",
      "\n",
      "--- Testing Node: socratic_question_node ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12503/1055099750.py\", line 493, in _test_node_functionality\n",
      "    tool_name_debug = tc.function.name\n",
      "                      ^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'function'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'socratic_question_node' executed successfully.\n",
      "Initial State (messages only): ['What are variables?', 'User asked about variables. I should ask a foundational question. What is your current understanding of variables in programming?']\n",
      "Resulting State Update: {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a26dc600-94b7-4473-8e7f-ca6fad2003f8-0', usage_metadata={'input_tokens': 386, 'output_tokens': 0, 'total_tokens': 386, 'input_token_details': {'cache_read': 0}})], 'agent_thought': ''}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: ...\n",
      "--- End Node Test: socratic_question_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_analysis_node ---\n",
      "Node 'code_analysis_node' executed successfully.\n",
      "Initial State (messages only): [\"debug this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Code Analysis Result: For the code snippet 'def my_func():\\n    pass', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\", name='code_analysis_agent', tool_call_id='43d13a0a-2796-4bcb-b2e1-1b9992859168')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Code Analysis Result: For the code snippet 'def my...\n",
      "--- End Node Test: code_analysis_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_explanation_node ---\n",
      "Node 'code_explanation_node' executed successfully.\n",
      "Initial State (messages only): ['Explain loops in Python']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Explanation Result: The concept of 'loops' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\", name='code_explanation_agent', tool_call_id='40d59969-02cd-48e7-baf5-34f2ea3dc27a')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Explanation Result: The concept of 'loops' in Pyth...\n",
      "--- End Node Test: code_explanation_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: challenge_generator_node ---\n",
      "Node 'challenge_generator_node' executed successfully.\n",
      "Initial State (messages only): ['Give me a challenge on functions.']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Challenge Result: For 'functions' at 'intermediate' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\", name='challenge_generator_agent', tool_call_id='b4817f84-2b8d-4a47-af4d-356cf9de8d5c')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Challenge Result: For 'functions' at 'intermediate...\n",
      "--- End Node Test: challenge_generator_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: generate_mcq_node ---\n",
      "Node 'generate_mcq_node' executed successfully.\n",
      "Initial State (messages only): ['Give me an MCQ on functions.']\n",
      "Resulting State Update: {'messages': [HumanMessage(content='Give me an MCQ on functions.', additional_kwargs={}, response_metadata={})], 'difficulty_level': 'beginner', 'user_struggle_count': 0, 'topic': 'Functions', 'sub_topic': 'MCQ', 'mcq_active': True, 'mcq_question': 'Which of the following operations would lead to an `IndentationError` in Python?', 'mcq_options': ['A) Missing a colon after a function definition', 'B) Inconsistent use of spaces and tabs for indentation', 'C) Using a reserved keyword as a variable name', 'D) Forgetting a closing parenthesis'], 'mcq_correct_answer': 'B', 'agent_thought': '', 'next_node': 'mcq_agent', 'tool_input': {'topic': 'functions', 'difficulty': 'beginner'}}\n",
      "Messages returned by node:\n",
      "  - Type: human, Content: Give me an MCQ on functions....\n",
      "--- End Node Test: generate_mcq_node ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool # Ensure tool decorator is imported\n",
    "import json\n",
    "import uuid # Import uuid for generating unique IDs\n",
    "\n",
    "# Removed MemorySaver import\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Import the logging utility (assuming logger.py will be created later)\n",
    "# from logger import setup_logger\n",
    "# logger = setup_logger() # Uncomment when logger.py is ready\n",
    "\n",
    "# --- Configuration for Memory Management ---\n",
    "MAX_MESSAGES_IN_CONTEXT = 10 # Keep the last 10 messages in the context window\n",
    "# This includes both HumanMessage and AIMessage. Adjust as needed based on LLM context limits.\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "        # Added for supervisor routing\n",
    "        next_node: str # The next node the supervisor has decided to route to\n",
    "        tool_input: dict # Input arguments for the tool if a tool is routed to\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], lambda x, y: x + y] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node: str\n",
    "    tool_input: dict\n",
    "\n",
    "\n",
    "# --- 2. Initialize the LLMs and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM (used for both Socratic and Supervisor agents)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "# --- Define User-Facing Simulated Agent Tools ---\n",
    "# These are the actual tools that will perform specific tasks.\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    The output is raw analysis, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    # logger.info(f\"Executing Code Analysis for: {code[:50]}...\") # Uncomment when logger is ready\n",
    "    return f\"Code Analysis Result: For the code snippet '{code}', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    The output is raw explanation, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    # logger.info(f\"Executing Code Explanation for: {concept}\") # Uncomment when logger is ready\n",
    "    return f\"Explanation Result: The concept of '{concept}' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    The output is the challenge, which the Socratic agent will present.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    # logger.info(f\"Executing Challenge Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    return f\"Challenge Result: For '{topic}' at '{difficulty}' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specifically for MCQ generation.\n",
    "    # logger.info(f\"Executing MCQ Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    mcq_data = {\n",
    "        \"question\": f\"Which of the following operations would lead to an `IndentationError` in Python?\",\n",
    "        \"options\": [\"A) Missing a colon after a function definition\", \"B) Inconsistent use of spaces and tabs for indentation\", \"C) Using a reserved keyword as a variable name\", \"D) Forgetting a closing parenthesis\"],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "    return json.dumps(mcq_data)\n",
    "\n",
    "# List of all user-facing tools\n",
    "# Changed to a dictionary for direct lookup by name\n",
    "user_facing_tools_map = {\n",
    "    code_analysis_agent.name: code_analysis_agent,\n",
    "    code_explanation_agent.name: code_explanation_agent,\n",
    "    challenge_generator_agent.name: challenge_generator_agent,\n",
    "    mcq_agent.name: mcq_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define Internal Supervisor Tools (for routing decisions) ---\n",
    "# These are \"tools\" the supervisor LLM will call to indicate its routing decision.\n",
    "@tool\n",
    "def route_to_socratic_question(query: str = None) -> str:\n",
    "    \"\"\"Routes the conversation to the main Socratic Questioning agent for general teaching or follow-up.\n",
    "    This is the default route for general queries, concept discussions, and after tool outputs.\n",
    "    Optionally includes a follow-up query for the Socratic agent if the intent is specific.\n",
    "    \"\"\"\n",
    "    return \"socratic_question\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_analysis(code: str) -> str:\n",
    "    \"\"\"Routes to the Code Analysis agent for debugging or code review. Requires the code snippet.\"\"\"\n",
    "    return \"code_analysis\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_explanation(concept: str) -> str:\n",
    "    \"\"\"Routes to the Code Explanation agent to explain a specific concept, keyword, or error. Requires the concept.\"\"\"\n",
    "    return \"code_explanation\"\n",
    "\n",
    "@tool\n",
    "def route_to_challenge_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the Challenge Generator agent to create a coding challenge. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"challenge_generator\"\n",
    "\n",
    "@tool\n",
    "def route_to_mcq_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the MCQ Generator agent to create a multiple-choice question. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"mcq_generator\"\n",
    "\n",
    "# List of all internal routing tools available to the Supervisor\n",
    "supervisor_routing_tools = [\n",
    "    route_to_socratic_question,\n",
    "    route_to_code_analysis,\n",
    "    route_to_code_explanation,\n",
    "    route_to_challenge_generator,\n",
    "    route_to_mcq_generator\n",
    "]\n",
    "\n",
    "# Supervisor Agent Setup\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a highly intelligent routing agent for a Socratic Python Tutor. Your task is to analyze\n",
    "the user's last message and the conversation history to determine the most appropriate next step\n",
    "in the learning process.\n",
    "\n",
    "You have access to several internal routing tools. Call exactly one of these tools to specify\n",
    "which specialized agent or flow should handle the user's request.\n",
    "\n",
    "Here are your routing rules:\n",
    "-   **Default:** For general questions, learning new topics, or continuing a Socratic dialogue, use `route_to_socratic_question`. This should be your most frequent choice.\n",
    "-   **Code Analysis:** If the user provides Python code and asks for debugging, feedback, review, or analysis, use `route_to_code_analysis` and pass the code.\n",
    "-   **Code Explanation:** If the user explicitly asks for an explanation of a specific Python concept, keyword, or error message, use `route_to_code_explanation` and pass the concept.\n",
    "-   **Challenge/Exercise:** If the user explicitly asks for a coding challenge, exercise, or fill-in-the-blanks, use `route_to_challenge_generator`.\n",
    "-   **MCQ:** If the user asks for a multiple-choice question or you determine an MCQ is a good way to test their understanding, use `route_to_mcq_generator`.\n",
    "\n",
    "Pay close attention to keywords and the overall intent. Your response MUST be a tool call.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "supervisor_runnable = supervisor_prompt | llm.bind_tools(supervisor_routing_tools)\n",
    "\n",
    "# Socratic Agent Setup (This is our main Socratic Questioning LLM)\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Interpret Tool Outputs Socratically:** If a tool provides information (e.g., Code Analysis Result, Explanation Result, Challenge Result), your task is to *process that information* and turn it into a Socratic question or guided step for the user. Do not just relay the tool's output directly.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub-topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **Direct Socratic Reply:** Do not include any \"Thought:\" prefix or similar internal monologue in your responses. Your output should be a direct Socratic question or feedback.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "socratic_agent_runnable = socratic_prompt | llm # Socratic agent does not call tools directly, supervisor does\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_supervisor(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the supervisor to determine the next action/agent based on user intent.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Supervisor node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    # Pass the full state to the prompt for contextual awareness in routing\n",
    "    response = supervisor_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"]\n",
    "    })\n",
    "\n",
    "    # The supervisor is expected to call one of its internal routing tools.\n",
    "    # Extract the tool call and set the next_node and tool_input in the state.\n",
    "    if response.tool_calls:\n",
    "        # Access tool name from the 'function' dict within the tool_call dictionary\n",
    "        # tool_call is a dictionary here, not a ToolCall object\n",
    "        tool_call_dict = response.tool_calls[0]\n",
    "        tool_name = tool_call_dict.get('function', {}).get('name')\n",
    "        tool_input = tool_call_dict.get('function', {}).get('arguments') # Arguments are also nested\n",
    "\n",
    "        # IMPORTANT: Add a check here if tool_name is None, indicating a malformed tool call\n",
    "        if tool_name is None:\n",
    "            print(\"Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\")\n",
    "            return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "        next_node = tool_name.replace(\"route_to_\", \"\") # Extract node name (e.g., \"socratic_question\")\n",
    "        # logger.info(f\"Supervisor decided to route to: {next_node} with input: {tool_input}\") # Uncomment when logger is ready\n",
    "        return {\"messages\": [response], \"next_node\": next_node, \"tool_input\": tool_input}\n",
    "    else:\n",
    "        # Fallback if supervisor doesn't call a tool (shouldn't happen with proper prompt)\n",
    "        # Force it to the socratic question node\n",
    "        print(\"Warning: Supervisor LLM did not call a tool. Defaulting to socratic_question.\")\n",
    "        return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "\n",
    "def socratic_question_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the main Socratic LLM to ask questions or interpret tool outputs.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Socratic Question Node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    # Removed extraction of agent_thought as per user request\n",
    "    # thought = \"\"\n",
    "    # if response.content and response.content.startswith(\"Thought:\"):\n",
    "    #     parts = response.content.split(\"Thought:\", 1)\n",
    "    #     if len(parts) > 1:\n",
    "    #         thought = parts[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    # Return without agent_thought\n",
    "    return {\"messages\": [response], \"agent_thought\": \"\"} # Set agent_thought to empty string\n",
    "\n",
    "\n",
    "# --- New Nodes for Specialized Tools ---\n",
    "\n",
    "def code_analysis_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_analysis_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_analysis_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def code_explanation_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_explanation_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_explanation_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def challenge_generator_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the challenge_generator_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"challenge_generator_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "\n",
    "def generate_mcq_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node specifically for generating an MCQ via the mcq_agent tool.\n",
    "    This also handles setting the MCQ active state for main.py.\n",
    "    \"\"\"\n",
    "    # logger.info(\"MCQ Generation Node activated.\") # Uncomment when logger is ready\n",
    "    tool_name = state[\"next_node\"] # This comes from the supervisor's routing decision\n",
    "    tool_args = state[\"tool_input\"] # Should contain topic and difficulty from supervisor\n",
    "\n",
    "    # Manually find and execute the tool function using the map\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    mcq_raw_output = \"\"\n",
    "\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            mcq_raw_output = tool_function.invoke(tool_args)\n",
    "            mcq_data = json.loads(mcq_raw_output)\n",
    "            state[\"mcq_active\"] = True\n",
    "            state[\"mcq_question\"] = mcq_data[\"question\"]\n",
    "            state[\"mcq_options\"] = mcq_data[\"options\"]\n",
    "            state[\"mcq_correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "            # logger.info(\"MCQ details updated in state.\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            mcq_raw_output = f\"Error generating MCQ: {e}\"\n",
    "            # logger.error(f\"Error generating MCQ: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    \n",
    "    # Add a ToolMessage for the MCQ generation, which the Socratic LLM can interpret\n",
    "    # or simply for logging purposes in the graph flow.\n",
    "    # Provide a unique tool_call_id for the ToolMessage\n",
    "    return {\"messages\": [ToolMessage(content=mcq_raw_output, name=tool_name, tool_call_id=str(uuid.uuid4()))], **state}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def route_supervisor_output(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Conditional edge from the supervisor to determine the next node based on its decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Routing supervisor output. Next node: {state['next_node']}\") # Uncomment when logger is ready\n",
    "    if state[\"next_node\"] == \"socratic_question\":\n",
    "        return \"socratic_question_node\"\n",
    "    elif state[\"next_node\"] == \"mcq_generator\":\n",
    "        return \"generate_mcq_node\"\n",
    "    # All other specialized tools now have their own nodes\n",
    "    elif state[\"next_node\"] == \"code_analysis\":\n",
    "        return \"code_analysis_node\"\n",
    "    elif state[\"next_node\"] == \"code_explanation\":\n",
    "        return \"code_explanation_node\"\n",
    "    elif state[\"next_node\"] == \"challenge_generator\":\n",
    "        return \"challenge_generator_node\"\n",
    "    return \"socratic_question_node\" # Fallback to socratic question if unexpected\n",
    "\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_supervisor\", call_supervisor)\n",
    "workflow.add_node(\"socratic_question_node\", socratic_question_node) # Renamed from call_llm\n",
    "# Add new specialized tool nodes\n",
    "workflow.add_node(\"code_analysis_node\", code_analysis_node)\n",
    "workflow.add_node(\"code_explanation_node\", code_explanation_node)\n",
    "workflow.add_node(\"challenge_generator_node\", challenge_generator_node)\n",
    "workflow.add_node(\"generate_mcq_node\", generate_mcq_node)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_supervisor\")\n",
    "\n",
    "# Define the edges.\n",
    "# From supervisor, route conditionally\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_supervisor\",\n",
    "    route_supervisor_output,\n",
    "    {\n",
    "        \"socratic_question_node\": \"socratic_question_node\",\n",
    "        \"code_analysis_node\": \"code_analysis_node\",\n",
    "        \"code_explanation_node\": \"code_explanation_node\",\n",
    "        \"challenge_generator_node\": \"challenge_generator_node\",\n",
    "        \"generate_mcq_node\": \"generate_mcq_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After each specialized tool node, return to the socratic_question_node\n",
    "# for the Socratic LLM to interpret the tool's output and formulate a question.\n",
    "workflow.add_edge(\"code_analysis_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"code_explanation_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"challenge_generator_node\", \"socratic_question_node\")\n",
    "\n",
    "# After the socratic_question_node, the run ends. The main.py loop will then take user input.\n",
    "workflow.add_edge(\"socratic_question_node\", END)\n",
    "\n",
    "# After generating an MCQ, the run ends. Main.py handles the MCQ display and user input.\n",
    "workflow.add_edge(\"generate_mcq_node\", END)\n",
    "\n",
    "# Removed MemorySaver initialization and compilation\n",
    "# checkpointer = MemorySaver()\n",
    "socratic_graph = workflow.compile() # Compile without checkpointer\n",
    "\n",
    "# --- Temporary LLM Connection Test ---\n",
    "# This function will be called once when the module is imported to test LLM connectivity.\n",
    "def _test_llm_connection():\n",
    "    print(\"\\n--- Testing LLM Connection (Temporary) ---\")\n",
    "    try:\n",
    "        test_message = HumanMessage(content=\"Say hi!\")\n",
    "        response = llm.invoke([test_message])\n",
    "        print(f\"LLM Test Response: {response.content}\")\n",
    "        print(\"LLM connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Connection Test FAILED: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and network connection.\")\n",
    "    print(\"--- End LLM Connection Test ---\\n\")\n",
    "\n",
    "# --- Temporary Node Functionality Test ---\n",
    "# This function allows testing individual nodes with a sample state.\n",
    "def _test_node_functionality(node_name: str, initial_state: SocraticAgentState):\n",
    "    print(f\"\\n--- Testing Node: {node_name} ---\")\n",
    "    try:\n",
    "        # Get the node function from the workflow's nodes\n",
    "        # Access the runnable attribute to get the actual function\n",
    "        node_runnable = workflow.nodes[node_name].runnable\n",
    "\n",
    "        # Invoke the node's runnable with the provided initial state\n",
    "        result_state_update = node_runnable.invoke(initial_state)\n",
    "\n",
    "\n",
    "        print(f\"Node '{node_name}' executed successfully.\")\n",
    "        print(f\"Initial State (messages only): {[msg.content if hasattr(msg, 'content') else str(msg) for msg in initial_state['messages']]}\")\n",
    "        print(f\"Resulting State Update: {result_state_update}\")\n",
    "\n",
    "        # For nodes that return messages, let's print them\n",
    "        if 'messages' in result_state_update and result_state_update['messages']:\n",
    "            print(\"Messages returned by node:\")\n",
    "            for msg in result_state_update['messages']:\n",
    "                if isinstance(msg, BaseMessage):\n",
    "                    print(f\"  - Type: {msg.type}, Content: {msg.content[:50]}...\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        # Ensure tool_calls are correctly converted to ToolCall objects\n",
    "                        for tc in msg.tool_calls:\n",
    "                            # The error was here when tc was a dict\n",
    "                            # Now, tc should be a ToolCall object or a dict that needs careful access\n",
    "                            # Let's make this more robust for the debug print\n",
    "                            if hasattr(tc, 'function') and hasattr(tc.function, 'name'):\n",
    "                                tool_name_debug = tc.function.name\n",
    "                                tool_args_debug = tc.function.arguments\n",
    "                            elif isinstance(tc, dict) and 'function' in tc and 'name' in tc['function']:\n",
    "                                tool_name_debug = tc['function']['name']\n",
    "                                tool_args_debug = tc['function'].get('arguments', {})\n",
    "                            else:\n",
    "                                tool_name_debug = \"UNKNOWN_TOOL_NAME\"\n",
    "                                tool_args_debug = \"UNKNOWN_ARGS\"\n",
    "                                print(f\"    Warning: Could not extract tool info from {tc}\")\n",
    "\n",
    "                            print(f\"    Tool Call: {tool_name_debug} with args {tool_args_debug}\")\n",
    "                else:\n",
    "                    print(f\"  - Raw: {msg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Node '{node_name}' Test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for node errors\n",
    "    print(f\"--- End Node Test: {node_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Call the LLM connection test immediately when this module is loaded\n",
    "_test_llm_connection()\n",
    "\n",
    "# Example usage of the new node testing function (uncomment to test specific nodes)\n",
    "# Ensure your GOOGLE_API_KEY is set for LLM-based nodes.\n",
    "\n",
    "# Test call_supervisor node\n",
    "sample_supervisor_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"I need help with debugging this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Introduction\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"call_supervisor\", sample_supervisor_state)\n",
    "\n",
    "# Test socratic_question_node\n",
    "sample_socratic_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"What are variables?\"), AIMessage(content=\"User asked about variables. I should ask a foundational question. What is your current understanding of variables in programming?\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Variables\",\n",
    "    sub_topic=\"Definition\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"socratic_question_node\", sample_socratic_state)\n",
    "\n",
    "# Test code_analysis_node\n",
    "sample_tool_state_code_analysis = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"debug this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Debugging\",\n",
    "    sub_topic=\"Code Analysis\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_analysis_agent\", # This should match the tool's actual name\n",
    "    tool_input={\"code\": \"def my_func():\\n    pass\"}\n",
    ")\n",
    "_test_node_functionality(\"code_analysis_node\", sample_tool_state_code_analysis)\n",
    "\n",
    "# Test code_explanation_node\n",
    "sample_tool_state_code_explanation = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Explain loops in Python\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Loops\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_explanation_agent\",\n",
    "    tool_input={\"concept\": \"loops\"}\n",
    ")\n",
    "_test_node_functionality(\"code_explanation_node\", sample_tool_state_code_explanation)\n",
    "\n",
    "# Test challenge_generator_node\n",
    "sample_tool_state_challenge_generator = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me a challenge on functions.\")],\n",
    "    difficulty_level=\"intermediate\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"Challenges\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"challenge_generator_agent\",\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"intermediate\"}\n",
    ")\n",
    "_test_node_functionality(\"challenge_generator_node\", sample_tool_state_challenge_generator)\n",
    "\n",
    "# Test generate_mcq_node\n",
    "sample_mcq_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me an MCQ on functions.\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"MCQ\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"mcq_agent\", # Supervisor would set this\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"beginner\"} # Supervisor would set this\n",
    ")\n",
    "_test_node_functionality(\"generate_mcq_node\", sample_mcq_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f761cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM Connection (Temporary) ---\n",
      "LLM Test Response: Hi!\n",
      "LLM connection successful!\n",
      "--- End LLM Connection Test ---\n",
      "\n",
      "\n",
      "--- Testing Node: call_supervisor ---\n",
      "Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\n",
      "Node 'call_supervisor' executed successfully.\n",
      "Initial State (messages only): [\"I need help with debugging this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'route_to_code_analysis', 'arguments': '{\"code\": \"print(\\'hello\\')\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--371d4d34-9269-4a25-a7cf-b3bfb390cb9c-0', tool_calls=[{'name': 'route_to_code_analysis', 'args': {'code': \"print('hello')\"}, 'id': '0ebd1842-91a7-4d6e-883b-8f14ef1b7552', 'type': 'tool_call'}], usage_metadata={'input_tokens': 533, 'output_tokens': 12, 'total_tokens': 545, 'input_token_details': {'cache_read': 0}})], 'next_node': 'socratic_question'}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: ...\n",
      "Node 'call_supervisor' Test FAILED: TypedDict does not support instance and class checks\n",
      "--- End Node Test: call_supervisor ---\n",
      "\n",
      "\n",
      "--- Testing Node: socratic_question_node ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12503/1400524166.py\", line 490, in _test_node_functionality\n",
      "    if isinstance(tc, ToolCall):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/codespace/.local/lib/python3.12/site-packages/typing_extensions.py\", line 1102, in __subclasscheck__\n",
      "    raise TypeError('TypedDict does not support instance and class checks')\n",
      "TypeError: TypedDict does not support instance and class checks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'socratic_question_node' executed successfully.\n",
      "Initial State (messages only): ['What are variables?', 'Thought: User asked about variables. I should ask a foundational question. What is your current understanding of variables in programming?']\n",
      "Resulting State Update: {'messages': [AIMessage(content='What comes to mind when you hear the term \"variable\" outside of the context of programming, perhaps in mathematics or everyday life?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--8b2f571e-0669-456c-b313-8c08f9c1ce39-0', usage_metadata={'input_tokens': 405, 'output_tokens': 27, 'total_tokens': 432, 'input_token_details': {'cache_read': 0}})], 'agent_thought': ''}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: What comes to mind when you hear the term \"variabl...\n",
      "--- End Node Test: socratic_question_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_analysis_node ---\n",
      "Node 'code_analysis_node' executed successfully.\n",
      "Initial State (messages only): [\"debug this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Code Analysis Result: For the code snippet 'def my_func():\\n    pass', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\", name='code_analysis_agent', tool_call_id='5885ff88-fe19-4856-a8ee-ce248e383473')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Code Analysis Result: For the code snippet 'def my...\n",
      "--- End Node Test: code_analysis_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_explanation_node ---\n",
      "Node 'code_explanation_node' executed successfully.\n",
      "Initial State (messages only): ['Explain loops in Python']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Explanation Result: The concept of 'loops' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\", name='code_explanation_agent', tool_call_id='abacaea8-2dd1-4a51-8ed0-9f1d942fe514')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Explanation Result: The concept of 'loops' in Pyth...\n",
      "--- End Node Test: code_explanation_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: challenge_generator_node ---\n",
      "Node 'challenge_generator_node' executed successfully.\n",
      "Initial State (messages only): ['Give me a challenge on functions.']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Challenge Result: For 'functions' at 'intermediate' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\", name='challenge_generator_agent', tool_call_id='df106beb-9bbd-4b9a-9c65-a994c5351c2f')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Challenge Result: For 'functions' at 'intermediate...\n",
      "--- End Node Test: challenge_generator_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: generate_mcq_node ---\n",
      "Node 'generate_mcq_node' executed successfully.\n",
      "Initial State (messages only): ['Give me an MCQ on functions.']\n",
      "Resulting State Update: {'messages': [HumanMessage(content='Give me an MCQ on functions.', additional_kwargs={}, response_metadata={})], 'difficulty_level': 'beginner', 'user_struggle_count': 0, 'topic': 'Functions', 'sub_topic': 'MCQ', 'mcq_active': True, 'mcq_question': 'Which of the following operations would lead to an `IndentationError` in Python?', 'mcq_options': ['A) Missing a colon after a function definition', 'B) Inconsistent use of spaces and tabs for indentation', 'C) Using a reserved keyword as a variable name', 'D) Forgetting a closing parenthesis'], 'mcq_correct_answer': 'B', 'agent_thought': '', 'next_node': 'mcq_agent', 'tool_input': {'topic': 'functions', 'difficulty': 'beginner'}}\n",
      "Messages returned by node:\n",
      "  - Type: human, Content: Give me an MCQ on functions....\n",
      "--- End Node Test: generate_mcq_node ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool # Ensure tool decorator is imported\n",
    "import json\n",
    "import uuid # Import uuid for generating unique IDs\n",
    "\n",
    "# Removed MemorySaver import\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Import the logging utility (assuming logger.py will be created later)\n",
    "# from logger import setup_logger\n",
    "# logger = setup_logger() # Uncomment when logger.py is ready\n",
    "\n",
    "# --- Configuration for Memory Management ---\n",
    "MAX_MESSAGES_IN_CONTEXT = 10 # Keep the last 10 messages in the context window\n",
    "# This includes both HumanMessage and AIMessage. Adjust as needed based on LLM context limits.\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "        # Added for supervisor routing\n",
    "        next_node: str # The next node the supervisor has decided to route to\n",
    "        tool_input: dict # Input arguments for the tool if a tool is routed to\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], lambda x, y: x + y] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node: str\n",
    "    tool_input: dict\n",
    "\n",
    "\n",
    "# --- 2. Initialize the LLMs and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM (used for both Socratic and Supervisor agents)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "# --- Define User-Facing Simulated Agent Tools ---\n",
    "# These are the actual tools that will perform specific tasks.\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    The output is raw analysis, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    # logger.info(f\"Executing Code Analysis for: {code[:50]}...\") # Uncomment when logger is ready\n",
    "    return f\"Code Analysis Result: For the code snippet '{code}', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    The output is raw explanation, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    # logger.info(f\"Executing Code Explanation for: {concept}\") # Uncomment when logger is ready\n",
    "    return f\"Explanation Result: The concept of '{concept}' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    The output is the challenge, which the Socratic agent will present.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    # logger.info(f\"Executing Challenge Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    return f\"Challenge Result: For '{topic}' at '{difficulty}' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specifically for MCQ generation.\n",
    "    # logger.info(f\"Executing MCQ Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    mcq_data = {\n",
    "        \"question\": f\"Which of the following operations would lead to an `IndentationError` in Python?\",\n",
    "        \"options\": [\"A) Missing a colon after a function definition\", \"B) Inconsistent use of spaces and tabs for indentation\", \"C) Using a reserved keyword as a variable name\", \"D) Forgetting a closing parenthesis\"],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "    return json.dumps(mcq_data)\n",
    "\n",
    "# List of all user-facing tools\n",
    "# Changed to a dictionary for direct lookup by name\n",
    "user_facing_tools_map = {\n",
    "    code_analysis_agent.name: code_analysis_agent,\n",
    "    code_explanation_agent.name: code_explanation_agent,\n",
    "    challenge_generator_agent.name: challenge_generator_agent,\n",
    "    mcq_agent.name: mcq_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define Internal Supervisor Tools (for routing decisions) ---\n",
    "# These are \"tools\" the supervisor LLM will call to indicate its routing decision.\n",
    "@tool\n",
    "def route_to_socratic_question(query: str = None) -> str:\n",
    "    \"\"\"Routes the conversation to the main Socratic Questioning agent for general teaching or follow-up.\n",
    "    This is the default route for general queries, concept discussions, and after tool outputs.\n",
    "    Optionally includes a follow-up query for the Socratic agent if the intent is specific.\n",
    "    \"\"\"\n",
    "    return \"socratic_question\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_analysis(code: str) -> str:\n",
    "    \"\"\"Routes to the Code Analysis agent for debugging or code review. Requires the code snippet.\"\"\"\n",
    "    return \"code_analysis\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_explanation(concept: str) -> str:\n",
    "    \"\"\"Routes to the Code Explanation agent to explain a specific concept, keyword, or error. Requires the concept.\"\"\"\n",
    "    return \"code_explanation\"\n",
    "\n",
    "@tool\n",
    "def route_to_challenge_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the Challenge Generator agent to create a coding challenge. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"challenge_generator\"\n",
    "\n",
    "@tool\n",
    "def route_to_mcq_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the MCQ Generator agent to create a multiple-choice question. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"mcq_generator\"\n",
    "\n",
    "# List of all internal routing tools available to the Supervisor\n",
    "supervisor_routing_tools = [\n",
    "    route_to_socratic_question,\n",
    "    route_to_code_analysis,\n",
    "    route_to_code_explanation,\n",
    "    route_to_challenge_generator,\n",
    "    route_to_mcq_generator\n",
    "]\n",
    "\n",
    "# Supervisor Agent Setup\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a highly intelligent routing agent for a Socratic Python Tutor. Your task is to analyze\n",
    "the user's last message and the conversation history to determine the most appropriate next step\n",
    "in the learning process.\n",
    "\n",
    "You have access to several internal routing tools. Call exactly one of these tools to specify\n",
    "which specialized agent or flow should handle the user's request.\n",
    "\n",
    "Here are your routing rules:\n",
    "-   **Default:** For general questions, learning new topics, or continuing a Socratic dialogue, use `route_to_socratic_question`. This should be your most frequent choice.\n",
    "-   **Code Analysis:** If the user provides Python code and asks for debugging, feedback, review, or analysis, use `route_to_code_analysis` and pass the code.\n",
    "-   **Code Explanation:** If the user explicitly asks for an explanation of a specific Python concept, keyword, or error message, use `route_to_code_explanation` and pass the concept.\n",
    "-   **Challenge/Exercise:** If the user explicitly asks for a coding challenge, exercise, or fill-in-the-blanks, use `route_to_challenge_generator`.\n",
    "-   **MCQ:** If the user asks for a multiple-choice question or you determine an MCQ is a good way to test their understanding, use `route_to_mcq_generator`.\n",
    "\n",
    "Pay close attention to keywords and the overall intent. Your response MUST be a tool call.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "supervisor_runnable = supervisor_prompt | llm.bind_tools(supervisor_routing_tools)\n",
    "\n",
    "# Socratic Agent Setup (This is our main Socratic Questioning LLM)\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Interpret Tool Outputs Socratically:** If a tool provides information (e.g., Code Analysis Result, Explanation Result, Challenge Result), your task is to *process that information* and turn it into a Socratic question or guided step for the user. Do not just relay the tool's output directly.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub-topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture (Internal Thought):** Before responding, articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". This thought will be logged but not shown to the user. Then, proceed with your Socratic question.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "socratic_agent_runnable = socratic_prompt | llm # Socratic agent does not call tools directly, supervisor does\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_supervisor(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the supervisor to determine the next action/agent based on user intent.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Supervisor node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    # Pass the full state to the prompt for contextual awareness in routing\n",
    "    response = supervisor_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"]\n",
    "    })\n",
    "\n",
    "    # The supervisor is expected to call one of its internal routing tools.\n",
    "    # Extract the tool call and set the next_node and tool_input in the state.\n",
    "    if response.tool_calls:\n",
    "        # Access tool name from the 'function' dict within the tool_call dictionary\n",
    "        # tool_call is a dictionary here, not a ToolCall object\n",
    "        tool_call_dict = response.tool_calls[0]\n",
    "        tool_name = tool_call_dict.get('function', {}).get('name')\n",
    "        tool_input = tool_call_dict.get('function', {}).get('arguments') # Arguments are also nested\n",
    "\n",
    "        # IMPORTANT: Add a check here if tool_name is None, indicating a malformed tool call\n",
    "        if tool_name is None:\n",
    "            print(\"Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\")\n",
    "            return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "        next_node = tool_name.replace(\"route_to_\", \"\") # Extract node name (e.g., \"socratic_question\")\n",
    "        # logger.info(f\"Supervisor decided to route to: {next_node} with input: {tool_input}\") # Uncomment when logger is ready\n",
    "        return {\"messages\": [response], \"next_node\": next_node, \"tool_input\": tool_input}\n",
    "    else:\n",
    "        # Fallback if supervisor doesn't call a tool (shouldn't happen with proper prompt)\n",
    "        # Force it to the socratic question node\n",
    "        print(\"Warning: Supervisor LLM did not call a tool. Defaulting to socratic_question.\")\n",
    "        return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "\n",
    "def socratic_question_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the main Socratic LLM to ask questions or interpret tool outputs.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Socratic Question Node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    # Extract thought (for logging)\n",
    "    thought = \"\"\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            thought = parts[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    return {\"messages\": [response], \"agent_thought\": thought}\n",
    "\n",
    "\n",
    "# --- New Nodes for Specialized Tools ---\n",
    "\n",
    "def code_analysis_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_analysis_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_analysis_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def code_explanation_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_explanation_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_explanation_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def challenge_generator_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the challenge_generator_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"challenge_generator_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "\n",
    "def generate_mcq_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node specifically for generating an MCQ via the mcq_agent tool.\n",
    "    This also handles setting the MCQ active state for main.py.\n",
    "    \"\"\"\n",
    "    # logger.info(\"MCQ Generation Node activated.\") # Uncomment when logger is ready\n",
    "    tool_name = state[\"next_node\"] # This comes from the supervisor's routing decision\n",
    "    tool_args = state[\"tool_input\"] # Should contain topic and difficulty from supervisor\n",
    "\n",
    "    # Manually find and execute the tool function using the map\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    mcq_raw_output = \"\"\n",
    "\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            mcq_raw_output = tool_function.invoke(tool_args)\n",
    "            mcq_data = json.loads(mcq_raw_output)\n",
    "            state[\"mcq_active\"] = True\n",
    "            state[\"mcq_question\"] = mcq_data[\"question\"]\n",
    "            state[\"mcq_options\"] = mcq_data[\"options\"]\n",
    "            state[\"mcq_correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "            # logger.info(\"MCQ details updated in state.\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            mcq_raw_output = f\"Error generating MCQ: {e}\"\n",
    "            # logger.error(f\"Error generating MCQ: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    \n",
    "    # Add a ToolMessage for the MCQ generation, which the Socratic LLM can interpret\n",
    "    # or simply for logging purposes in the graph flow.\n",
    "    # Provide a unique tool_call_id for the ToolMessage\n",
    "    return {\"messages\": [ToolMessage(content=mcq_raw_output, name=tool_name, tool_call_id=str(uuid.uuid4()))], **state}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def route_supervisor_output(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Conditional edge from the supervisor to determine the next node based on its decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Routing supervisor output. Next node: {state['next_node']}\") # Uncomment when logger is ready\n",
    "    if state[\"next_node\"] == \"socratic_question\":\n",
    "        return \"socratic_question_node\"\n",
    "    elif state[\"next_node\"] == \"mcq_generator\":\n",
    "        return \"generate_mcq_node\"\n",
    "    # All other specialized tools now have their own nodes\n",
    "    elif state[\"next_node\"] == \"code_analysis\":\n",
    "        return \"code_analysis_node\"\n",
    "    elif state[\"next_node\"] == \"code_explanation\":\n",
    "        return \"code_explanation_node\"\n",
    "    elif state[\"next_node\"] == \"challenge_generator\":\n",
    "        return \"challenge_generator_node\"\n",
    "    return \"socratic_question_node\" # Fallback to socratic question if unexpected\n",
    "\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_supervisor\", call_supervisor)\n",
    "workflow.add_node(\"socratic_question_node\", socratic_question_node) # Renamed from call_llm\n",
    "# Add new specialized tool nodes\n",
    "workflow.add_node(\"code_analysis_node\", code_analysis_node)\n",
    "workflow.add_node(\"code_explanation_node\", code_explanation_node)\n",
    "workflow.add_node(\"challenge_generator_node\", challenge_generator_node)\n",
    "workflow.add_node(\"generate_mcq_node\", generate_mcq_node)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_supervisor\")\n",
    "\n",
    "# Define the edges.\n",
    "# From supervisor, route conditionally\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_supervisor\",\n",
    "    route_supervisor_output,\n",
    "    {\n",
    "        \"socratic_question_node\": \"socratic_question_node\",\n",
    "        \"code_analysis_node\": \"code_analysis_node\",\n",
    "        \"code_explanation_node\": \"code_explanation_node\",\n",
    "        \"challenge_generator_node\": \"challenge_generator_node\",\n",
    "        \"generate_mcq_node\": \"generate_mcq_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After each specialized tool node, return to the socratic_question_node\n",
    "# for the Socratic LLM to interpret the tool's output and formulate a question.\n",
    "workflow.add_edge(\"code_analysis_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"code_explanation_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"challenge_generator_node\", \"socratic_question_node\")\n",
    "\n",
    "# After the socratic_question_node, the run ends. The main.py loop will then take user input.\n",
    "workflow.add_edge(\"socratic_question_node\", END)\n",
    "\n",
    "# After generating an MCQ, the run ends. Main.py handles the MCQ display and user input.\n",
    "workflow.add_edge(\"generate_mcq_node\", END)\n",
    "\n",
    "# Removed MemorySaver initialization and compilation\n",
    "# checkpointer = MemorySaver()\n",
    "socratic_graph = workflow.compile() # Compile without checkpointer\n",
    "\n",
    "# --- Temporary LLM Connection Test ---\n",
    "# This function will be called once when the module is imported to test LLM connectivity.\n",
    "def _test_llm_connection():\n",
    "    print(\"\\n--- Testing LLM Connection (Temporary) ---\")\n",
    "    try:\n",
    "        test_message = HumanMessage(content=\"Say hi!\")\n",
    "        response = llm.invoke([test_message])\n",
    "        print(f\"LLM Test Response: {response.content}\")\n",
    "        print(\"LLM connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Connection Test FAILED: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and network connection.\")\n",
    "    print(\"--- End LLM Connection Test ---\\n\")\n",
    "\n",
    "# --- Temporary Node Functionality Test ---\n",
    "# This function allows testing individual nodes with a sample state.\n",
    "def _test_node_functionality(node_name: str, initial_state: SocraticAgentState):\n",
    "    print(f\"\\n--- Testing Node: {node_name} ---\")\n",
    "    try:\n",
    "        # Get the node function from the workflow's nodes\n",
    "        # Access the runnable attribute to get the actual function\n",
    "        node_runnable = workflow.nodes[node_name].runnable\n",
    "\n",
    "        # Invoke the node's runnable with the provided initial state\n",
    "        result_state_update = node_runnable.invoke(initial_state)\n",
    "\n",
    "\n",
    "        print(f\"Node '{node_name}' executed successfully.\")\n",
    "        print(f\"Initial State (messages only): {[msg.content if hasattr(msg, 'content') else str(msg) for msg in initial_state['messages']]}\")\n",
    "        print(f\"Resulting State Update: {result_state_update}\")\n",
    "\n",
    "        # For nodes that return messages, let's print them\n",
    "        if 'messages' in result_state_update and result_state_update['messages']:\n",
    "            print(\"Messages returned by node:\")\n",
    "            for msg in result_state_update['messages']:\n",
    "                if isinstance(msg, BaseMessage):\n",
    "                    print(f\"  - Type: {msg.type}, Content: {msg.content[:50]}...\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        # Ensure tool_calls are correctly converted to ToolCall objects\n",
    "                        for tc in msg.tool_calls:\n",
    "                            if isinstance(tc, ToolCall):\n",
    "                                print(f\"    Tool Call: {tc.function.name} with args {tc.function.arguments}\")\n",
    "                            else:\n",
    "                                # Fallback for debugging if it's still a dict\n",
    "                                print(f\"    Raw Tool Call (still dict?): {tc}\")\n",
    "                                print(f\"    Attempting dict access: Name={tc.get('function', {}).get('name')}, Args={tc.get('function', {}).get('arguments')}\")\n",
    "                else:\n",
    "                    print(f\"  - Raw: {msg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Node '{node_name}' Test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for node errors\n",
    "    print(f\"--- End Node Test: {node_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Call the LLM connection test immediately when this module is loaded\n",
    "_test_llm_connection()\n",
    "\n",
    "# Example usage of the new node testing function (uncomment to test specific nodes)\n",
    "# Ensure your GOOGLE_API_KEY is set for LLM-based nodes.\n",
    "\n",
    "# Test call_supervisor node\n",
    "sample_supervisor_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"I need help with debugging this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Introduction\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"call_supervisor\", sample_supervisor_state)\n",
    "\n",
    "# Test socratic_question_node\n",
    "sample_socratic_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"What are variables?\"), AIMessage(content=\"Thought: User asked about variables. I should ask a foundational question. What is your current understanding of variables in programming?\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Variables\",\n",
    "    sub_topic=\"Definition\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"socratic_question_node\", sample_socratic_state)\n",
    "\n",
    "# Test code_analysis_node\n",
    "sample_tool_state_code_analysis = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"debug this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Debugging\",\n",
    "    sub_topic=\"Code Analysis\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_analysis_agent\", # This should match the tool's actual name\n",
    "    tool_input={\"code\": \"def my_func():\\n    pass\"}\n",
    ")\n",
    "_test_node_functionality(\"code_analysis_node\", sample_tool_state_code_analysis)\n",
    "\n",
    "# Test code_explanation_node\n",
    "sample_tool_state_code_explanation = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Explain loops in Python\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Loops\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_explanation_agent\",\n",
    "    tool_input={\"concept\": \"loops\"}\n",
    ")\n",
    "_test_node_functionality(\"code_explanation_node\", sample_tool_state_code_explanation)\n",
    "\n",
    "# Test challenge_generator_node\n",
    "sample_tool_state_challenge_generator = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me a challenge on functions.\")],\n",
    "    difficulty_level=\"intermediate\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"Challenges\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"challenge_generator_agent\",\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"intermediate\"}\n",
    ")\n",
    "_test_node_functionality(\"challenge_generator_node\", sample_tool_state_challenge_generator)\n",
    "\n",
    "# Test generate_mcq_node\n",
    "sample_mcq_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me an MCQ on functions.\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"MCQ\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"mcq_agent\", # Supervisor would set this\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"beginner\"} # Supervisor would set this\n",
    ")\n",
    "_test_node_functionality(\"generate_mcq_node\", sample_mcq_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3b663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5231111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM Connection (Temporary) ---\n",
      "LLM Test Response: Hi there! How can I help you today?\n",
      "LLM connection successful!\n",
      "--- End LLM Connection Test ---\n",
      "\n",
      "\n",
      "--- Testing Node: socratic_question_node ---\n",
      "Node 'socratic_question_node' executed successfully.\n",
      "Initial State (messages only): ['What are variables?']\n",
      "Resulting State Update: {'messages': [AIMessage(content=\"Okay! We're starting with variables.\\n\\nVariables are fundamental to programming. In Python, they're like named containers that hold values.\\n\\nTo begin, can you describe in your own words what you think a variable is and what purpose it serves in a program?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--8b375431-23d7-48db-b652-4c462da8a9c9-0', usage_metadata={'input_tokens': 494, 'output_tokens': 56, 'total_tokens': 550, 'input_token_details': {'cache_read': 0}})], 'agent_thought': ''}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: Okay! We're starting with variables.\n",
      "\n",
      "Variables ar...\n",
      "--- End Node Test: socratic_question_node ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool # Ensure tool decorator is imported\n",
    "import json\n",
    "import uuid # Import uuid for generating unique IDs\n",
    "\n",
    "# Removed MemorySaver import\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Import the logging utility (assuming logger.py will be created later)\n",
    "# from logger import setup_logger\n",
    "# logger = setup_logger() # Uncomment when logger.py is ready\n",
    "\n",
    "# --- Configuration for Memory Management ---\n",
    "MAX_MESSAGES_IN_CONTEXT = 10 # Keep the last 10 messages in the context window\n",
    "# This includes both HumanMessage and AIMessage. Adjust as needed based on LLM context limits.\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "        # Added for supervisor routing\n",
    "        next_node: str # The next node the supervisor has decided to route to\n",
    "        tool_input: dict # Input arguments for the tool if a tool is routed to\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], lambda x, y: x + y] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node: str\n",
    "    tool_input: dict\n",
    "\n",
    "\n",
    "# --- 2. Initialize the LLMs and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM (used for both Socratic and Supervisor agents)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7) # Changed model to gemini-2.0-flash\n",
    "\n",
    "# --- Define User-Facing Simulated Agent Tools ---\n",
    "# These are the actual tools that will perform specific tasks.\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    The output is raw analysis, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    # logger.info(f\"Executing Code Analysis for: {code[:50]}...\") # Uncomment when logger is ready\n",
    "    return f\"Code Analysis Result: For the code snippet '{code}', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    The output is raw explanation, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    # logger.info(f\"Executing Code Explanation for: {concept}\") # Uncomment when logger is ready\n",
    "    return f\"Explanation Result: The concept of '{concept}' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    The output is the challenge, which the Socratic agent will present.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    # logger.info(f\"Executing Challenge Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    return f\"Challenge Result: For '{topic}' at '{difficulty}' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specifically for MCQ generation.\n",
    "    # logger.info(f\"Executing MCQ Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    mcq_data = {\n",
    "        \"question\": f\"Which of the following operations would lead to an `IndentationError` in Python?\",\n",
    "        \"options\": [\"A) Missing a colon after a function definition\", \"B) Inconsistent use of spaces and tabs for indentation\", \"C) Using a reserved keyword as a variable name\", \"D) Forgetting a closing parenthesis\"],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "    return json.dumps(mcq_data)\n",
    "\n",
    "# List of all user-facing tools\n",
    "# Changed to a dictionary for direct lookup by name\n",
    "user_facing_tools_map = {\n",
    "    code_analysis_agent.name: code_analysis_agent,\n",
    "    code_explanation_agent.name: code_explanation_agent,\n",
    "    challenge_generator_agent.name: challenge_generator_agent,\n",
    "    mcq_agent.name: mcq_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define Internal Supervisor Tools (for routing decisions) ---\n",
    "# These are \"tools\" the supervisor LLM will call to indicate its routing decision.\n",
    "@tool\n",
    "def route_to_socratic_question(query: str = None) -> str:\n",
    "    \"\"\"Routes the conversation to the main Socratic Questioning agent for general teaching or follow-up.\n",
    "    This is the default route for general queries, concept discussions, and after tool outputs.\n",
    "    Optionally includes a follow-up query for the Socratic agent if the intent is specific.\n",
    "    \"\"\"\n",
    "    return \"socratic_question\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_analysis(code: str) -> str:\n",
    "    \"\"\"Routes to the Code Analysis agent for debugging or code review. Requires the code snippet.\"\"\"\n",
    "    return \"code_analysis\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_explanation(concept: str) -> str:\n",
    "    \"\"\"Routes to the Code Explanation agent to explain a specific concept, keyword, or error. Requires the concept.\"\"\"\n",
    "    return \"code_explanation\"\n",
    "\n",
    "@tool\n",
    "def route_to_challenge_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the Challenge Generator agent to create a coding challenge. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"challenge_generator\"\n",
    "\n",
    "@tool\n",
    "def route_to_mcq_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the MCQ Generator agent to create a multiple-choice question. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"mcq_generator\"\n",
    "\n",
    "# List of all internal routing tools available to the Supervisor\n",
    "supervisor_routing_tools = [\n",
    "    route_to_socratic_question,\n",
    "    route_to_code_analysis,\n",
    "    route_to_code_explanation,\n",
    "    route_to_challenge_generator,\n",
    "    route_to_mcq_generator\n",
    "]\n",
    "\n",
    "# Supervisor Agent Setup\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a highly intelligent routing agent for a Socratic Python Tutor. Your task is to analyze\n",
    "the user's last message and the conversation history to determine the most appropriate next step\n",
    "in the learning process.\n",
    "\n",
    "You have access to several internal routing tools. Call exactly one of these tools to specify\n",
    "which specialized agent or flow should handle the user's request.\n",
    "\n",
    "Here are your routing rules:\n",
    "-   **Default:** For general questions, learning new topics, or continuing a Socratic dialogue, use `route_to_socratic_question`. This should be your most frequent choice.\n",
    "-   **Code Analysis:** If the user provides Python code and asks for debugging, feedback, review, or analysis, use `route_to_code_analysis` and pass the code.\n",
    "-   **Code Explanation:** If the user explicitly asks for an explanation of a specific Python concept, keyword, or error message, use `route_to_code_explanation` and pass the concept.\n",
    "-   **Challenge/Exercise:** If the user explicitly asks for a coding challenge, exercise, or fill-in-the-blanks, use `route_to_challenge_generator`.\n",
    "-   **MCQ:** If the user asks for a multiple-choice question or you determine an MCQ is a good way to test their understanding, use `route_to_mcq_generator`.\n",
    "\n",
    "Pay close attention to keywords and the overall intent. Your response MUST be a tool call.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "supervisor_runnable = supervisor_prompt | llm.bind_tools(supervisor_routing_tools)\n",
    "\n",
    "# Socratic Agent Setup (This is our main Socratic Questioning LLM)\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides Python code and asks for feedback, debugging, or analysis.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation of a Python concept, function, keyword, or error message.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge or a fill-in-the-blanks exercise.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question to test the user's understanding, especially if they are struggling or you want to quickly assess a concept.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "socratic_agent_runnable = socratic_prompt | llm # Socratic agent does not call tools directly, supervisor does\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_supervisor(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the supervisor to determine the next action/agent based on user intent.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Supervisor node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    # Pass the full state to the prompt for contextual awareness in routing\n",
    "    response = supervisor_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"]\n",
    "    })\n",
    "\n",
    "    # The supervisor is expected to call one of its internal routing tools.\n",
    "    # Extract the tool call and set the next_node and tool_input in the state.\n",
    "    if response.tool_calls:\n",
    "        # Access tool name from the 'function' dict within the tool_call dictionary\n",
    "        # tool_call is a dictionary here, not a ToolCall object\n",
    "        tool_call_dict = response.tool_calls[0]\n",
    "        tool_name = tool_call_dict.get('function', {}).get('name')\n",
    "        tool_input = tool_call_dict.get('function', {}).get('arguments') # Arguments are also nested\n",
    "\n",
    "        # IMPORTANT: Add a check here if tool_name is None, indicating a malformed tool call\n",
    "        if tool_name is None:\n",
    "            print(\"Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\")\n",
    "            return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "        next_node = tool_name.replace(\"route_to_\", \"\") # Extract node name (e.g., \"socratic_question\")\n",
    "        # logger.info(f\"Supervisor decided to route to: {next_node} with input: {tool_input}\") # Uncomment when logger is ready\n",
    "        return {\"messages\": [response], \"next_node\": next_node, \"tool_input\": tool_input}\n",
    "    else:\n",
    "        # Fallback if supervisor doesn't call a tool (shouldn't happen with proper prompt)\n",
    "        # Force it to the socratic question node\n",
    "        print(\"Warning: Supervisor LLM did not call a tool. Defaulting to socratic_question.\")\n",
    "        return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "\n",
    "def socratic_question_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the main Socratic LLM to ask questions or interpret tool outputs.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Socratic Question Node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    # Removed extraction of agent_thought as per user request\n",
    "    # thought = \"\"\n",
    "    # if response.content and response.content.startswith(\"Thought:\"):\n",
    "    #     parts = response.content.split(\"Thought:\", 1)\n",
    "    #     if len(parts) > 1:\n",
    "    #         thought = parts[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    # Return without agent_thought\n",
    "    return {\"messages\": [response], \"agent_thought\": \"\"} # Set agent_thought to empty string\n",
    "\n",
    "\n",
    "# --- New Nodes for Specialized Tools ---\n",
    "\n",
    "def code_analysis_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_analysis_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_analysis_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def code_explanation_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_explanation_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_explanation_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def challenge_generator_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the challenge_generator_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"challenge_generator_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "\n",
    "def generate_mcq_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node specifically for generating an MCQ via the mcq_agent tool.\n",
    "    This also handles setting the MCQ active state for main.py.\n",
    "    \"\"\"\n",
    "    # logger.info(\"MCQ Generation Node activated.\") # Uncomment when logger is ready\n",
    "    tool_name = state[\"next_node\"] # This comes from the supervisor's routing decision\n",
    "    tool_args = state[\"tool_input\"] # Should contain topic and difficulty from supervisor\n",
    "\n",
    "    # Manually find and execute the tool function using the map\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    mcq_raw_output = \"\"\n",
    "\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            mcq_raw_output = tool_function.invoke(tool_args)\n",
    "            mcq_data = json.loads(mcq_raw_output)\n",
    "            state[\"mcq_active\"] = True\n",
    "            state[\"mcq_question\"] = mcq_data[\"question\"]\n",
    "            state[\"mcq_options\"] = mcq_data[\"options\"]\n",
    "            state[\"mcq_correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "            # logger.info(\"MCQ details updated in state.\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            mcq_raw_output = f\"Error generating MCQ: {e}\"\n",
    "            # logger.error(f\"Error generating MCQ: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    \n",
    "    # Add a ToolMessage for the MCQ generation, which the Socratic LLM can interpret\n",
    "    # or simply for logging purposes in the graph flow.\n",
    "    # Provide a unique tool_call_id for the ToolMessage\n",
    "    return {\"messages\": [ToolMessage(content=mcq_raw_output, name=tool_name, tool_call_id=str(uuid.uuid4()))], **state}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def route_supervisor_output(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Conditional edge from the supervisor to determine the next node based on its decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Routing supervisor output. Next node: {state['next_node']}\") # Uncomment when logger is ready\n",
    "    if state[\"next_node\"] == \"socratic_question\":\n",
    "        return \"socratic_question_node\"\n",
    "    elif state[\"next_node\"] == \"mcq_generator\":\n",
    "        return \"generate_mcq_node\"\n",
    "    # All other specialized tools now have their own nodes\n",
    "    elif state[\"next_node\"] == \"code_analysis\":\n",
    "        return \"code_analysis_node\"\n",
    "    elif state[\"next_node\"] == \"code_explanation\":\n",
    "        return \"code_explanation_node\"\n",
    "    elif state[\"next_node\"] == \"challenge_generator\":\n",
    "        return \"challenge_generator_node\"\n",
    "    return \"socratic_question_node\" # Fallback to socratic question if unexpected\n",
    "\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_supervisor\", call_supervisor)\n",
    "workflow.add_node(\"socratic_question_node\", socratic_question_node) # Renamed from call_llm\n",
    "# Add new specialized tool nodes\n",
    "workflow.add_node(\"code_analysis_node\", code_analysis_node)\n",
    "workflow.add_node(\"code_explanation_node\", code_explanation_node)\n",
    "workflow.add_node(\"challenge_generator_node\", challenge_generator_node)\n",
    "workflow.add_node(\"generate_mcq_node\", generate_mcq_node)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_supervisor\")\n",
    "\n",
    "# Define the edges.\n",
    "# From supervisor, route conditionally\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_supervisor\",\n",
    "    route_supervisor_output,\n",
    "    {\n",
    "        \"socratic_question_node\": \"socratic_question_node\",\n",
    "        \"code_analysis_node\": \"code_analysis_node\",\n",
    "        \"code_explanation_node\": \"code_explanation_node\",\n",
    "        \"challenge_generator_node\": \"challenge_generator_node\",\n",
    "        \"generate_mcq_node\": \"generate_mcq_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After each specialized tool node, return to the socratic_question_node\n",
    "# for the Socratic LLM to interpret the tool's output and formulate a question.\n",
    "workflow.add_edge(\"code_analysis_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"code_explanation_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"challenge_generator_node\", \"socratic_question_node\")\n",
    "\n",
    "# After the socratic_question_node, the run ends. The main.py loop will then take user input.\n",
    "workflow.add_edge(\"socratic_question_node\", END)\n",
    "\n",
    "# After generating an MCQ, the run ends. Main.py handles the MCQ display and user input.\n",
    "workflow.add_edge(\"generate_mcq_node\", END)\n",
    "\n",
    "# Removed MemorySaver initialization and compilation\n",
    "# checkpointer = MemorySaver()\n",
    "socratic_graph = workflow.compile() # Compile without checkpointer\n",
    "\n",
    "# --- Temporary LLM Connection Test ---\n",
    "# This function will be called once when the module is imported to test LLM connectivity.\n",
    "def _test_llm_connection():\n",
    "    print(\"\\n--- Testing LLM Connection (Temporary) ---\")\n",
    "    try:\n",
    "        test_message = HumanMessage(content=\"Say hi!\")\n",
    "        response = llm.invoke([test_message])\n",
    "        print(f\"LLM Test Response: {response.content}\")\n",
    "        print(\"LLM connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Connection Test FAILED: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and network connection.\")\n",
    "    print(\"--- End LLM Connection Test ---\\n\")\n",
    "\n",
    "# --- Temporary Node Functionality Test ---\n",
    "# This function allows testing individual nodes with a sample state.\n",
    "def _test_node_functionality(node_name: str, initial_state: SocraticAgentState):\n",
    "    print(f\"\\n--- Testing Node: {node_name} ---\")\n",
    "    try:\n",
    "        # Get the node function from the workflow's nodes\n",
    "        # Access the runnable attribute to get the actual function\n",
    "        node_runnable = workflow.nodes[node_name].runnable\n",
    "\n",
    "        # Invoke the node's runnable with the provided initial state\n",
    "        result_state_update = node_runnable.invoke(initial_state)\n",
    "\n",
    "\n",
    "        print(f\"Node '{node_name}' executed successfully.\")\n",
    "        print(f\"Initial State (messages only): {[msg.content if hasattr(msg, 'content') else str(msg) for msg in initial_state['messages']]}\")\n",
    "        print(f\"Resulting State Update: {result_state_update}\")\n",
    "\n",
    "        # For nodes that return messages, let's print them\n",
    "        if 'messages' in result_state_update and result_state_update['messages']:\n",
    "            print(\"Messages returned by node:\")\n",
    "            for msg in result_state_update['messages']:\n",
    "                if isinstance(msg, BaseMessage):\n",
    "                    print(f\"  - Type: {msg.type}, Content: {msg.content[:50]}...\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        # Ensure tool_calls are correctly converted to ToolCall objects\n",
    "                        for tc in msg.tool_calls:\n",
    "                            # Direct access attributes, assuming it's a ToolCall object\n",
    "                            # The error was here when tc was a dict\n",
    "                            tool_name_debug = tc.function.name\n",
    "                            tool_args_debug = tc.function.arguments\n",
    "                            print(f\"    Tool Call: {tool_name_debug} with args {tool_args_debug}\")\n",
    "                else:\n",
    "                    print(f\"  - Raw: {msg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Node '{node_name}' Test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for node errors\n",
    "    print(f\"--- End Node Test: {node_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Call the LLM connection test immediately when this module is loaded\n",
    "_test_llm_connection()\n",
    "\n",
    "# Example usage of the new node testing function (uncomment to test specific nodes)\n",
    "# Ensure your GOOGLE_API_KEY is set for LLM-based nodes.\n",
    "\n",
    "# Test call_supervisor node\n",
    "# sample_supervisor_state = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"I need help with debugging this code: print('hello')\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Python Basics\",\n",
    "#     sub_topic=\"Introduction\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\", next_node=\"\", tool_input={}\n",
    "# )\n",
    "# _test_node_functionality(\"call_supervisor\", sample_supervisor_state)\n",
    "\n",
    "# Test socratic_question_node\n",
    "sample_socratic_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"What are variables?\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Variables\",\n",
    "    sub_topic=\"Definition\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"socratic_question_node\", sample_socratic_state)\n",
    "\n",
    "# Test code_analysis_node\n",
    "# sample_tool_state_code_analysis = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"debug this code: print('hello')\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Debugging\",\n",
    "#     sub_topic=\"Code Analysis\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\",\n",
    "#     next_node=\"code_analysis_agent\", # This should match the tool's actual name\n",
    "#     tool_input={\"code\": \"def my_func():\\n    pass\"}\n",
    "# )\n",
    "# _test_node_functionality(\"code_analysis_node\", sample_tool_state_code_analysis)\n",
    "\n",
    "# Test code_explanation_node\n",
    "# sample_tool_state_code_explanation = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"Explain loops in Python\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Python Basics\",\n",
    "#     sub_topic=\"Loops\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\",\n",
    "#     next_node=\"code_explanation_agent\",\n",
    "#     tool_input={\"concept\": \"loops\"}\n",
    "# )\n",
    "# _test_node_functionality(\"code_explanation_node\", sample_tool_state_code_explanation)\n",
    "\n",
    "# Test challenge_generator_node\n",
    "# sample_tool_state_challenge_generator = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"Give me a challenge on functions.\")],\n",
    "#     difficulty_level=\"intermediate\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Functions\",\n",
    "#     sub_topic=\"Challenges\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\",\n",
    "#     next_node=\"challenge_generator_agent\",\n",
    "#     tool_input={\"topic\": \"functions\", \"difficulty\": \"intermediate\"}\n",
    "# )\n",
    "# _test_node_functionality(\"challenge_generator_node\", sample_tool_state_challenge_generator)\n",
    "\n",
    "# Test generate_mcq_node\n",
    "# sample_mcq_state = SocraticAgentState(\n",
    "#     messages=[HumanMessage(content=\"Give me an MCQ on functions.\")],\n",
    "#     difficulty_level=\"beginner\",\n",
    "#     user_struggle_count=0,\n",
    "#     topic=\"Functions\",\n",
    "#     sub_topic=\"MCQ\",\n",
    "#     mcq_active=False,\n",
    "#     mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "#     agent_thought=\"\",\n",
    "#     next_node=\"mcq_agent\", # Supervisor would set this\n",
    "#     tool_input={\"topic\": \"functions\", \"difficulty\": \"beginner\"} # Supervisor would set this\n",
    "# )\n",
    "# _test_node_functionality(\"generate_mcq_node\", sample_mcq_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "df9db0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM Connection (Temporary) ---\n",
      "LLM Test Response: Hi there! How can I help you today?\n",
      "LLM connection successful!\n",
      "--- End LLM Connection Test ---\n",
      "\n",
      "\n",
      "--- Testing Node: call_supervisor ---\n",
      "Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\n",
      "Node 'call_supervisor' executed successfully.\n",
      "Initial State (messages only): [\"I need help with debugging this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'route_to_code_analysis', 'arguments': '{\"code\": \"print(\\'hello\\')\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--3ad70e8a-34cb-4afe-9681-81ccf669845c-0', tool_calls=[{'name': 'route_to_code_analysis', 'args': {'code': \"print('hello')\"}, 'id': '55900851-de5d-45f1-b729-7fb3e01bfb45', 'type': 'tool_call'}], usage_metadata={'input_tokens': 533, 'output_tokens': 12, 'total_tokens': 545, 'input_token_details': {'cache_read': 0}})], 'next_node': 'socratic_question'}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: ...\n",
      "Node 'call_supervisor' Test FAILED: 'dict' object has no attribute 'function'\n",
      "--- End Node Test: call_supervisor ---\n",
      "\n",
      "\n",
      "--- Testing Node: socratic_question_node ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12503/1375230495.py\", line 471, in _test_node_functionality\n",
      "    tool_name_debug = tc.function.name\n",
      "                      ^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'function'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'socratic_question_node' executed successfully.\n",
      "Initial State (messages only): ['What are variables?']\n",
      "Resulting State Update: {'messages': [AIMessage(content='In simple terms, what do you think a variable represents in programming?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ae7bc57e-9d17-48dd-9853-7d404a0f2f14-0', usage_metadata={'input_tokens': 376, 'output_tokens': 15, 'total_tokens': 391, 'input_token_details': {'cache_read': 0}})], 'agent_thought': ''}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: In simple terms, what do you think a variable repr...\n",
      "--- End Node Test: socratic_question_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_analysis_node ---\n",
      "Node 'code_analysis_node' executed successfully.\n",
      "Initial State (messages only): [\"debug this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Code Analysis Result: For the code snippet 'def my_func():\\n    pass', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\", name='code_analysis_agent', tool_call_id='4f6ceae7-b3c3-497f-a2d2-cd0dcf1e4828')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Code Analysis Result: For the code snippet 'def my...\n",
      "--- End Node Test: code_analysis_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_explanation_node ---\n",
      "Node 'code_explanation_node' executed successfully.\n",
      "Initial State (messages only): ['Explain loops in Python']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Explanation Result: The concept of 'loops' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\", name='code_explanation_agent', tool_call_id='ae621c07-4ff6-4341-8ce3-299a636250ab')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Explanation Result: The concept of 'loops' in Pyth...\n",
      "--- End Node Test: code_explanation_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: challenge_generator_node ---\n",
      "Node 'challenge_generator_node' executed successfully.\n",
      "Initial State (messages only): ['Give me a challenge on functions.']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Challenge Result: For 'functions' at 'intermediate' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\", name='challenge_generator_agent', tool_call_id='3ffe778b-78db-4991-8ed0-eacb7e1e16d7')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Challenge Result: For 'functions' at 'intermediate...\n",
      "--- End Node Test: challenge_generator_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: generate_mcq_node ---\n",
      "Node 'generate_mcq_node' executed successfully.\n",
      "Initial State (messages only): ['Give me an MCQ on functions.']\n",
      "Resulting State Update: {'messages': [HumanMessage(content='Give me an MCQ on functions.', additional_kwargs={}, response_metadata={})], 'difficulty_level': 'beginner', 'user_struggle_count': 0, 'topic': 'Functions', 'sub_topic': 'MCQ', 'mcq_active': True, 'mcq_question': 'Which of the following operations would lead to an `IndentationError` in Python?', 'mcq_options': ['A) Missing a colon after a function definition', 'B) Inconsistent use of spaces and tabs for indentation', 'C) Using a reserved keyword as a variable name', 'D) Forgetting a closing parenthesis'], 'mcq_correct_answer': 'B', 'agent_thought': '', 'next_node': 'mcq_agent', 'tool_input': {'topic': 'functions', 'difficulty': 'beginner'}}\n",
      "Messages returned by node:\n",
      "  - Type: human, Content: Give me an MCQ on functions....\n",
      "--- End Node Test: generate_mcq_node ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool # Ensure tool decorator is imported\n",
    "import json\n",
    "import uuid # Import uuid for generating unique IDs\n",
    "\n",
    "# Removed MemorySaver import\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Import the logging utility (assuming logger.py will be created later)\n",
    "# from logger import setup_logger\n",
    "# logger = setup_logger() # Uncomment when logger.py is ready\n",
    "\n",
    "# --- Configuration for Memory Management ---\n",
    "MAX_MESSAGES_IN_CONTEXT = 10 # Keep the last 10 messages in the context window\n",
    "# This includes both HumanMessage and AIMessage. Adjust as needed based on LLM context limits.\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "        # Added for supervisor routing\n",
    "        next_node: str # The next node the supervisor has decided to route to\n",
    "        tool_input: dict # Input arguments for the tool if a tool is routed to\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], lambda x, y: x + y] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node: str\n",
    "    tool_input: dict\n",
    "\n",
    "\n",
    "# --- 2. Initialize the LLMs and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM (used for both Socratic and Supervisor agents)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7) # Changed model to gemini-2.0-flash\n",
    "\n",
    "# --- Define User-Facing Simulated Agent Tools ---\n",
    "# These are the actual tools that will perform specific tasks.\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    The output is raw analysis, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    # logger.info(f\"Executing Code Analysis for: {code[:50]}...\") # Uncomment when logger is ready\n",
    "    return f\"Code Analysis Result: For the code snippet '{code}', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    The output is raw explanation, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    # logger.info(f\"Executing Code Explanation for: {concept}\") # Uncomment when logger is ready\n",
    "    return f\"Explanation Result: The concept of '{concept}' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    The output is the challenge, which the Socratic agent will present.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    # logger.info(f\"Executing Challenge Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    return f\"Challenge Result: For '{topic}' at '{difficulty}' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specifically for MCQ generation.\n",
    "    # logger.info(f\"Executing MCQ Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    mcq_data = {\n",
    "        \"question\": f\"Which of the following operations would lead to an `IndentationError` in Python?\",\n",
    "        \"options\": [\"A) Missing a colon after a function definition\", \"B) Inconsistent use of spaces and tabs for indentation\", \"C) Using a reserved keyword as a variable name\", \"D) Forgetting a closing parenthesis\"],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "    return json.dumps(mcq_data)\n",
    "\n",
    "# List of all user-facing tools\n",
    "# Changed to a dictionary for direct lookup by name\n",
    "user_facing_tools_map = {\n",
    "    code_analysis_agent.name: code_analysis_agent,\n",
    "    code_explanation_agent.name: code_explanation_agent,\n",
    "    challenge_generator_agent.name: challenge_generator_agent,\n",
    "    mcq_agent.name: mcq_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define Internal Supervisor Tools (for routing decisions) ---\n",
    "# These are \"tools\" the supervisor LLM will call to indicate its routing decision.\n",
    "@tool\n",
    "def route_to_socratic_question(query: str = None) -> str:\n",
    "    \"\"\"Routes the conversation to the main Socratic Questioning agent for general teaching or follow-up.\n",
    "    This is the default route for general queries, concept discussions, and after tool outputs.\n",
    "    Optionally includes a follow-up query for the Socratic agent if the intent is specific.\n",
    "    \"\"\"\n",
    "    return \"socratic_question\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_analysis(code: str) -> str:\n",
    "    \"\"\"Routes to the Code Analysis agent for debugging or code review. Requires the code snippet.\"\"\"\n",
    "    return \"code_analysis\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_explanation(concept: str) -> str:\n",
    "    \"\"\"Routes to the Code Explanation agent to explain a specific concept, keyword, or error. Requires the concept.\"\"\"\n",
    "    return \"code_explanation\"\n",
    "\n",
    "@tool\n",
    "def route_to_challenge_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the Challenge Generator agent to create a coding challenge. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"challenge_generator\"\n",
    "\n",
    "@tool\n",
    "def route_to_mcq_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the MCQ Generator agent to create a multiple-choice question. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"mcq_generator\"\n",
    "\n",
    "# List of all internal routing tools available to the Supervisor\n",
    "supervisor_routing_tools = [\n",
    "    route_to_socratic_question,\n",
    "    route_to_code_analysis,\n",
    "    route_to_code_explanation,\n",
    "    route_to_challenge_generator,\n",
    "    route_to_mcq_generator\n",
    "]\n",
    "\n",
    "# Supervisor Agent Setup\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "supervisor_runnable = supervisor_prompt | llm.bind_tools(supervisor_routing_tools)\n",
    "\n",
    "# Socratic Agent Setup (This is our main Socratic Questioning LLM)\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Interpret Tool Outputs Socratically:** If a tool provides information (e.g., Code Analysis Result, Explanation Result, Challenge Result), your task is to *process that information* and turn it into a Socratic question or guided step for the user. Do not just relay the tool's output directly.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub-topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **Strict Output Format:** Your response MUST be a direct Socratic question. Do NOT include any \"Thought:\" prefix, internal monologue, conversational filler, or anything other than the question itself. Ensure the question is the ONLY content.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "socratic_agent_runnable = socratic_prompt | llm # Socratic agent does not call tools directly, supervisor does\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_supervisor(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the supervisor to determine the next action/agent based on user intent.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Supervisor node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    # Pass the full state to the prompt for contextual awareness in routing\n",
    "    response = supervisor_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"]\n",
    "    })\n",
    "\n",
    "    # The supervisor is expected to call one of its internal routing tools.\n",
    "    # Extract the tool call and set the next_node and tool_input in the state.\n",
    "    if response.tool_calls:\n",
    "        # Access tool name from the 'function' dict within the tool_call dictionary\n",
    "        # tool_call is a dictionary here, not a ToolCall object\n",
    "        tool_call_dict = response.tool_calls[0]\n",
    "        tool_name = tool_call_dict.get('function', {}).get('name')\n",
    "        tool_input = tool_call_dict.get('function', {}).get('arguments') # Arguments are also nested\n",
    "\n",
    "        # IMPORTANT: Add a check here if tool_name is None, indicating a malformed tool call\n",
    "        if tool_name is None:\n",
    "            print(\"Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\")\n",
    "            return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "        next_node = tool_name.replace(\"route_to_\", \"\") # Extract node name (e.g., \"socratic_question\")\n",
    "        # logger.info(f\"Supervisor decided to route to: {next_node} with input: {tool_input}\") # Uncomment when logger is ready\n",
    "        return {\"messages\": [response], \"next_node\": next_node, \"tool_input\": tool_input}\n",
    "    else:\n",
    "        # Fallback if supervisor doesn't call a tool (shouldn't happen with proper prompt)\n",
    "        # Force it to the socratic question node\n",
    "        print(\"Warning: Supervisor LLM did not call a tool. Defaulting to socratic_question.\")\n",
    "        return {\"messages\": [response], \"next_node\": \"socratic_question\"}\n",
    "\n",
    "\n",
    "def socratic_question_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the main Socratic LLM to ask questions or interpret tool outputs.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Socratic Question Node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    # Removed extraction of agent_thought as per user request\n",
    "    # thought = \"\"\n",
    "    # if response.content and response.content.startswith(\"Thought:\"):\n",
    "    #     parts = response.content.split(\"Thought:\", 1)\n",
    "    #     if len(parts) > 1:\n",
    "    #         thought = parts[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    # Return without agent_thought\n",
    "    return {\"messages\": [response], \"agent_thought\": \"\"} # Set agent_thought to empty string\n",
    "\n",
    "\n",
    "# --- New Nodes for Specialized Tools ---\n",
    "\n",
    "def code_analysis_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_analysis_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_analysis_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def code_explanation_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_explanation_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_explanation_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def challenge_generator_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the challenge_generator_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"challenge_generator_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "\n",
    "def generate_mcq_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node specifically for generating an MCQ via the mcq_agent tool.\n",
    "    This also handles setting the MCQ active state for main.py.\n",
    "    \"\"\"\n",
    "    # logger.info(\"MCQ Generation Node activated.\") # Uncomment when logger is ready\n",
    "    tool_name = state[\"next_node\"] # This comes from the supervisor's routing decision\n",
    "    tool_args = state[\"tool_input\"] # Should contain topic and difficulty from supervisor\n",
    "\n",
    "    # Manually find and execute the tool function using the map\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    mcq_raw_output = \"\"\n",
    "\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            mcq_raw_output = tool_function.invoke(tool_args)\n",
    "            mcq_data = json.loads(mcq_raw_output)\n",
    "            state[\"mcq_active\"] = True\n",
    "            state[\"mcq_question\"] = mcq_data[\"question\"]\n",
    "            state[\"mcq_options\"] = mcq_data[\"options\"]\n",
    "            state[\"mcq_correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "            # logger.info(\"MCQ details updated in state.\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            mcq_raw_output = f\"Error generating MCQ: {e}\"\n",
    "            # logger.error(f\"Error generating MCQ: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    \n",
    "    # Add a ToolMessage for the MCQ generation, which the Socratic LLM can interpret\n",
    "    # or simply for logging purposes in the graph flow.\n",
    "    # Provide a unique tool_call_id for the ToolMessage\n",
    "    return {\"messages\": [ToolMessage(content=mcq_raw_output, name=tool_name, tool_call_id=str(uuid.uuid4()))], **state}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def route_supervisor_output(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Conditional edge from the supervisor to determine the next node based on its decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Routing supervisor output. Next node: {state['next_node']}\") # Uncomment when logger is ready\n",
    "    if state[\"next_node\"] == \"socratic_question\":\n",
    "        return \"socratic_question_node\"\n",
    "    elif state[\"next_node\"] == \"mcq_generator\":\n",
    "        return \"generate_mcq_node\"\n",
    "    # All other specialized tools now have their own nodes\n",
    "    elif state[\"next_node\"] == \"code_analysis\":\n",
    "        return \"code_analysis_node\"\n",
    "    elif state[\"next_node\"] == \"code_explanation\":\n",
    "        return \"code_explanation_node\"\n",
    "    elif state[\"next_node\"] == \"challenge_generator\":\n",
    "        return \"challenge_generator_node\"\n",
    "    return \"socratic_question_node\" # Fallback to socratic question if unexpected\n",
    "\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_supervisor\", call_supervisor)\n",
    "workflow.add_node(\"socratic_question_node\", socratic_question_node) # Renamed from call_llm\n",
    "# Add new specialized tool nodes\n",
    "workflow.add_node(\"code_analysis_node\", code_analysis_node)\n",
    "workflow.add_node(\"code_explanation_node\", code_explanation_node)\n",
    "workflow.add_node(\"challenge_generator_node\", challenge_generator_node)\n",
    "workflow.add_node(\"generate_mcq_node\", generate_mcq_node)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_supervisor\")\n",
    "\n",
    "# Define the edges.\n",
    "# From supervisor, route conditionally\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_supervisor\",\n",
    "    route_supervisor_output,\n",
    "    {\n",
    "        \"socratic_question_node\": \"socratic_question_node\",\n",
    "        \"code_analysis_node\": \"code_analysis_node\",\n",
    "        \"code_explanation_node\": \"code_explanation_node\",\n",
    "        \"challenge_generator_node\": \"challenge_generator_node\",\n",
    "        \"generate_mcq_node\": \"generate_mcq_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After each specialized tool node, return to the socratic_question_node\n",
    "# for the Socratic LLM to interpret the tool's output and formulate a question.\n",
    "workflow.add_edge(\"code_analysis_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"code_explanation_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"challenge_generator_node\", \"socratic_question_node\")\n",
    "\n",
    "# After the socratic_question_node, the run ends. The main.py loop will then take user input.\n",
    "workflow.add_edge(\"socratic_question_node\", END)\n",
    "\n",
    "# After generating an MCQ, the run ends. Main.py handles the MCQ display and user input.\n",
    "workflow.add_edge(\"generate_mcq_node\", END)\n",
    "\n",
    "# Removed MemorySaver initialization and compilation\n",
    "# checkpointer = MemorySaver()\n",
    "socratic_graph = workflow.compile() # Compile without checkpointer\n",
    "\n",
    "# --- Temporary LLM Connection Test ---\n",
    "# This function will be called once when the module is imported to test LLM connectivity.\n",
    "def _test_llm_connection():\n",
    "    print(\"\\n--- Testing LLM Connection (Temporary) ---\")\n",
    "    try:\n",
    "        test_message = HumanMessage(content=\"Say hi!\")\n",
    "        response = llm.invoke([test_message])\n",
    "        print(f\"LLM Test Response: {response.content}\")\n",
    "        print(\"LLM connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Connection Test FAILED: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and network connection.\")\n",
    "    print(\"--- End LLM Connection Test ---\\n\")\n",
    "\n",
    "# --- Temporary Node Functionality Test ---\n",
    "# This function allows testing individual nodes with a sample state.\n",
    "def _test_node_functionality(node_name: str, initial_state: SocraticAgentState):\n",
    "    print(f\"\\n--- Testing Node: {node_name} ---\")\n",
    "    try:\n",
    "        # Get the node function from the workflow's nodes\n",
    "        # Access the runnable attribute to get the actual function\n",
    "        node_runnable = workflow.nodes[node_name].runnable\n",
    "\n",
    "        # Invoke the node's runnable with the provided initial state\n",
    "        result_state_update = node_runnable.invoke(initial_state)\n",
    "\n",
    "\n",
    "        print(f\"Node '{node_name}' executed successfully.\")\n",
    "        print(f\"Initial State (messages only): {[msg.content if hasattr(msg, 'content') else str(msg) for msg in initial_state['messages']]}\")\n",
    "        print(f\"Resulting State Update: {result_state_update}\")\n",
    "\n",
    "        # For nodes that return messages, let's print them\n",
    "        if 'messages' in result_state_update and result_state_update['messages']:\n",
    "            print(\"Messages returned by node:\")\n",
    "            for msg in result_state_update['messages']:\n",
    "                if isinstance(msg, BaseMessage):\n",
    "                    print(f\"  - Type: {msg.type}, Content: {msg.content[:50]}...\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        # Ensure tool_calls are correctly converted to ToolCall objects\n",
    "                        for tc in msg.tool_calls:\n",
    "                            # Direct access attributes, assuming it's a ToolCall object\n",
    "                            # The error was here when tc was a dict\n",
    "                            tool_name_debug = tc.function.name\n",
    "                            tool_args_debug = tc.function.arguments\n",
    "                            print(f\"    Tool Call: {tool_name_debug} with args {tool_args_debug}\")\n",
    "                else:\n",
    "                    print(f\"  - Raw: {msg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Node '{node_name}' Test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for node errors\n",
    "    print(f\"--- End Node Test: {node_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Call the LLM connection test immediately when this module is loaded\n",
    "_test_llm_connection()\n",
    "\n",
    "# Example usage of the new node testing function (uncomment to test specific nodes)\n",
    "# Ensure your GOOGLE_API_KEY is set for LLM-based nodes.\n",
    "\n",
    "# Test call_supervisor node\n",
    "sample_supervisor_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"I need help with debugging this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Introduction\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"call_supervisor\", sample_supervisor_state)\n",
    "\n",
    "# Test socratic_question_node\n",
    "sample_socratic_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"What are variables?\")], # Removed the AIMessage from here\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Variables\",\n",
    "    sub_topic=\"Definition\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"socratic_question_node\", sample_socratic_state)\n",
    "\n",
    "# Test code_analysis_node\n",
    "sample_tool_state_code_analysis = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"debug this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Debugging\",\n",
    "    sub_topic=\"Code Analysis\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_analysis_agent\", # This should match the tool's actual name\n",
    "    tool_input={\"code\": \"def my_func():\\n    pass\"}\n",
    ")\n",
    "_test_node_functionality(\"code_analysis_node\", sample_tool_state_code_analysis)\n",
    "\n",
    "# Test code_explanation_node\n",
    "sample_tool_state_code_explanation = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Explain loops in Python\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Loops\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_explanation_agent\",\n",
    "    tool_input={\"concept\": \"loops\"}\n",
    ")\n",
    "_test_node_functionality(\"code_explanation_node\", sample_tool_state_code_explanation)\n",
    "\n",
    "# Test challenge_generator_node\n",
    "sample_tool_state_challenge_generator = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me a challenge on functions.\")],\n",
    "    difficulty_level=\"intermediate\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"Challenges\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"challenge_generator_agent\",\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"intermediate\"}\n",
    ")\n",
    "_test_node_functionality(\"challenge_generator_node\", sample_tool_state_challenge_generator)\n",
    "\n",
    "# Test generate_mcq_node\n",
    "sample_mcq_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me an MCQ on functions.\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"MCQ\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"mcq_agent\", # Supervisor would set this\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"beginner\"} # Supervisor would set this\n",
    ")\n",
    "_test_node_functionality(\"generate_mcq_node\", sample_mcq_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd67f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM Connection (Temporary) ---\n",
      "LLM Test Response: Hi there! How can I help you today?\n",
      "LLM connection successful!\n",
      "--- End LLM Connection Test ---\n",
      "\n",
      "\n",
      "--- Testing Node: call_supervisor ---\n",
      "Raw tool_call structure: {'name': 'route_to_code_analysis', 'args': {'code': \"print('hello')\"}, 'id': 'f769684d-c821-4bb3-a0e0-9b54708ee525', 'type': 'tool_call'}\n",
      "Node 'call_supervisor' executed successfully.\n",
      "Initial State (messages only): [\"I need help with debugging this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'route_to_code_analysis', 'arguments': '{\"code\": \"print(\\'hello\\')\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--96b3b31b-58fd-4f3f-9edb-8d7143804163-0', tool_calls=[{'name': 'route_to_code_analysis', 'args': {'code': \"print('hello')\"}, 'id': 'f769684d-c821-4bb3-a0e0-9b54708ee525', 'type': 'tool_call'}], usage_metadata={'input_tokens': 533, 'output_tokens': 12, 'total_tokens': 545, 'input_token_details': {'cache_read': 0}})], 'next_node': 'code_analysis', 'tool_input': {'code': \"print('hello')\"}}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: ...\n",
      "    Tool Call: route_to_code_analysis with args {'code': \"print('hello')\"}\n",
      "--- End Node Test: call_supervisor ---\n",
      "\n",
      "\n",
      "--- Testing Node: socratic_question_node ---\n",
      "Node 'socratic_question_node' executed successfully.\n",
      "Initial State (messages only): ['What are variables?']\n",
      "Resulting State Update: {'messages': [AIMessage(content='In simple terms, what do you think a variable represents in programming?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--5a5d1743-8064-4a0c-82d9-17c122bf2ebf-0', usage_metadata={'input_tokens': 376, 'output_tokens': 15, 'total_tokens': 391, 'input_token_details': {'cache_read': 0}})], 'agent_thought': ''}\n",
      "Messages returned by node:\n",
      "  - Type: ai, Content: In simple terms, what do you think a variable repr...\n",
      "--- End Node Test: socratic_question_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_analysis_node ---\n",
      "Node 'code_analysis_node' executed successfully.\n",
      "Initial State (messages only): [\"debug this code: print('hello')\"]\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Code Analysis Result: For the code snippet 'def my_func():\\n    pass', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\", name='code_analysis_agent', tool_call_id='0ee343ef-ffbf-4182-afd1-a8db610eb840')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Code Analysis Result: For the code snippet 'def my...\n",
      "--- End Node Test: code_analysis_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: code_explanation_node ---\n",
      "Node 'code_explanation_node' executed successfully.\n",
      "Initial State (messages only): ['Explain loops in Python']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Explanation Result: The concept of 'loops' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\", name='code_explanation_agent', tool_call_id='0b8b64f8-5f23-4929-a2d2-80b4a82ebd68')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Explanation Result: The concept of 'loops' in Pyth...\n",
      "--- End Node Test: code_explanation_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: challenge_generator_node ---\n",
      "Node 'challenge_generator_node' executed successfully.\n",
      "Initial State (messages only): ['Give me a challenge on functions.']\n",
      "Resulting State Update: {'messages': [ToolMessage(content=\"Challenge Result: For 'functions' at 'intermediate' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\", name='challenge_generator_agent', tool_call_id='a18f16b4-3773-4cfb-b0f8-de17327aada1')]}\n",
      "Messages returned by node:\n",
      "  - Type: tool, Content: Challenge Result: For 'functions' at 'intermediate...\n",
      "--- End Node Test: challenge_generator_node ---\n",
      "\n",
      "\n",
      "--- Testing Node: generate_mcq_node ---\n",
      "Node 'generate_mcq_node' executed successfully.\n",
      "Initial State (messages only): ['Give me an MCQ on functions.']\n",
      "Resulting State Update: {'messages': [HumanMessage(content='Give me an MCQ on functions.', additional_kwargs={}, response_metadata={})], 'difficulty_level': 'beginner', 'user_struggle_count': 0, 'topic': 'Functions', 'sub_topic': 'MCQ', 'mcq_active': True, 'mcq_question': 'Which of the following operations would lead to an `IndentationError` in Python?', 'mcq_options': ['A) Missing a colon after a function definition', 'B) Inconsistent use of spaces and tabs for indentation', 'C) Using a reserved keyword as a variable name', 'D) Forgetting a closing parenthesis'], 'mcq_correct_answer': 'B', 'agent_thought': '', 'next_node': 'mcq_agent', 'tool_input': {'topic': 'functions', 'difficulty': 'beginner'}}\n",
      "Messages returned by node:\n",
      "  - Type: human, Content: Give me an MCQ on functions....\n",
      "--- End Node Test: generate_mcq_node ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool # Ensure tool decorator is imported\n",
    "import json\n",
    "import uuid # Import uuid for generating unique IDs\n",
    "\n",
    "# Removed MemorySaver import\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Import the logging utility (assuming logger.py will be created later)\n",
    "# from logger import setup_logger\n",
    "# logger = setup_logger() # Uncomment when logger.py is ready\n",
    "\n",
    "# --- Configuration for Memory Management ---\n",
    "MAX_MESSAGES_IN_CONTEXT = 10 # Keep the last 10 messages in the context window\n",
    "# This includes both HumanMessage and AIMessage. Adjust as needed based on LLM context limits.\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "        # Added for supervisor routing\n",
    "        next_node: str # The next node the supervisor has decided to route to\n",
    "        tool_input: dict # Input arguments for the tool if a tool is routed to\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], lambda x, y: x + y] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node: str\n",
    "    tool_input: dict\n",
    "\n",
    "\n",
    "# --- 2. Initialize the LLMs and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM (used for both Socratic and Supervisor agents)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7) # Changed model to gemini-2.0-flash\n",
    "\n",
    "# --- Define User-Facing Simulated Agent Tools ---\n",
    "# These are the actual tools that will perform specific tasks.\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    The output is raw analysis, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    # logger.info(f\"Executing Code Analysis for: {code[:50]}...\") # Uncomment when logger is ready\n",
    "    return f\"Code Analysis Result: For the code snippet '{code}', a potential area to explore is its efficiency in handling large inputs, or error handling. Also, consider adding comments for clarity.\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    The output is raw explanation, which the Socratic agent will then use to ask questions.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    # logger.info(f\"Executing Code Explanation for: {concept}\") # Uncomment when logger is ready\n",
    "    return f\"Explanation Result: The concept of '{concept}' in Python generally refers to [brief factual summary]. For instance, if it's about 'loops', it's about repetitive execution. If it's 'objects', it's about data and behavior bundling.\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    The output is the challenge, which the Socratic agent will present.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    # logger.info(f\"Executing Challenge Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    return f\"Challenge Result: For '{topic}' at '{difficulty}' difficulty: 'Write a Python function that takes a list of numbers and returns the sum of all **odd** numbers.' How would you approach solving this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specifically for MCQ generation.\n",
    "    # logger.info(f\"Executing MCQ Generation for: {topic}, Difficulty: {difficulty}\") # Uncomment when logger is ready\n",
    "    mcq_data = {\n",
    "        \"question\": f\"Which of the following operations would lead to an `IndentationError` in Python?\",\n",
    "        \"options\": [\"A) Missing a colon after a function definition\", \"B) Inconsistent use of spaces and tabs for indentation\", \"C) Using a reserved keyword as a variable name\", \"D) Forgetting a closing parenthesis\"],\n",
    "        \"correct_answer\": \"B\"\n",
    "    }\n",
    "    return json.dumps(mcq_data)\n",
    "\n",
    "# List of all user-facing tools\n",
    "# Changed to a dictionary for direct lookup by name\n",
    "user_facing_tools_map = {\n",
    "    code_analysis_agent.name: code_analysis_agent,\n",
    "    code_explanation_agent.name: code_explanation_agent,\n",
    "    challenge_generator_agent.name: challenge_generator_agent,\n",
    "    mcq_agent.name: mcq_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# --- Define Internal Supervisor Tools (for routing decisions) ---\n",
    "# These are \"tools\" the supervisor LLM will call to indicate its routing decision.\n",
    "@tool\n",
    "def route_to_socratic_question(query: str = None) -> str:\n",
    "    \"\"\"Routes the conversation to the main Socratic Questioning agent for general teaching or follow-up.\n",
    "    This is the default route for general queries, concept discussions, and after tool outputs.\n",
    "    Optionally includes a follow-up query for the Socratic agent if the intent is specific.\n",
    "    \"\"\"\n",
    "    return \"socratic_question\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_analysis(code: str) -> str:\n",
    "    \"\"\"Routes to the Code Analysis agent for debugging or code review. Requires the code snippet.\"\"\"\n",
    "    return \"code_analysis\"\n",
    "\n",
    "@tool\n",
    "def route_to_code_explanation(concept: str) -> str:\n",
    "    \"\"\"Routes to the Code Explanation agent to explain a specific concept, keyword, or error. Requires the concept.\"\"\"\n",
    "    return \"code_explanation\"\n",
    "\n",
    "@tool\n",
    "def route_to_challenge_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the Challenge Generator agent to create a coding challenge. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"challenge_generator\"\n",
    "\n",
    "@tool\n",
    "def route_to_mcq_generator(topic: str = None, difficulty: str = None) -> str:\n",
    "    \"\"\"Routes to the MCQ Generator agent to create a multiple-choice question. Optionally specify topic and difficulty.\"\"\"\n",
    "    return \"mcq_generator\"\n",
    "\n",
    "# List of all internal routing tools available to the Supervisor\n",
    "supervisor_routing_tools = [\n",
    "    route_to_socratic_question,\n",
    "    route_to_code_analysis,\n",
    "    route_to_code_explanation,\n",
    "    route_to_challenge_generator,\n",
    "    route_to_mcq_generator\n",
    "]\n",
    "\n",
    "# Supervisor Agent Setup\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", supervisor_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "supervisor_runnable = supervisor_prompt | llm.bind_tools(supervisor_routing_tools)\n",
    "\n",
    "# Socratic Agent Setup (This is our main Socratic Questioning LLM)\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Interpret Tool Outputs Socratically:** If a tool provides information (e.g., Code Analysis Result, Explanation Result, Challenge Result), your task is to *process that information* and turn it into a Socratic question or guided step for the user. Do not just relay the tool's output directly.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub-topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **Strict Output Format:** Your response MUST be a direct Socratic question. Do NOT include any \"Thought:\" prefix, internal monologue, conversational filler, or anything other than the question itself. Ensure the question is the ONLY content.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub-topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "socratic_agent_runnable = socratic_prompt | llm # Socratic agent does not call tools directly, supervisor does\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_supervisor(state: SocraticAgentState):\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:]\n",
    "    response = supervisor_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"]\n",
    "    })\n",
    "\n",
    "    tool_name = None\n",
    "    tool_input = {}\n",
    "\n",
    "    if response.tool_calls:\n",
    "        tool_call_item = response.tool_calls[0]\n",
    "\n",
    "        # Log structure for debugging\n",
    "        print(\"Raw tool_call structure:\", tool_call_item)\n",
    "\n",
    "        if isinstance(tool_call_item, dict):\n",
    "            tool_name = tool_call_item.get(\"name\", None)\n",
    "            tool_input = tool_call_item.get(\"args\", {})\n",
    "        elif hasattr(tool_call_item, \"function\"):\n",
    "            tool_name = tool_call_item.function.name\n",
    "            tool_input = tool_call_item.function.arguments\n",
    "        else:\n",
    "            print(\"Warning: Unexpected tool call structure.\")\n",
    "            tool_name = None\n",
    "\n",
    "    if not tool_name:\n",
    "        print(\"Warning: Supervisor LLM returned a malformed tool call (name is None). Defaulting to socratic_question.\")\n",
    "        return {\n",
    "            \"messages\": [response],\n",
    "            \"next_node\": \"socratic_question\",\n",
    "            \"tool_input\": {}\n",
    "        }\n",
    "\n",
    "    next_node = tool_name.replace(\"route_to_\", \"\")\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"next_node\": next_node,\n",
    "        \"tool_input\": tool_input\n",
    "    }\n",
    "\n",
    "def socratic_question_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node for the main Socratic LLM to ask questions or interpret tool outputs.\n",
    "    \"\"\"\n",
    "    # logger.info(\"Socratic Question Node activated.\") # Uncomment when logger is ready\n",
    "    messages = state[\"messages\"][-MAX_MESSAGES_IN_CONTEXT:] # Apply truncation here\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    # Removed extraction of agent_thought as per user request\n",
    "    # thought = \"\"\n",
    "    # if response.content and response.content.startswith(\"Thought:\"):\n",
    "    #     parts = response.content.split(\"Thought:\", 1)\n",
    "    #     if len(parts) > 1:\n",
    "    #         thought = parts[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    # Return without agent_thought\n",
    "    return {\"messages\": [response], \"agent_thought\": \"\"} # Set agent_thought to empty string\n",
    "\n",
    "\n",
    "# --- New Nodes for Specialized Tools ---\n",
    "\n",
    "def code_analysis_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_analysis_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_analysis_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def code_explanation_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the code_explanation_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"code_explanation_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "def challenge_generator_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute the challenge_generator_agent tool.\n",
    "    \"\"\"\n",
    "    tool_name = \"challenge_generator_agent\"\n",
    "    tool_args = state[\"tool_input\"]\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    tool_output = \"\"\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            tool_output = tool_function.invoke(tool_args)\n",
    "        except Exception as e:\n",
    "            tool_output = f\"Error executing tool {tool_name}: {e}\"\n",
    "    else:\n",
    "        tool_output = f\"Error: Specialized tool '{tool_name}' not found.\"\n",
    "    return {\"messages\": [ToolMessage(content=tool_output, name=tool_name, tool_call_id=str(uuid.uuid4()))]}\n",
    "\n",
    "\n",
    "def generate_mcq_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node specifically for generating an MCQ via the mcq_agent tool.\n",
    "    This also handles setting the MCQ active state for main.py.\n",
    "    \"\"\"\n",
    "    # logger.info(\"MCQ Generation Node activated.\") # Uncomment when logger is ready\n",
    "    tool_name = state[\"next_node\"] # This comes from the supervisor's routing decision\n",
    "    tool_args = state[\"tool_input\"] # Should contain topic and difficulty from supervisor\n",
    "\n",
    "    # Manually find and execute the tool function using the map\n",
    "    tool_function = user_facing_tools_map.get(tool_name)\n",
    "    mcq_raw_output = \"\"\n",
    "\n",
    "    if tool_function:\n",
    "        try:\n",
    "            # Correct way to invoke the tool function (BaseTool object)\n",
    "            mcq_raw_output = tool_function.invoke(tool_args)\n",
    "            mcq_data = json.loads(mcq_raw_output)\n",
    "            state[\"mcq_active\"] = True\n",
    "            state[\"mcq_question\"] = mcq_data[\"question\"]\n",
    "            state[\"mcq_options\"] = mcq_data[\"options\"]\n",
    "            state[\"mcq_correct_answer\"] = mcq_data[\"correct_answer\"]\n",
    "            # logger.info(\"MCQ details updated in state.\") # Uncomment when logger is ready\n",
    "        except Exception as e:\n",
    "            mcq_raw_output = f\"Error generating MCQ: {e}\"\n",
    "            # logger.error(f\"Error generating MCQ: {e}\", exc_info=True) # Uncomment when logger is ready\n",
    "    \n",
    "    # Add a ToolMessage for the MCQ generation, which the Socratic LLM can interpret\n",
    "    # or simply for logging purposes in the graph flow.\n",
    "    # Provide a unique tool_call_id for the ToolMessage\n",
    "    return {\"messages\": [ToolMessage(content=mcq_raw_output, name=tool_name, tool_call_id=str(uuid.uuid4()))], **state}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def route_supervisor_output(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Conditional edge from the supervisor to determine the next node based on its decision.\n",
    "    \"\"\"\n",
    "    # logger.info(f\"Routing supervisor output. Next node: {state['next_node']}\") # Uncomment when logger is ready\n",
    "    if state[\"next_node\"] == \"socratic_question\":\n",
    "        return \"socratic_question_node\"\n",
    "    elif state[\"next_node\"] == \"mcq_generator\":\n",
    "        return \"generate_mcq_node\"\n",
    "    # All other specialized tools now have their own nodes\n",
    "    elif state[\"next_node\"] == \"code_analysis\":\n",
    "        return \"code_analysis_node\"\n",
    "    elif state[\"next_node\"] == \"code_explanation\":\n",
    "        return \"code_explanation_node\"\n",
    "    elif state[\"next_node\"] == \"challenge_generator\":\n",
    "        return \"challenge_generator_node\"\n",
    "    return \"socratic_question_node\" # Fallback to socratic question if unexpected\n",
    "\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_supervisor\", call_supervisor)\n",
    "workflow.add_node(\"socratic_question_node\", socratic_question_node) # Renamed from call_llm\n",
    "# Add new specialized tool nodes\n",
    "workflow.add_node(\"code_analysis_node\", code_analysis_node)\n",
    "workflow.add_node(\"code_explanation_node\", code_explanation_node)\n",
    "workflow.add_node(\"challenge_generator_node\", challenge_generator_node)\n",
    "workflow.add_node(\"generate_mcq_node\", generate_mcq_node)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_supervisor\")\n",
    "\n",
    "# Define the edges.\n",
    "# From supervisor, route conditionally\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_supervisor\",\n",
    "    route_supervisor_output,\n",
    "    {\n",
    "        \"socratic_question_node\": \"socratic_question_node\",\n",
    "        \"code_analysis_node\": \"code_analysis_node\",\n",
    "        \"code_explanation_node\": \"code_explanation_node\",\n",
    "        \"challenge_generator_node\": \"challenge_generator_node\",\n",
    "        \"generate_mcq_node\": \"generate_mcq_node\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After each specialized tool node, return to the socratic_question_node\n",
    "# for the Socratic LLM to interpret the tool's output and formulate a question.\n",
    "workflow.add_edge(\"code_analysis_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"code_explanation_node\", \"socratic_question_node\")\n",
    "workflow.add_edge(\"challenge_generator_node\", \"socratic_question_node\")\n",
    "\n",
    "# After the socratic_question_node, the run ends. The main.py loop will then take user input.\n",
    "workflow.add_edge(\"socratic_question_node\", END)\n",
    "\n",
    "# After generating an MCQ, the run ends. Main.py handles the MCQ display and user input.\n",
    "workflow.add_edge(\"generate_mcq_node\", END)\n",
    "\n",
    "# Removed MemorySaver initialization and compilation\n",
    "# checkpointer = MemorySaver()\n",
    "socratic_graph = workflow.compile() # Compile without checkpointer\n",
    "\n",
    "# --- Temporary LLM Connection Test ---\n",
    "# This function will be called once when the module is imported to test LLM connectivity.\n",
    "def _test_llm_connection():\n",
    "    print(\"\\n--- Testing LLM Connection (Temporary) ---\")\n",
    "    try:\n",
    "        test_message = HumanMessage(content=\"Say hi!\")\n",
    "        response = llm.invoke([test_message])\n",
    "        print(f\"LLM Test Response: {response.content}\")\n",
    "        print(\"LLM connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Connection Test FAILED: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and network connection.\")\n",
    "    print(\"--- End LLM Connection Test ---\\n\")\n",
    "\n",
    "# --- Temporary Node Functionality Test ---\n",
    "# This function allows testing individual nodes with a sample state.\n",
    "def _test_node_functionality(node_name: str, initial_state: SocraticAgentState):\n",
    "    print(f\"\\n--- Testing Node: {node_name} ---\")\n",
    "    try:\n",
    "        node_runnable = workflow.nodes[node_name].runnable\n",
    "        result_state_update = node_runnable.invoke(initial_state)\n",
    "\n",
    "        print(f\"Node '{node_name}' executed successfully.\")\n",
    "        print(f\"Initial State (messages only): {[msg.content if hasattr(msg, 'content') else str(msg) for msg in initial_state['messages']]}\")\n",
    "        print(f\"Resulting State Update: {result_state_update}\")\n",
    "\n",
    "        if 'messages' in result_state_update and result_state_update['messages']:\n",
    "            print(\"Messages returned by node:\")\n",
    "            for msg in result_state_update['messages']:\n",
    "                if isinstance(msg, BaseMessage):\n",
    "                    print(f\"  - Type: {msg.type}, Content: {msg.content[:50]}...\")\n",
    "                    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                        for tc in msg.tool_calls:\n",
    "                            if hasattr(tc, \"function\"):\n",
    "                                tool_name_debug = tc.function.name\n",
    "                                tool_args_debug = tc.function.arguments\n",
    "                            elif isinstance(tc, dict):\n",
    "                                tool_name_debug = tc.get(\"name\")\n",
    "                                tool_args_debug = tc.get(\"args\")\n",
    "                            else:\n",
    "                                tool_name_debug = \"unknown\"\n",
    "                                tool_args_debug = {}\n",
    "                            print(f\"    Tool Call: {tool_name_debug} with args {tool_args_debug}\")\n",
    "                else:\n",
    "                    print(f\"  - Raw: {msg}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Node '{node_name}' Test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    print(f\"--- End Node Test: {node_name} ---\\n\")\n",
    "\n",
    "\n",
    "# Call the LLM connection test immediately when this module is loaded\n",
    "_test_llm_connection()\n",
    "\n",
    "# Example usage of the new node testing function (uncomment to test specific nodes)\n",
    "# Ensure your GOOGLE_API_KEY is set for LLM-based nodes.\n",
    "\n",
    "# Test call_supervisor node\n",
    "sample_supervisor_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"I need help with debugging this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Introduction\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"call_supervisor\", sample_supervisor_state)\n",
    "\n",
    "# Test socratic_question_node\n",
    "sample_socratic_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"What are variables?\")], # Removed the AIMessage from here\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Variables\",\n",
    "    sub_topic=\"Definition\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\", next_node=\"\", tool_input={}\n",
    ")\n",
    "_test_node_functionality(\"socratic_question_node\", sample_socratic_state)\n",
    "\n",
    "# Test code_analysis_node\n",
    "sample_tool_state_code_analysis = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"debug this code: print('hello')\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Debugging\",\n",
    "    sub_topic=\"Code Analysis\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_analysis_agent\", # This should match the tool's actual name\n",
    "    tool_input={\"code\": \"def my_func():\\n    pass\"}\n",
    ")\n",
    "_test_node_functionality(\"code_analysis_node\", sample_tool_state_code_analysis)\n",
    "\n",
    "# Test code_explanation_node\n",
    "sample_tool_state_code_explanation = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Explain loops in Python\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Python Basics\",\n",
    "    sub_topic=\"Loops\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"code_explanation_agent\",\n",
    "    tool_input={\"concept\": \"loops\"}\n",
    ")\n",
    "_test_node_functionality(\"code_explanation_node\", sample_tool_state_code_explanation)\n",
    "\n",
    "# Test challenge_generator_node\n",
    "sample_tool_state_challenge_generator = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me a challenge on functions.\")],\n",
    "    difficulty_level=\"intermediate\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"Challenges\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"challenge_generator_agent\",\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"intermediate\"}\n",
    ")\n",
    "_test_node_functionality(\"challenge_generator_node\", sample_tool_state_challenge_generator)\n",
    "\n",
    "# Test generate_mcq_node\n",
    "sample_mcq_state = SocraticAgentState(\n",
    "    messages=[HumanMessage(content=\"Give me an MCQ on functions.\")],\n",
    "    difficulty_level=\"beginner\",\n",
    "    user_struggle_count=0,\n",
    "    topic=\"Functions\",\n",
    "    sub_topic=\"MCQ\",\n",
    "    mcq_active=False,\n",
    "    mcq_question=\"\", mcq_options=[], mcq_correct_answer=\"\",\n",
    "    agent_thought=\"\",\n",
    "    next_node=\"mcq_agent\", # Supervisor would set this\n",
    "    tool_input={\"topic\": \"functions\", \"difficulty\": \"beginner\"} # Supervisor would set this\n",
    ")\n",
    "_test_node_functionality(\"generate_mcq_node\", sample_mcq_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a521998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1612a760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115579d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2290ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from socratic_bot_logic import SocraticBot\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from config import load_environment_variables\n",
    "from logger import setup_logging\n",
    "\n",
    "def main():\n",
    "# Set up logging and configuration\n",
    "# logger = setup_logging()\n",
    "# google_api_key = load_environment_variables()\n",
    "\n",
    "# Create the SocraticBot instance\n",
    "bot = SocraticBot()\n",
    "\n",
    "# Set an initial topic (for example, variables in Python)\n",
    "initial_topic = \"variables in Python\"\n",
    "bot.update_current_topic(initial_topic)\n",
    "welcome_message = f\"Hello! I'm your Socratic Python Tutor. Today, we can start with '{initial_topic}'.\"\n",
    "print(f\"Bot: {welcome_message}\")\n",
    "bot.add_message_to_history(AIMessage(content=welcome_message))\n",
    "\n",
    "# Present options to the user\n",
    "options_message = (\"\\nBot: Would you like to:\\n\"\n",
    "\"1. Test your knowledge on variables in Python?\\n\"\n",
    "\"2. Learn more about variables in Python?\\n\"\n",
    "\"Please type '1' or '2'.\")\n",
    "print(options_message)\n",
    "\n",
    "# Main interaction loop\n",
    "while True:\n",
    "user_input = input(\"You: \").strip()\n",
    "logger.info(f\"User: {user_input}\")\n",
    "\n",
    "# Exit condition\n",
    "if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "print(\"Bot: Goodbye! Keep coding!\")\n",
    "logger.info(\"User exited the session.\")\n",
    "break\n",
    "\n",
    "# A simple routing logic: if the input contains code indicators, use code_analysis\n",
    "if \"def \" in user_input or \"print(\" in user_input:\n",
    "# Let the tool analyze the code first\n",
    "analysis_feedback = bot.tools[\"code_analysis_tool\"](user_input)\n",
    "print(f\"Bot (Code Analysis): {analysis_feedback}\")\n",
    "logger.info(f\"Bot (Code Analysis): {analysis_feedback}\")\n",
    "# Follow up with a Socratic question based on the feedback\n",
    "follow_up_q = \"Based on the code analysis, what do you think might be improved?\"\n",
    "print(f\"Bot: {follow_up_q}\")\n",
    "bot.add_message_to_history(AIMessage(content=follow_up_q))\n",
    "continue\n",
    "\n",
    "# If the input asks for an explanation (e.g., 'what is sort()' or similar), use code_explanation\n",
    "if \"explain\" in user_input.lower() or \"what is\" in user_input.lower():\n",
    "explanation = bot.tools[\"code_explanation_tool\"](user_input)\n",
    "print(f\"Bot (Explanation): {explanation}\")\n",
    "logger.info(f\"Bot (Explanation): {explanation}\")\n",
    "# Return to Socratic mode after giving the explanation\n",
    "socratic_follow_up = \"Can you tell me how this explanation might help you fix an issue in your code?\"\n",
    "print(f\"Bot: {socratic_follow_up}\")\n",
    "bot.add_message_to_history(AIMessage(content=socratic_follow_up))\n",
    "continue\n",
    "\n",
    "# Otherwise, use the general LLM flow for Socratic reasoning:\n",
    "bot.add_message_to_history(HumanMessage(content=user_input))\n",
    "response = bot.send_message_to_llm(user_input)\n",
    "print(f\"Bot: {response}\")\n",
    "logger.info(f\"Bot: {response}\")\n",
    "bot.add_message_to_history(AIMessage(content=response))\n",
    "if name == \"main\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da726105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7ccf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f40ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7ad35d99e270>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the system prompt for the Socratic Agent.\n",
    "# This prompt guides the LLM's behavior, making it act as a Socratic tutor, detecting struggle, adapting difficulty, and using tools.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a friendly Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions. \n",
    "The user may provide code snippets that needs to be debugged. Help the user to debug and understand the code by calling the appropriate tool agents.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides Python code and asks for feedback, debugging, or analysis.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation of a Python concept, function, keyword, or error message.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge or a fill-in-the-blanks exercise.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question to test the user's understanding, especially if they are struggling or you want to quickly assess a concept.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9515ffa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "# Removed ToolExecutor and ToolInvocation imports\n",
    "# from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "# This TypedDict defines the structure of our graph's state.\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str # Added for ReAct architecture\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM for the Socratic Agent\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "# Define the system prompt for the Socratic Agent.\n",
    "# This prompt guides the LLM's behavior, making it act as a Socratic tutor,\n",
    "# detecting struggle, adapting difficulty, and using tools.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides Python code and asks for feedback, debugging, or analysis.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation of a Python concept, function, keyword, or error message.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge or a fill-in-the-blanks exercise.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question to test the user's understanding, especially if they are struggling or you want to quickly assess a concept.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template for the Socratic LLM, including tool calling capabilities.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Simulated Agent Tools (will be replaced by actual agents later) ---\n",
    "# These are placeholder tools for now. In a full multi-agent system,\n",
    "# these would likely be separate LangChain agents or services.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. Let's analyze it together. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Instead of me explaining it directly, can you tell me what you already know or suspect about '{concept}'?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    return f\"Simulated Challenge: For '{topic}' at '{difficulty}' difficulty, here's a challenge: 'Write a Python function that takes a list of numbers and returns the sum of all even numbers.' How would you approach this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # Simulate dynamic MCQ generation based on topic\n",
    "    mcqs = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        # Add more topics and MCQs as needed\n",
    "    }\n",
    "    \n",
    "    # Default MCQ if topic not found or a general one\n",
    "    selected_mcq = mcqs.get(topic.lower(), {\n",
    "        \"question\": f\"Here's a general Python MCQ related to {topic}: What does Python primarily use for code blocking?\",\n",
    "        \"options\": [\"A) Curly braces {}\", \"B) Parentheses ()\", \"C) Indentation\", \"D) Keywords like 'begin' and 'end'\"],\n",
    "        \"correct_answer\": \"C\"\n",
    "    })\n",
    "    \n",
    "    return json.dumps(selected_mcq)\n",
    "# List of all tools available to the Socratic agent\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent]\n",
    "\n",
    "# Bind the tools to the LLM first\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice = \"auto\")\n",
    "\n",
    "# Then, create the runnable agent by piping the prompt and the LLM with tools\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    messages = [msg for msg in state[\"messages\"] if getattr(msg, \"content\", None)]\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    print(\"[DEBUG] LLM Response:\", response)\n",
    "    print(\"[DEBUG] Tool Calls:\", getattr(response, \"tool_calls\", None))\n",
    "\n",
    "    thought = \"\"\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            thought_and_rest = parts[1].strip()\n",
    "            thought = thought_and_rest.split('\\n', 1)[0]\n",
    "\n",
    "    return {\"messages\": [response], \"agent_thought\": thought}\n",
    "\n",
    "\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute a tool call requested by the LLM.\n",
    "    It parses the tool invocation and runs the tool.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_output_messages = []\n",
    "    \n",
    "    # Create a mutable copy of the state to update\n",
    "    new_state = state.copy()\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_messages.append(AIMessage(content=str(response), name=tool_call[\"name\"]))\n",
    "\n",
    "                # Special handling for MCQ agent output\n",
    "                if tool_call[\"name\"] == \"mcq_agent\":\n",
    "                    try:\n",
    "                        mcq_data = json.loads(response)\n",
    "                        new_state[\"mcq_active\"] = True\n",
    "                        new_state[\"mcq_question\"] = mcq_data.get(\"question\", \"\")\n",
    "                        new_state[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                        new_state[\"mcq_correct_answer\"] = mcq_data.get(\"correct_answer\", \"\")\n",
    "                        # Update the topic based on the MCQ requested if not already set\n",
    "                        if not new_state[\"topic\"] and tool_args.get(\"topic\"):\n",
    "                            new_state[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"[ERROR] Failed to decode MCQ JSON: {response}\")\n",
    "                        tool_output_messages.append(AIMessage(content=\"Error: MCQ agent returned invalid JSON.\", name=tool_call[\"name\"]))\n",
    "            else:\n",
    "                tool_output_messages.append(\n",
    "                    AIMessage(content=f\"Error: Tool '{tool_call['name']}' not found.\", name=tool_call[\"name\"])\n",
    "                )\n",
    "\n",
    "    # Return updated state including MCQ details and tool output messages\n",
    "    # The tool output messages need to be added to the state's message history.\n",
    "    # We should return a dictionary of updates to the state.\n",
    "    return {\"messages\": tool_output_messages, **new_state}\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def should_continue(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Determines the next step in the graph based on the last message.\n",
    "    If the LLM requested a tool, it goes to 'call_tool'. Otherwise, it ends (for now).\n",
    "    Later, we'll add more complex logic here for MCQ handling, etc.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        # If the MCQ agent was called, we need a specific flow for it\n",
    "        return \"call_tool\"\n",
    "    return \"END\" # Direct response from LLM, conversation loop will continue in main.py\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_llm\", call_llm)\n",
    "workflow.add_node(\"call_tool\", call_tool)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_llm\")\n",
    "\n",
    "# Define the edges.\n",
    "# From 'call_llm', decide if we need to call a tool or end the current graph run.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"call_tool\": \"call_tool\",\n",
    "        \"end\": END # If no tool call, the LLM's direct response ends this graph run.\n",
    "    }\n",
    ")\n",
    "\n",
    "# After calling a tool or generating an MCQ, we typically want to loop back to the LLM\n",
    "# to process the tool's output or continue the Socratic questioning.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Compile the graph into a runnable agent.\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# socratic_graph\n",
    "\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(socratic_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This is a common error if graphviz is not installed.\n",
    "    # We can just ignore it for now.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e766fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "# Removed ToolExecutor and ToolInvocation imports\n",
    "# from langgraph.prebuilt import ToolExecutor, ToolInvocation\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "# This TypedDict defines the structure of our graph's state.\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic agent's conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of chat messages exchanged so far.\n",
    "        difficulty_level: The current difficulty level of questions (e.g., 'beginner', 'intermediate', 'advanced').\n",
    "        user_struggle_count: Counter for consecutive times the user struggles.\n",
    "        topic: The current Python topic being discussed.\n",
    "        sub_topic: The specific sub-topic within the main topic.\n",
    "        mcq_active: Boolean indicating if an MCQ is currently active.\n",
    "        mcq_question: The active MCQ question text.\n",
    "        mcq_options: List of options for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer for the active MCQ.\n",
    "        agent_thought: The last thought process articulated by the Socratic agent.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages] # Appends new messages to the list\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str # Added for ReAct architecture\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the Gemini LLM for the Socratic Agent\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "# Define the system prompt for the Socratic Agent.\n",
    "# This prompt guides the LLM's behavior, making it act as a Socratic tutor,\n",
    "# detecting struggle, adapting difficulty, and using tools.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides Python code and asks for feedback, debugging, or analysis.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation of a Python concept, function, keyword, or error message.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge or a fill-in-the-blanks exercise.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question to test the user's understanding, especially if they are struggling or you want to quickly assess a concept.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template for the Socratic LLM, including tool calling capabilities.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Simulated Agent Tools (will be replaced by actual agents later) ---\n",
    "# These are placeholder tools for now. In a full multi-agent system,\n",
    "# these would likely be separate LangChain agents or services.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code, identifies potential issues, suggests improvements,\n",
    "    and provides feedback. Use this when the user provides code and asks for review or debugging.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a static analysis tool.\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. Let's analyze it together. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept, function, keyword, or error message in detail.\n",
    "    Use this when the user asks for an explanation of something.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM specialized in explanations.\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Instead of me explaining it directly, can you tell me what you already know or suspect about '{concept}'?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge or a fill-in-the-blanks exercise based on the specified topic and difficulty.\n",
    "    Use this when the user requests a challenge.\n",
    "    \"\"\"\n",
    "    # In a real scenario, this would call another LLM or a challenge generation service.\n",
    "    return f\"Simulated Challenge: For '{topic}' at '{difficulty}' difficulty, here's a challenge: 'Write a Python function that takes a list of numbers and returns the sum of all even numbers.' How would you approach this?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    The output will be a JSON string containing the question, options, and correct answer.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ.\n",
    "    \"\"\"\n",
    "    # Simulate dynamic MCQ generation based on topic\n",
    "    mcqs = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        # Add more topics and MCQs as needed\n",
    "    }\n",
    "    \n",
    "    # Default MCQ if topic not found or a general one\n",
    "    selected_mcq = mcqs.get(topic.lower(), {\n",
    "        \"question\": f\"Here's a general Python MCQ related to {topic}: What does Python primarily use for code blocking?\",\n",
    "        \"options\": [\"A) Curly braces {}\", \"B) Parentheses ()\", \"C) Indentation\", \"D) Keywords like 'begin' and 'end'\"],\n",
    "        \"correct_answer\": \"C\"\n",
    "    })\n",
    "    \n",
    "    return json.dumps(selected_mcq)\n",
    "# List of all tools available to the Socratic agent\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent]\n",
    "\n",
    "# Bind the tools to the LLM first\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice = \"auto\")\n",
    "\n",
    "# Then, create the runnable agent by piping the prompt and the LLM with tools\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    # Ensure all messages have content for the LLM call\n",
    "    # Filter out messages that might have empty content, particularly if they are not meant to be directly sent to the LLM\n",
    "    # In LangGraph, it's common to have ToolMessages follow AIMessages with tool_calls.\n",
    "    # The LLM expects a sequence of alternating HumanMessage and AIMessage/ToolMessage.\n",
    "    # Let's ensure we are sending a valid sequence.\n",
    "    \n",
    "    # Construct messages to send to the LLM, ensuring proper turns\n",
    "    llm_messages = state[\"messages\"]\n",
    "    # for msg in state[\"messages\"]:\n",
    "    #     if isinstance(msg, AIMessage):\n",
    "    #         # If it's an AI message with tool calls, we should send it as is,\n",
    "    #         # and the *next* message should be a ToolMessage with the output.\n",
    "    #         llm_messages.append(msg)\n",
    "    #     elif isinstance(msg, ToolMessage):\n",
    "    #         if msg.content: # Double-check just in case\n",
    "    #             llm_messages.append(msg)\n",
    "    #         else:\n",
    "    #             print(f\"[WARNING] Skipping empty ToolMessage: {msg}\")\n",
    "    #     elif msg.content: # Only add if it has content\n",
    "    #         llm_messages.append(msg)\n",
    "    #     else:\n",
    "    #         print(f\"[WARNING] Skipping message with no content: {msg}\")\n",
    "\n",
    "    # # Debug print to see what messages are being sent\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", llm_messages) # Print message objects to see types and content\n",
    "\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": llm_messages,\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"sub_topic\": state[\"sub_topic\"],\n",
    "        \"mcq_active\": state[\"mcq_active\"]\n",
    "    })\n",
    "\n",
    "    print(\"[DEBUG] LLM Response:\", response)\n",
    "    print(\"[DEBUG] Tool Calls:\", getattr(response, \"tool_calls\", None))\n",
    "\n",
    "    thought = \"\"\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            thought_and_rest = parts[1].strip()\n",
    "            thought = thought_and_rest.split('\\n', 1)[0].strip()\n",
    "\n",
    "    return {\"messages\": [response], \"agent_thought\": thought}\n",
    "\n",
    "\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Node to execute a tool call requested by the LLM.\n",
    "    It parses the tool invocation and runs the tool.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # This will be the list of messages to add to the state\n",
    "    messages_to_add = [] \n",
    "    \n",
    "    # Create a mutable copy of the state to update\n",
    "    state_updates = {} \n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            \n",
    "            tool_output_content = \"\" # Initialize empty string for tool output content\n",
    "            \n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_content = str(response)\n",
    "                \n",
    "                # Special handling for MCQ agent output\n",
    "                if tool_call[\"name\"] == \"mcq_agent\":\n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content)\n",
    "                        new_state[\"mcq_active\"] = True\n",
    "                        new_state[\"mcq_question\"] = mcq_data.get(\"question\", \"\")\n",
    "                        new_state[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                        new_state[\"mcq_correct_answer\"] = mcq_data.get(\"correct_answer\", \"\")\n",
    "                        # Update the topic based on the MCQ requested if not already set\n",
    "                        if not new_state[\"topic\"] and tool_args.get(\"topic\"):\n",
    "                            new_state[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"[ERROR] Failed to decode MCQ JSON: {tool_output_content}\")\n",
    "                        tool_output_content = \"Error: MCQ agent returned invalid JSON.\"\n",
    "            else:\n",
    "                tool_output_content = f\"Error: Tool '{tool_call['name']}' not found.\"\n",
    "\n",
    "            messages_to_add.append(\n",
    "                ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "# --- 4. Define the Graph Edges (Conditional Logic) ---\n",
    "\n",
    "def should_continue(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Determines the next step in the graph based on the last message.\n",
    "    If the LLM requested a tool, it goes to 'call_tool'. Otherwise, it ends (for now).\n",
    "    Later, we'll add more complex logic here for MCQ handling, etc.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        # If the MCQ agent was called, we need a specific flow for it\n",
    "        return \"call_tool\"\n",
    "    return \"end\" # Direct response from LLM, conversation loop will continue in main.py\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Create a StateGraph instance with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add nodes to the workflow.\n",
    "workflow.add_node(\"call_llm\", call_llm)\n",
    "workflow.add_node(\"call_tool\", call_tool)\n",
    "\n",
    "# Set the entry point for the graph.\n",
    "workflow.set_entry_point(\"call_llm\")\n",
    "\n",
    "# Define the edges.\n",
    "# From 'call_llm', decide if we need to call a tool or end the current graph run.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"call_tool\": \"call_tool\",\n",
    "        \"end\": END # If no tool call, the LLM's direct response ends this graph run.\n",
    "    }\n",
    ")\n",
    "\n",
    "# After calling a tool or generating an MCQ, we typically want to loop back to the LLM\n",
    "# to process the tool's output or continue the Socratic questioning.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Compile the graph into a runnable agent.\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# socratic_graph\n",
    "\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(socratic_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This is a common error if graphviz is not installed.\n",
    "    # We can just ignore it for now.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81be8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Hi test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='2c303c9f-9979-4e6f-b3d7-ab1455835344')]\n",
      "[DEBUG] LLM Response: content=\"Thought:The user wants to test their knowledge of classes with an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\" additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--31cfc8c1-e864-4a47-82a1-0374acfb2e22-0' tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': 'e8f0bc31-89cc-481a-a115-00e55f8d5878', 'type': 'tool_call'}] usage_metadata={'input_tokens': 746, 'output_tokens': 43, 'total_tokens': 789, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Tool Calls: [{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': 'e8f0bc31-89cc-481a-a115-00e55f8d5878', 'type': 'tool_call'}]\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Hi test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='2c303c9f-9979-4e6f-b3d7-ab1455835344'), AIMessage(content=\"Thought:The user wants to test their knowledge of classes with an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\", additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--31cfc8c1-e864-4a47-82a1-0374acfb2e22-0', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': 'e8f0bc31-89cc-481a-a115-00e55f8d5878', 'type': 'tool_call'}], usage_metadata={'input_tokens': 746, 'output_tokens': 43, 'total_tokens': 789, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='{\"question\": \"Here\\'s a general Python MCQ related to class: What does Python primarily use for code blocking?\", \"options\": [\"A) Curly braces {}\", \"B) Parentheses ()\", \"C) Indentation\", \"D) Keywords like \\'begin\\' and \\'end\\'\"], \"correct_answer\": \"C\"}', id='20500a37-1797-45e6-883d-49becde02b93', tool_call_id='e8f0bc31-89cc-481a-a115-00e55f8d5878')]\n",
      "[DEBUG] LLM Response: content=\"Thought:The MCQ agent has provided a multiple choice question on classes in Python.  Let's see if the user can answer it.\\n\\nWhat is your answer to the MCQ question: What does Python primarily use for code blocking in classes?  Choose from options A, B, C, or D.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--25607d4e-6b4c-45b9-b8df-9acc21d8f5a6-0' usage_metadata={'input_tokens': 810, 'output_tokens': 63, 'total_tokens': 873, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Tool Calls: []\n",
      "{'agent_thought': 'The MCQ agent has provided a multiple choice question on '\n",
      "                  \"classes in Python.  Let's see if the user can answer it.\",\n",
      " 'difficulty_level': 'beginner',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Hi test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='2c303c9f-9979-4e6f-b3d7-ab1455835344'),\n",
      "              AIMessage(content=\"Thought:The user wants to test their knowledge of classes with an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\", additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--31cfc8c1-e864-4a47-82a1-0374acfb2e22-0', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': 'e8f0bc31-89cc-481a-a115-00e55f8d5878', 'type': 'tool_call'}], usage_metadata={'input_tokens': 746, 'output_tokens': 43, 'total_tokens': 789, 'input_token_details': {'cache_read': 0}}),\n",
      "              ToolMessage(content='{\"question\": \"Here\\'s a general Python MCQ related to class: What does Python primarily use for code blocking?\", \"options\": [\"A) Curly braces {}\", \"B) Parentheses ()\", \"C) Indentation\", \"D) Keywords like \\'begin\\' and \\'end\\'\"], \"correct_answer\": \"C\"}', id='20500a37-1797-45e6-883d-49becde02b93', tool_call_id='e8f0bc31-89cc-481a-a115-00e55f8d5878'),\n",
      "              AIMessage(content=\"Thought:The MCQ agent has provided a multiple choice question on classes in Python.  Let's see if the user can answer it.\\n\\nWhat is your answer to the MCQ question: What does Python primarily use for code blocking in classes?  Choose from options A, B, C, or D.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--25607d4e-6b4c-45b9-b8df-9acc21d8f5a6-0', usage_metadata={'input_tokens': 810, 'output_tokens': 63, 'total_tokens': 873, 'input_token_details': {'cache_read': 0}})],\n",
      " 'sub_topic': '',\n",
      " 'topic': '',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- Test with an initial state ---\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hi test me with an MCQ question on class\")],\n",
    "    \"difficulty_level\": \"beginner\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\"\n",
    "}\n",
    "result = socratic_graph.invoke(initial_state)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05497d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# socratic_bot_logic.py\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)\n",
    "\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides Python code and asks for feedback, debugging, or analysis.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation of a Python concept, function, keyword, or error message.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge or a fill-in-the-blanks exercise.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question to test the user's understanding, especially if they are struggling or you want to quickly assess a concept.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"Analyzes the provided Python code...\"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"Explains a given Python concept...\"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"Generates a Python coding challenge...\"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"Generates a multiple-choice question (MCQ)...\"\"\"\n",
    "    mcqs = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"Tuple\", \"String\", \"List\", \"Integer\"],\n",
    "            \"correct_answer\": \"List\"\n",
    "        },\n",
    "        \"class\": { # Added 'class' as a lowercase key\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"To destroy an object when it's no longer needed.\",\n",
    "                \"To define static methods.\",\n",
    "                \"To initialize the attributes of an object when it's created.\",\n",
    "                \"To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"To initialize the attributes of an object when it's created.\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"func\", \"define\", \"def\", \"function\"],\n",
    "            \"correct_answer\": \"def\"\n",
    "        },\n",
    "    }\n",
    "    selected_mcq = mcqs.get(topic.lower(), {\n",
    "        \"question\": f\"A general Python question: What does Python use for code blocking?\",\n",
    "        \"options\": [\"Curly braces {}\", \"Parentheses ()\", \"Indentation\", \"Keywords like 'begin' and 'end'\"],\n",
    "        \"correct_answer\": \"Indentation\"\n",
    "    })\n",
    "    return json.dumps(selected_mcq)\n",
    "\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent]\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"Invokes the LLM with the current conversation history.\"\"\"\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        **{k: v for k, v in state.items() if k != 'messages'} # Pass all other state variables to the prompt\n",
    "    })\n",
    "    print(\"[DEBUG] LLM Response:\", response)\n",
    "    \n",
    "    thought = \"\"\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        thought = response.content.split(\"Thought:\", 1)[1].strip().split('\\n', 1)[0]\n",
    "\n",
    "    # Create a new dictionary to return, preserving all existing state\n",
    "    # but updating 'messages' and 'agent_thought'.\n",
    "    # This ensures mcq_active, mcq_question, etc., persist.\n",
    "    updated_state = state.copy()\n",
    "    updated_state[\"messages\"] = [response] # add_messages will append this\n",
    "    updated_state[\"agent_thought\"] = thought\n",
    "    \n",
    "    return updated_state # Return the entire updated state dictionary\n",
    "\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    \"\"\"Executes a tool call and returns only the updated state fields.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    messages_to_add = []\n",
    "    state_updates = {} # Dictionary to hold only the fields that are changing\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            \n",
    "            tool_output_content = \"\"\n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_content = str(response)\n",
    "\n",
    "                if tool_name == \"mcq_agent\":\n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content)\n",
    "                        state_updates[\"mcq_active\"] = True\n",
    "                        state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\")\n",
    "                        state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                        state_updates[\"mcq_correct_answer\"] = mcq_data.get(\"correct_answer\", \"\")\n",
    "                        if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                            state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        tool_output_content = \"Error: MCQ agent returned invalid JSON.\"\n",
    "            else:\n",
    "                tool_output_content = f\"Error: Tool '{tool_name}' not found.\"\n",
    "\n",
    "            messages_to_add.append(\n",
    "                ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "            )\n",
    "\n",
    "    # Return a dictionary with ONLY the new messages and updated state keys\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue(state: SocraticAgentState):\n",
    "    \"\"\"Determines the next step in the graph.\"\"\"\n",
    "    if isinstance(state[\"messages\"][-1], AIMessage) and state[\"messages\"][-1].tool_calls:\n",
    "        return \"call_tool\"\n",
    "    return \"end\"\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "workflow.add_node(\"call_llm\", call_llm)\n",
    "workflow.add_node(\"call_tool\", call_tool)\n",
    "\n",
    "workflow.set_entry_point(\"call_llm\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue,\n",
    "    {\"call_tool\": \"call_tool\", \"end\": END}\n",
    ")\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# --- Test ---\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(socratic_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63c46b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Hi, test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='e69157a0-3986-435d-a049-940a801b1d5c')]\n",
      "[DEBUG] LLM Response: content=\"Thought:The user wants to test their knowledge of classes with an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\" additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--ab38dd70-f718-434b-9d9f-b1301920cc48-0' tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': '445578c7-f365-4fd3-988e-63ff1694d5b5', 'type': 'tool_call'}] usage_metadata={'input_tokens': 626, 'output_tokens': 43, 'total_tokens': 669, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Hi, test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='e69157a0-3986-435d-a049-940a801b1d5c'), AIMessage(content=\"Thought:The user wants to test their knowledge of classes with an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\", additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--ab38dd70-f718-434b-9d9f-b1301920cc48-0', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': '445578c7-f365-4fd3-988e-63ff1694d5b5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 626, 'output_tokens': 43, 'total_tokens': 669, 'input_token_details': {'cache_read': 0}}), ToolMessage(content='{\"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\", \"options\": [\"To destroy an object when it\\'s no longer needed.\", \"To define static methods.\", \"To initialize the attributes of an object when it\\'s created.\", \"To define the string representation of an object.\"], \"correct_answer\": \"To initialize the attributes of an object when it\\'s created.\"}', id='f631f4fb-1ce0-43d6-a394-a56d24dd19bd', tool_call_id='445578c7-f365-4fd3-988e-63ff1694d5b5')]\n",
      "[DEBUG] LLM Response: content=\"Thought:The MCQ agent has provided a multiple choice question about the `__init__` method in a class.  This is a good starting point to assess the user's understanding of classes.\\n\\nWhat is your answer to the question: In Python, what is the primary purpose of the `__init__` method in a class?  Choose from the options provided:\\n\\nA. To destroy an object when it's no longer needed.\\nB. To define static methods.\\nC. To initialize the attributes of an object when it's created.\\nD. To define the string representation of an object.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--ddebf069-f0f4-4560-9d93-40fe7f3cf009-0' usage_metadata={'input_tokens': 716, 'output_tokens': 128, 'total_tokens': 844, 'input_token_details': {'cache_read': 0}}\n",
      "{'agent_thought': 'The MCQ agent has provided a multiple choice question about '\n",
      "                  'the `__init__` method in a class.  This is a good starting '\n",
      "                  \"point to assess the user's understanding of classes.\",\n",
      " 'difficulty_level': 'beginner',\n",
      " 'mcq_active': True,\n",
      " 'mcq_correct_answer': \"To initialize the attributes of an object when it's \"\n",
      "                       'created.',\n",
      " 'mcq_options': [\"To destroy an object when it's no longer needed.\",\n",
      "                 'To define static methods.',\n",
      "                 \"To initialize the attributes of an object when it's created.\",\n",
      "                 'To define the string representation of an object.'],\n",
      " 'mcq_question': 'In Python, what is the primary purpose of the `__init__` '\n",
      "                 'method in a class?',\n",
      " 'messages': [HumanMessage(content='Hi, test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='e69157a0-3986-435d-a049-940a801b1d5c'),\n",
      "              AIMessage(content=\"Thought:The user wants to test their knowledge of classes with an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\", additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--ab38dd70-f718-434b-9d9f-b1301920cc48-0', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': '445578c7-f365-4fd3-988e-63ff1694d5b5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 626, 'output_tokens': 43, 'total_tokens': 669, 'input_token_details': {'cache_read': 0}}),\n",
      "              ToolMessage(content='{\"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\", \"options\": [\"To destroy an object when it\\'s no longer needed.\", \"To define static methods.\", \"To initialize the attributes of an object when it\\'s created.\", \"To define the string representation of an object.\"], \"correct_answer\": \"To initialize the attributes of an object when it\\'s created.\"}', id='f631f4fb-1ce0-43d6-a394-a56d24dd19bd', tool_call_id='445578c7-f365-4fd3-988e-63ff1694d5b5'),\n",
      "              AIMessage(content=\"Thought:The MCQ agent has provided a multiple choice question about the `__init__` method in a class.  This is a good starting point to assess the user's understanding of classes.\\n\\nWhat is your answer to the question: In Python, what is the primary purpose of the `__init__` method in a class?  Choose from the options provided:\\n\\nA. To destroy an object when it's no longer needed.\\nB. To define static methods.\\nC. To initialize the attributes of an object when it's created.\\nD. To define the string representation of an object.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--ddebf069-f0f4-4560-9d93-40fe7f3cf009-0', usage_metadata={'input_tokens': 716, 'output_tokens': 128, 'total_tokens': 844, 'input_token_details': {'cache_read': 0}})],\n",
      " 'sub_topic': '',\n",
      " 'topic': 'class',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hi, test me with an MCQ question on class\")],\n",
    "    \"difficulty_level\": \"beginner\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"class\", # Pre-setting topic for clarity\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\"\n",
    "}\n",
    "result = socratic_graph.invoke(initial_state)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f318f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "import copy\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from socrabot_logic_2 import socratic_graph, SocraticAgentState\n",
    "# from logger import logger\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "st.set_page_config(page_title=\"Socratic Python Tutor\", page_icon=\"\")\n",
    "st.title(\" Socratic Python Tutor\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Check if API key is loaded\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    st.error(\"GOOGLE_API_KEY not found in environment. Please check your .env file.\")\n",
    "    st.stop()\n",
    "\n",
    "# --- Initialize Session State ---\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "if \"socratic_agent_state\" not in st.session_state:\n",
    "    st.session_state.socratic_agent_state = SocraticAgentState(\n",
    "        messages=[],\n",
    "        difficulty_level=\"beginner\",\n",
    "        user_struggle_count=0,\n",
    "        topic=\"Python Basics\",\n",
    "        sub_topic=\"Introduction\",\n",
    "        mcq_active=False,\n",
    "        mcq_question=\"\",\n",
    "        mcq_options=[],\n",
    "        mcq_correct_answer=\"\",\n",
    "        agent_thought=\"\"\n",
    "    )\n",
    "\n",
    "if \"initial_greeting_done\" not in st.session_state:\n",
    "    st.session_state.initial_greeting_done = False\n",
    "\n",
    "if \"mcq_displayed\" not in st.session_state:\n",
    "    st.session_state.mcq_displayed = False\n",
    "\n",
    "# --- Display Chat History ---\n",
    "for message in st.session_state.chat_history:\n",
    "    with st.chat_message(\"user\" if isinstance(message, HumanMessage) else \"assistant\"):\n",
    "        st.markdown(message.content)\n",
    "        if isinstance(message, AIMessage) and message.name:\n",
    "            st.info(f\"Tool Output ({message.name}): {message.content}\")\n",
    "\n",
    "# --- Initial Greeting ---\n",
    "if not st.session_state.initial_greeting_done:\n",
    "    greeting = \"Hello! I'm your Socratic Python Tutor. What Python topic would you like to learn or practice today? Or would you like to test your knowledge with a challenge or an MCQ?\"\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(greeting)\n",
    "    st.session_state.chat_history.append(AIMessage(content=greeting))\n",
    "    st.session_state.initial_greeting_done = True\n",
    "\n",
    "# --- Main Chat Input ---\n",
    "user_input = st.chat_input(\"Your message:\")\n",
    "if user_input:\n",
    "    st.session_state.chat_history.append(HumanMessage(content=user_input))\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    current_state = copy.deepcopy(st.session_state.socratic_agent_state)\n",
    "    current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "    try:\n",
    "        final_state = socratic_graph.invoke(current_state)\n",
    "        st.session_state.socratic_agent_state = final_state\n",
    "\n",
    "        # Display assistant message\n",
    "        last_ai_message = next(\n",
    "            (msg for msg in reversed(final_state[\"messages\"]) if isinstance(msg, AIMessage)), None\n",
    "        )\n",
    "\n",
    "        if last_ai_message:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                content = last_ai_message.content\n",
    "                if content and content.startswith(\"Thought:\"):\n",
    "                    content = content.split(\"\\n\", 1)[1] if \"\\n\" in content else \"\"\n",
    "\n",
    "                if content:\n",
    "                    st.markdown(content)\n",
    "\n",
    "                if last_ai_message.tool_calls:\n",
    "                    for tool_call in last_ai_message.tool_calls:\n",
    "                        st.info(f\"Tool Call: `{tool_call.name}` | Args: `{tool_call.args}`\")\n",
    "\n",
    "            st.session_state.chat_history.append(last_ai_message)\n",
    "\n",
    "            # --- Display MCQ if Active ---\n",
    "            if (\n",
    "                final_state[\"mcq_active\"]\n",
    "                and final_state[\"mcq_question\"]\n",
    "                and not st.session_state.mcq_displayed\n",
    "            ):\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    st.subheader(\" Multiple Choice Question:\")\n",
    "                    st.markdown(final_state[\"mcq_question\"])\n",
    "\n",
    "                    # Show options with unique Streamlit key\n",
    "                    options = final_state[\"mcq_options\"]\n",
    "                    option_map = {chr(65 + i): option for i, option in enumerate(options)}\n",
    "\n",
    "                    selected_option = st.radio(\n",
    "                        \"Choose your answer:\",\n",
    "                        list(option_map.keys()),\n",
    "                        key=f\"mcq_radio_{final_state['mcq_question']}\"\n",
    "                    )\n",
    "\n",
    "                    if st.button(\"Submit Answer\", key=\"mcq_submit_button\"):\n",
    "                        user_answer_key = selected_option\n",
    "                        correct_key = final_state[\"mcq_correct_answer\"]\n",
    "                        is_correct = user_answer_key == correct_key\n",
    "\n",
    "                        if is_correct:\n",
    "                            feedback = f\" Correct! The answer is {correct_key}) {option_map[correct_key]}.\"\n",
    "                            st.session_state.socratic_agent_state[\"user_struggle_count\"] = 0\n",
    "                        else:\n",
    "                            feedback = f\" Incorrect. The correct answer is {correct_key}) {option_map[correct_key]}. Let's go over this again.\"\n",
    "                            st.session_state.socratic_agent_state[\"user_struggle_count\"] += 1\n",
    "\n",
    "                        with st.chat_message(\"assistant\"):\n",
    "                            st.success(feedback) if is_correct else st.error(feedback)\n",
    "\n",
    "                        # Save MCQ + Feedback in chat history\n",
    "                        full_mcq = f\"MCQ: {final_state['mcq_question']}\\nOptions:\\n\" + \\\n",
    "                                   \"\\n\".join(final_state[\"mcq_options\"]) + f\"\\n\\nUser answered: {user_answer_key})\"\n",
    "                        st.session_state.chat_history.append(AIMessage(content=full_mcq))\n",
    "                        st.session_state.chat_history.append(AIMessage(content=feedback))\n",
    "\n",
    "                        # Reset MCQ state\n",
    "                        st.session_state.socratic_agent_state.update({\n",
    "                            \"mcq_active\": False,\n",
    "                            \"mcq_question\": \"\",\n",
    "                            \"mcq_options\": [],\n",
    "                            \"mcq_correct_answer\": \"\"\n",
    "                        })\n",
    "\n",
    "                        st.session_state.mcq_displayed = False\n",
    "                        st.rerun()\n",
    "\n",
    "                # Only set this after rendering MCQ\n",
    "                st.session_state.mcq_displayed = True\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        # logger.error(f\"Graph Error: {e}\", exc_info=True)\n",
    "\n",
    "# --- Sidebar ---\n",
    "st.sidebar.header(\"Tutor Settings\")\n",
    "st.sidebar.write(f\"Current Difficulty: {st.session_state.socratic_agent_state['difficulty_level']}\")\n",
    "st.sidebar.write(f\"Topic: {st.session_state.socratic_agent_state['topic']}\")\n",
    "st.sidebar.write(f\"Sub-topic: {st.session_state.socratic_agent_state['sub_topic']}\")\n",
    "st.sidebar.write(f\"Struggle Count: {st.session_state.socratic_agent_state['user_struggle_count']}\")\n",
    "\n",
    "if st.sidebar.button(\" Reset Chat\"):\n",
    "    st.session_state.chat_history = []\n",
    "    st.session_state.socratic_agent_state = SocraticAgentState(\n",
    "        messages=[],\n",
    "        difficulty_level=\"beginner\",\n",
    "        user_struggle_count=0,\n",
    "        topic=\"Python Basics\",\n",
    "        sub_topic=\"Introduction\",\n",
    "        mcq_active=False,\n",
    "        mcq_question=\"\",\n",
    "        mcq_options=[],\n",
    "        mcq_correct_answer=\"\",\n",
    "        agent_thought=\"\"\n",
    "    )\n",
    "    st.session_state.initial_greeting_done = False\n",
    "    st.session_state.mcq_displayed = False\n",
    "    st.rerun()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37386e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5d9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb0aaf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hi test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='56926f61-0fde-4993-ad7b-9a0aebc2054c'), AIMessage(content=\"Thought:The user wants to test their knowledge on classes using an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\", additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--4a37b449-5efc-4faf-aee9-bfa9c1e74a48-0', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': '6daf3554-acea-4875-b377-93d732895959', 'type': 'tool_call'}], usage_metadata={'input_tokens': 746, 'output_tokens': 43, 'total_tokens': 789, 'input_token_details': {'cache_read': 0}})], 'difficulty_level': 'beginner', 'user_struggle_count': 0, 'topic': 'variables', 'sub_topic': '', 'mcq_active': True, 'mcq_question': 'Which of the following data types is mutable in Python?', 'mcq_options': ['A) Tuple', 'B) String', 'C) List', 'D) Integer'], 'mcq_correct_answer': 'C', 'agent_thought': \"The user wants to test their knowledge on classes using an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\"}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6434a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_thought': 'The user wants to test their knowledge on classes using an '\n",
      "                  \"MCQ. I'll use the `mcq_agent` tool to generate a suitable \"\n",
      "                  'question.',\n",
      " 'difficulty_level': 'beginner',\n",
      " 'mcq_active': True,\n",
      " 'mcq_correct_answer': 'C',\n",
      " 'mcq_options': ['A) Tuple', 'B) String', 'C) List', 'D) Integer'],\n",
      " 'mcq_question': 'Which of the following data types is mutable in Python?',\n",
      " 'messages': [HumanMessage(content='Hi test me with an MCQ question on class', additional_kwargs={}, response_metadata={}, id='56926f61-0fde-4993-ad7b-9a0aebc2054c'),\n",
      "              AIMessage(content=\"Thought:The user wants to test their knowledge on classes using an MCQ. I'll use the `mcq_agent` tool to generate a suitable question.\", additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"class\", \"difficulty\": \"beginner\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--4a37b449-5efc-4faf-aee9-bfa9c1e74a48-0', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'class', 'difficulty': 'beginner'}, 'id': '6daf3554-acea-4875-b377-93d732895959', 'type': 'tool_call'}], usage_metadata={'input_tokens': 746, 'output_tokens': 43, 'total_tokens': 789, 'input_token_details': {'cache_read': 0}})],\n",
      " 'sub_topic': '',\n",
      " 'topic': 'variables',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b12acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4ae4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Agent Thought: The user wants a multiple choice question on variables. I will use the mcq_agent tool to generate an appropriate MCQ.\n",
      " MCQ Question: \n",
      " Options: []\n",
      " Correct Answer: \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Give me an MCQ on variables\")\n",
    "    ],\n",
    "    \"difficulty_level\": \"beginner\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"variables\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\"\n",
    "}\n",
    "\n",
    "#  Must run through socratic_graph, not call_llm\n",
    "result = socratic_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n Agent Thought:\", result[\"agent_thought\"])\n",
    "print(\" MCQ Question:\", result[\"mcq_question\"])\n",
    "print(\" Options:\", result[\"mcq_options\"])\n",
    "print(\" Correct Answer:\", result[\"mcq_correct_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35390964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52690/812438100.py:10: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(mcq_agent(topic=\"variables\", difficulty=\"beginner\"))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseTool.__call__() got an unexpected keyword argument 'topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# from pprint import pprint\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# pprint(new_state[\"messages\"][-1].content)  # LLM/tool response\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     \"correct_answer\": new_state[\"mcq_correct_answer\"],\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmcq_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvariables\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifficulty\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbeginner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/socra_bot_streamlit/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: BaseTool.__call__() got an unexpected keyword argument 'topic'"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# pprint(new_state[\"messages\"][-1].content)  # LLM/tool response\n",
    "# pprint({\n",
    "#     \"mcq_question\": new_state[\"mcq_question\"],\n",
    "#     \"mcq_options\": new_state[\"mcq_options\"],\n",
    "#     \"correct_answer\": new_state[\"mcq_correct_answer\"],\n",
    "# })\n",
    "\n",
    "print(mcq_agent(topic=\"variables\", difficulty=\"beginner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd29ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socra-bot-streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eebbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--0fb0e40d-3e73-41a5-8652-4141a84de7e0-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1188, 'output_tokens': 25, 'total_tokens': 1269, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 56}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] Raw LLM MCQ Response: '```json\\n{\\n    \"question\": \"Given the following Python code:\\\\n\\\\ndef decorator_a(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(\\\\\"Wrapper A: Before\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(\\\\\"Wrapper A: After\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\ndef decorator_b(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(\\\\\"Wrapper B: Before\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(\\\\\"Wrapper B: After\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@decorator_a\\\\n@decorator_b\\\\ndef my_function():\\\\n    print(\\\\\"Original Function\\\\\")\\\\n\\\\nWhat will be the output when `my_function()` is called?\",\\n    \"options\": [\\n        \"Wrapper A: Before\\\\nOriginal Function\\\\nWrapper A: After\",\\n        \"Wrapper A: Before\\\\nWrapper B: Before\\\\nOriginal Function\\\\nWrapper B: After\\\\nWrapper A: After\",\\n        \"Wrapper B: Before\\\\nWrapper A: Before\\\\nOriginal Function\\\\nWrapper A: After\\\\nWrapper B: After\",\\n        \"Original Function\\\\nWrapper B: Before\\\\nWrapper A: Before\\\\nWrapper A: After\\\\nWrapper B: After\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"Decorators are applied from bottom to top. This means `@decorator_b` is applied first to `my_function`, and then `@decorator_a` is applied to the result of `@decorator_b` (which is `decorator_b`\\'s `wrapper` function).\\\\n\\\\nWhen `my_function()` is called, the execution flow is as follows:\\\\n1.  The outermost decorator\\'s wrapper (`decorator_a`\\'s `wrapper`) executes first. It prints \\\\\"Wrapper A: Before\\\\\".\\\\n2.  `decorator_a`\\'s `wrapper` then calls the function it wrapped, which is `decorator_b`\\'s `wrapper`.\\\\n3.  `decorator_b`\\'s `wrapper` executes. It prints \\\\\"Wrapper B: Before\\\\\".\\\\n4.  `decorator_b`\\'s `wrapper` then calls the function it wrapped, which is the original `my_function`.\\\\n5.  The original `my_function` executes, printing \\\\\"Original Function\\\\\".\\\\n6.  Execution returns to `decorator_b`\\'s `wrapper`, which then prints \\\\\"Wrapper B: After\\\\\".\\\\n7.  Execution returns to `decorator_a`\\'s `wrapper`, which then prints \\\\\"Wrapper A: After\\\\\".\"\\n}\\n```'\n",
      "[DEBUG] Cleaned LLM MCQ Response: '{\\n    \"question\": \"Given the following Python code:  def decorator_a(func):     def wrapper(*args, **kwargs):         print(\"Wrapper A: Before\")         result = func(*args, **kwargs)         print(\"Wrapper A: After\")         return result     return wrapper  def decorator_b(func):     def wrapper(*args, **kwargs):         print(\"Wrapper B: Before\")         result = func(*args, **kwargs)         print(\"Wrapper B: After\")         return result     return wrapper  @decorator_a @decorator_b def my_function():     print(\"Original Function\")  What will be the output when `my_function()` is called?\",\\n    \"options\": [\\n        \"Wrapper A: Before Original Function Wrapper A: After\",\\n        \"Wrapper A: Before Wrapper B: Before Original Function Wrapper B: After Wrapper A: After\",\\n        \"Wrapper B: Before Wrapper A: Before Original Function Wrapper A: After Wrapper B: After\",\\n        \"Original Function Wrapper B: Before Wrapper A: Before Wrapper A: After Wrapper B: After\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"Decorators are applied from bottom to top. This means `@decorator_b` is applied first to `my_function`, and then `@decorator_a` is applied to the result of `@decorator_b` (which is `decorator_b`\\'s `wrapper` function).  When `my_function()` is called, the execution flow is as follows: 1.  The outermost decorator\\'s wrapper (`decorator_a`\\'s `wrapper`) executes first. It prints \"Wrapper A: Before\". 2.  `decorator_a`\\'s `wrapper` then calls the function it wrapped, which is `decorator_b`\\'s `wrapper`. 3.  `decorator_b`\\'s `wrapper` executes. It prints \"Wrapper B: Before\". 4.  `decorator_b`\\'s `wrapper` then calls the function it wrapped, which is the original `my_function`. 5.  The original `my_function` executes, printing \"Original Function\". 6.  Execution returns to `decorator_b`\\'s `wrapper`, which then prints \"Wrapper B: After\". 7.  Execution returns to `decorator_a`\\'s `wrapper`, which then prints \"Wrapper A: After\".\"\\n}\\n'\n",
      "[ERROR] Could not parse cleaned JSON: '{\\n    \"question\": \"Given the following Python code:  def decorator_a(func):     def wrapper(*args, **kwargs):         print(\"Wrapper A: Before\")         result = func(*args, **kwargs)         print(\"Wrapper A: After\")         return result     return wrapper  def decorator_b(func):     def wrapper(*args, **kwargs):         print(\"Wrapper B: Before\")         result = func(*args, **kwargs)         print(\"Wrapper B: After\")         return result     return wrapper  @decorator_a @decorator_b def my_function():     print(\"Original Function\")  What will be the output when `my_function()` is called?\",\\n    \"options\": [\\n        \"Wrapper A: Before Original Function Wrapper A: After\",\\n        \"Wrapper A: Before Wrapper B: Before Original Function Wrapper B: After Wrapper A: After\",\\n        \"Wrapper B: Before Wrapper A: Before Original Function Wrapper A: After Wrapper B: After\",\\n        \"Original Function Wrapper B: Before Wrapper A: Before Wrapper A: After Wrapper B: After\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"Decorators are applied from bottom to top. This means `@decorator_b` is applied first to `my_function`, and then `@decorator_a` is applied to the result of `@decorator_b` (which is `decorator_b`\\'s `wrapper` function).  When `my_function()` is called, the execution flow is as follows: 1.  The outermost decorator\\'s wrapper (`decorator_a`\\'s `wrapper`) executes first. It prints \"Wrapper A: Before\". 2.  `decorator_a`\\'s `wrapper` then calls the function it wrapped, which is `decorator_b`\\'s `wrapper`. 3.  `decorator_b`\\'s `wrapper` executes. It prints \"Wrapper B: Before\". 4.  `decorator_b`\\'s `wrapper` then calls the function it wrapped, which is the original `my_function`. 5.  The original `my_function` executes, printing \"Original Function\". 6.  Execution returns to `decorator_b`\\'s `wrapper`, which then prints \"Wrapper B: After\". 7.  Execution returns to `decorator_a`\\'s `wrapper`, which then prints \"Wrapper A: After\".\"\\n}\\n'\n",
      "[ERROR] JSONDecodeError: Expecting ',' delimiter: line 2 column 124 (char 125)\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c4cbc01d-17b7-432d-83e8-9f55f9ca87df', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}]), ToolMessage(content='Error: MCQ agent returned invalid JSON.', id='5de91f05-70f4-481d-a8d2-7bb65022c9c2', tool_call_id='7e61c2d9-05b3-4859-973a-d90c18da6fbb')]\n",
      "[DEBUG] LLM Response: content='Thought: The user wants an MCQ on Python decorators. My previous attempt to use `llm_mcq_generator` failed. I should try the `mcq_agent` tool, as it might have a predefined question for this topic. I will set the difficulty to \"intermediate\".' additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--d3a36985-6b74-4b55-9adb-420734428ac4-0' tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '8c802060-dc6b-4b3b-b601-4666b4d6c398', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1239, 'output_tokens': 81, 'total_tokens': 1392, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 72}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c4cbc01d-17b7-432d-83e8-9f55f9ca87df', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}]), ToolMessage(content='Error: MCQ agent returned invalid JSON.', id='5de91f05-70f4-481d-a8d2-7bb65022c9c2', tool_call_id='7e61c2d9-05b3-4859-973a-d90c18da6fbb'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='3a816d94-8c57-4bc5-a290-83b508482a68', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '8c802060-dc6b-4b3b-b601-4666b4d6c398', 'type': 'tool_call'}]), ToolMessage(content='No predefined MCQ found for this topic.', id='7c7c2d18-a87c-401f-af05-e6c557f6e86e', tool_call_id='8c802060-dc6b-4b3b-b601-4666b4d6c398')]\n",
      "[DEBUG] LLM Response: content=\"Thought: I tried to generate an MCQ on Python decorators using both the `llm_mcq_generator` and `mcq_agent` tools, but neither was successful. The `llm_mcq_generator` returned an error, and the `mcq_agent` indicated that there are no predefined MCQs for decorators. Therefore, I cannot provide an MCQ on that topic right now. I should let the user know and offer an alternative.\\n\\nIt seems I don't have a pre-built MCQ on Python decorators at the moment. Would you like to try an MCQ on a different Python topic, or perhaps a coding challenge related to decorators?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--2e6f596e-b845-42ee-82ca-2dfa78cf735c-0' usage_metadata={'input_tokens': 1284, 'output_tokens': 136, 'total_tokens': 1521, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 101}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent's turn ends (no tool calls).\n",
      "{'agent_thought': 'I tried to generate an MCQ on Python decorators using both '\n",
      "                  'the `llm_mcq_generator` and `mcq_agent` tools, but neither '\n",
      "                  'was successful. The `llm_mcq_generator` returned an error, '\n",
      "                  'and the `mcq_agent` indicated that there are no predefined '\n",
      "                  'MCQs for decorators. Therefore, I cannot provide an MCQ on '\n",
      "                  'that topic right now. I should let the user know and offer '\n",
      "                  'an alternative.',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c4cbc01d-17b7-432d-83e8-9f55f9ca87df', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='Error: MCQ agent returned invalid JSON.', id='5de91f05-70f4-481d-a8d2-7bb65022c9c2', tool_call_id='7e61c2d9-05b3-4859-973a-d90c18da6fbb'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='3a816d94-8c57-4bc5-a290-83b508482a68', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '8c802060-dc6b-4b3b-b601-4666b4d6c398', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='No predefined MCQ found for this topic.', id='7c7c2d18-a87c-401f-af05-e6c557f6e86e', tool_call_id='8c802060-dc6b-4b3b-b601-4666b4d6c398'),\n",
      "              AIMessage(content=\"It seems I don't have a pre-built MCQ on Python decorators at the moment. Would you like to try an MCQ on a different Python topic, or perhaps a coding challenge related to decorators?\", additional_kwargs={}, response_metadata={}, id='89b864a8-eef3-4c2d-8f3c-ea41232009d6')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': '',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "import time\n",
    "import re # Import regex for answer parsing\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic tutoring agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: A list of BaseMessage objects representing the conversation history.\n",
    "                  Annotated with add_messages to automatically append new messages.\n",
    "        difficulty_level: The current difficulty level of the tutoring session (e.g., \"beginner\", \"intermediate\").\n",
    "        user_struggle_count: An integer tracking how many times the user has struggled or answered incorrectly.\n",
    "        topic: The main Python topic currently being discussed.\n",
    "        sub_topic: A more specific sub-topic within the main topic.\n",
    "        mcq_active: A boolean indicating if a Multiple Choice Question is currently active.\n",
    "        mcq_question: The full text of the active MCQ, including options.\n",
    "        mcq_options: A list of strings, each representing an option for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer (e.g., \"A\", \"B\", \"C\", \"D\") for the active MCQ.\n",
    "        agent_thought: The internal thought process of the Socratic LLM before generating a response.\n",
    "        next_node_decision: A string indicating the next node the router should transition to.\n",
    "                            Used by the supervisor/router to control graph flow.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node_decision: str\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the main Socratic LLM for general conversation and tool binding.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "\n",
    "# Initialize a separate LLM for generating MCQs. This allows for different\n",
    "# temperature or model settings specifically for MCQ generation.\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5)\n",
    "\n",
    "# System prompt for the Socratic LLM, guiding its behavior and principles.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides code and asks for feedback.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question for **well-known or predefined topics** (like \"variables\", \"functions\", \"classes\"). This tool has pre-built questions.\n",
    "    * `llm_mcq_generator`: Use this when the user asks for an MCQ on a topic that is **not explicitly covered by the `mcq_agent`'s predefined list**, or if you believe a more custom or nuanced question is needed based on the current discussion. This tool will ask the LLM to create a new MCQ.\n",
    "    * `mcq_answer_processor`: Use this tool when the user submits an answer to an active MCQ. Provide the user's answer and the correct answer to this tool. This tool will handle updating the struggle count and resetting the MCQ state.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "MCQ Question (internal): {mcq_question} # Note: This is now the formatted string\n",
    "MCQ Options (internal): {mcq_options}\n",
    "MCQ Correct Answer (internal): {mcq_correct_answer}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Chat prompt template for the Socratic LLM, including system prompt and message history.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Tools ---\n",
    "# These tools simulate external functionalities that the Socratic LLM can call.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code.\n",
    "    This is a simulated tool. In a real application, it would run static analysis, linters, etc.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept.\n",
    "    This is a simulated tool. In a real application, it would provide detailed explanations.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge based on a topic and difficulty level.\n",
    "    This is a simulated tool. In a real application, it would generate a specific coding problem.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level\n",
    "    from a predefined list. The output will be a JSON string containing the question,\n",
    "    options, and correct answer. The 'question' field will be pre-formatted to include\n",
    "    options for direct display.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ\n",
    "    and a predefined question is available for the topic.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the exact topic exists in our predefined list (case-insensitive)\n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        # Format the question to include options for direct display in chat\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        # If topic not found, return a special string to indicate that the LLM should\n",
    "        # consider using the `llm_mcq_generator` tool instead.\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an MCQ using an LLM based on a topic and difficulty level.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python tutor who generates multiple choice questions (MCQs) for practice.\n",
    "Generate an MCQ on the topic \"{topic}\" at a \"{difficulty}\" level.\n",
    "\n",
    "The MCQ must follow this format strictly as a JSON object:\n",
    "{{\n",
    "    \"question\": \"string\",\n",
    "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "    \"answer_index\": 1,\n",
    "    \"explanation\": \"string\"\n",
    "}}\n",
    "\n",
    "For questions involving code snippets, format the code within triple backticks (```) to preserve readability, and ensure all strings are JSON-compatible (newlines escaped as \\\\n).\n",
    "DO NOT include outer markdown code fences like ```json or ```python\n",
    "Respond with raw valid JSON only. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    raw_llm_content = llm_response.content.strip()\n",
    "    print(\"[DEBUG] Raw LLM MCQ Response:\", repr(raw_llm_content))\n",
    "\n",
    "    # Strip outer markdown fences\n",
    "    cleaned_content = re.sub(r'^```(json|python)?\\n?', '', raw_llm_content, flags=re.MULTILINE)\n",
    "    cleaned_content = re.sub(r'\\n?```$', '', cleaned_content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace problematic characters, but preserve code formatting\n",
    "    cleaned_content = (\n",
    "        cleaned_content\n",
    "        .replace('“', '\"').replace('”', '\"')  # Replace smart quotes\n",
    "        .replace('‘', \"'\").replace('’', \"'\")  # Replace smart single quotes\n",
    "        .replace('\\u201c', '\"').replace('\\u201d', '\"')  # Replace Unicode quotes\n",
    "        .replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Replace Unicode single quotes\n",
    "        .replace('\\t', '    ')  # Replace tabs with spaces for code readability\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Cleaned LLM MCQ Response:\", repr(cleaned_content))\n",
    "\n",
    "    try:\n",
    "        mcq_data = json.loads(cleaned_content)\n",
    "        # Validate JSON structure\n",
    "        required_keys = {\"question\", \"options\", \"answer_index\", \"explanation\"}\n",
    "        if not all(key in mcq_data for key in required_keys):\n",
    "            raise ValueError(\"Invalid MCQ format: Missing required keys\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"Invalid MCQ format: Options must be a list of 4 strings\")\n",
    "        if not isinstance(mcq_data[\"answer_index\"], int) or mcq_data[\"answer_index\"] not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid MCQ format: answer_index must be an integer between 0 and 3\")\n",
    "        \n",
    "        # Format question for display with options\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data[\"options\"])\n",
    "        mcq_data[\"question\"] = formatted_question\n",
    "        return mcq_data\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(\"[ERROR] JSON parsing or validation failed:\", str(e))\n",
    "        print(\"[ERROR] Cleaned content:\", repr(cleaned_content))\n",
    "        default_mcq = {\n",
    "            \"question\": \"**What is a Python decorator?**\\n\\n\" + \n",
    "                        \"\\n\".join([\n",
    "                            \"A) A function that modifies another function or method\",\n",
    "                            \"B) A type of class inheritance\",\n",
    "                            \"C) A syntax for defining variables\",\n",
    "                            \"D) A loop construct\"\n",
    "                        ]),\n",
    "            \"options\": [\n",
    "                \"A) A function that modifies another function or method\",\n",
    "                \"B) A type of class inheritance\",\n",
    "                \"C) A syntax for defining variables\",\n",
    "                \"D) A loop construct\"\n",
    "            ],\n",
    "            \"answer_index\": 0,\n",
    "            \"explanation\": \"A Python decorator is a function that wraps another function or method to extend or modify its behavior.\"\n",
    "        }\n",
    "        return {\"error\": f\"Failed to parse or validate MCQ JSON: {str(e)}\", **default_mcq}\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ.\n",
    "    Compares the user's answer with the correct answer and returns feedback.\n",
    "    This tool is used by the `process_mcq_answer` node.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return \"Correct!\"\n",
    "    else:\n",
    "        return \"Incorrect.\"\n",
    "\n",
    "\n",
    "# List of all tools available to the Socratic LLM\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent, llm_mcq_generator, mcq_answer_processor]\n",
    "# Bind the tools to the main Socratic LLM, allowing it to call them.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "# Combine the prompt and the LLM with tools into a runnable for the Socratic agent.\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Invokes the Socratic LLM with the current conversation history and state.\n",
    "    This node represents the core logic of the Socratic tutoring agent.\n",
    "    It extracts the LLM's \"thought\" and formats the content for display.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_llm node.\")\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    \n",
    "    # Invoke the Socratic agent runnable with the current state.\n",
    "    # Exclude 'messages' and 'next_node_decision' as they are handled separately by LangGraph/prompt.\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        **{k: v for k, v in state.items() if k not in ['messages', 'next_node_decision']}\n",
    "    })\n",
    "    print(\"[DEBUG] LLM Response:\", response)\n",
    "    \n",
    "    thought = \"\"\n",
    "    display_content = response.content # Initialize with the full content from LLM\n",
    "\n",
    "    # Extract the internal \"Thought:\" from the LLM's response if present.\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        thought_and_content = parts[1].strip()\n",
    "        \n",
    "        # The thought is typically the first line after \"Thought:\"\n",
    "        thought_lines = thought_and_content.split('\\n', 1)\n",
    "        thought = thought_lines[0].strip()\n",
    "        \n",
    "        # The display content is the rest, if any\n",
    "        if len(thought_lines) > 1:\n",
    "            display_content = thought_lines[1].strip()\n",
    "        else:\n",
    "            display_content = \"\" # If only thought was present, hide it from user display\n",
    "\n",
    "    # Create a new AIMessage to add to the state, preserving tool calls.\n",
    "    new_ai_message = AIMessage(\n",
    "        content=display_content,\n",
    "        tool_calls=response.tool_calls\n",
    "    )\n",
    "\n",
    "    # Return the updated messages and the extracted thought.\n",
    "    return {\"messages\": [new_ai_message], \"agent_thought\": thought}\n",
    "\n",
    "# A dictionary mapping tool names to their corresponding Python functions.\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "    \"llm_mcq_generator\": llm_mcq_generator,\n",
    "    \"mcq_answer_processor\": mcq_answer_processor,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Executes a tool call requested by the Socratic LLM.\n",
    "    Updates the state with the tool's output and relevant MCQ information if applicable.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_tool node.\")\n",
    "    last_message = state[\"messages\"][-1] # Get the last message, which should contain tool calls.\n",
    "    \n",
    "    messages_to_add = [] # List to accumulate new messages (e.g., ToolMessage)\n",
    "    state_updates = {}   # Dictionary to accumulate state changes\n",
    "\n",
    "    # Check if the last message is an AIMessage and contains tool calls.\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name) # Get the actual function for the tool.\n",
    "            \n",
    "            tool_output_content = \"\"\n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args) # Invoke the tool.\n",
    "                tool_output_content = str(response) # Convert tool response to string.\n",
    "\n",
    "                # Special handling for MCQ generation tools (`mcq_agent`, `llm_mcq_generator`)\n",
    "                if tool_name in [\"mcq_agent\", \"llm_mcq_generator\"]:\n",
    "                    if tool_output_content == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "                        # If mcq_agent couldn't find a predefined MCQ, add a ToolMessage\n",
    "                        # and continue, allowing the LLM to potentially call llm_mcq_generator next.\n",
    "                        messages_to_add.append(\n",
    "                            ToolMessage(content=\"No predefined MCQ found for this topic.\", tool_call_id=tool_call[\"id\"])\n",
    "                        )\n",
    "                        continue # Skip to next tool call or finish this loop.\n",
    "                    \n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content) if isinstance(response, str) else response\n",
    "                        # Check if the MCQ generation tool returned an error.\n",
    "                        if \"error\" in mcq_data:\n",
    "                            messages_to_add.append(\n",
    "                                ToolMessage(content=f\"Error generating MCQ: {mcq_data['error']}\", tool_call_id=tool_call[\"id\"])\n",
    "                            )\n",
    "                            continue # Skip if there was an error.\n",
    "\n",
    "                        # Update state with MCQ details if generation was successful.\n",
    "                        state_updates[\"mcq_active\"] = True\n",
    "                        state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\")\n",
    "                        state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                        state_updates[\"mcq_correct_answer\"] = mcq_data.get(\"correct_answer\", \"\")\n",
    "                        # If a topic wasn't set yet in the state, set it from the tool arguments.\n",
    "                        if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                            state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Handle cases where the tool's output is not valid JSON.\n",
    "                        messages_to_add.append(\n",
    "                            ToolMessage(content=\"Error: MCQ agent returned invalid JSON.\", tool_call_id=tool_call[\"id\"])\n",
    "                        )\n",
    "                        continue # Skip to next tool call or finish this loop.\n",
    "\n",
    "                # Special handling for the `mcq_answer_processor` tool.\n",
    "                elif tool_name == \"mcq_answer_processor\":\n",
    "                    if tool_output_content == \"Correct!\":\n",
    "                        state_updates[\"user_struggle_count\"] = 0 # Reset struggle count on correct answer.\n",
    "                    else:\n",
    "                        # Increment struggle count on incorrect answer.\n",
    "                        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "                    \n",
    "                    # Reset MCQ state after processing an answer.\n",
    "                    state_updates[\"mcq_active\"] = False\n",
    "                    state_updates[\"mcq_question\"] = \"\"\n",
    "                    state_updates[\"mcq_options\"] = []\n",
    "                    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "                \n",
    "                # For all other tools, just append their output as a ToolMessage.\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # If a tool function is not found, add an error message.\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "    print(f\"[DEBUG] call_tool: State updates - {state_updates}\")\n",
    "    # Return the new messages and any state updates.\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "def router(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    The supervisor node that decides the next action based on the current state and user input.\n",
    "    It primarily routes between processing an MCQ answer directly or letting the Socratic LLM respond.\n",
    "    \n",
    "    Returns a dictionary containing 'next_node_decision' to control graph flow.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering router node.\")\n",
    "    last_message = state[\"messages\"][-1] # Get the latest message in the conversation.\n",
    "    \n",
    "    next_decision = \"\"\n",
    "    # Check if an MCQ is active and the user's last message looks like an MCQ answer.\n",
    "    if state.get(\"mcq_active\", False) and isinstance(last_message, HumanMessage):\n",
    "        user_input = last_message.content.strip().upper()\n",
    "        # Use regex to robustly match typical MCQ answer formats (A, B, C, D, A., B), etc.\n",
    "        if re.match(r\"^[ABCD](\\.|\\))?$\", user_input):\n",
    "            print(\"[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\")\n",
    "            next_decision = \"process_mcq_answer\"\n",
    "        else:\n",
    "            print(\"[DEBUG] Router: MCQ active but no valid answer, routing to call_llm (Socratic Agent).\")\n",
    "            next_decision = \"call_llm\" # If MCQ active but answer is not valid, let LLM handle it\n",
    "    else:\n",
    "        print(\"[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\")\n",
    "        next_decision = \"call_llm\"\n",
    "    \n",
    "    # Return a dictionary with the decision. LangGraph nodes must return a dict.\n",
    "    return {\"next_node_decision\": next_decision}\n",
    "\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Processes the user's MCQ answer by calling the mcq_answer_processor tool directly.\n",
    "    This node is invoked by the `router` when an MCQ answer is detected.\n",
    "    It updates the state based on the correctness of the answer and provides feedback.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering process_mcq_answer node.\")\n",
    "    last_human_message = None\n",
    "    # Find the most recent HumanMessage in the history.\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        print(\"[ERROR] process_mcq_answer: Could not find a HumanMessage to process.\")\n",
    "        return {\"messages\": []} # If no human message, nothing to process.\n",
    "\n",
    "    user_answer = last_human_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "\n",
    "    # Directly call the `mcq_answer_processor` tool function.\n",
    "    tool_output_content = mcq_answer_processor.invoke({\"user_answer\": user_answer, \"correct_answer\": correct_answer})\n",
    "    \n",
    "    state_updates = {}\n",
    "    messages_to_add = []\n",
    "\n",
    "    # Update struggle count based on the tool's output.\n",
    "    if tool_output_content == \"Correct!\":\n",
    "        state_updates[\"user_struggle_count\"] = 0\n",
    "    else:\n",
    "        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    # Reset all MCQ-related state variables as the MCQ has been answered.\n",
    "    state_updates[\"mcq_active\"] = False\n",
    "    state_updates[\"mcq_question\"] = \"\"\n",
    "    state_updates[\"mcq_options\"] = []\n",
    "    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "\n",
    "    # Add a ToolMessage to the history to record the tool's execution.\n",
    "    messages_to_add.append(\n",
    "        ToolMessage(content=tool_output_content, tool_call_id=\"mcq_processor_direct_call\") # Using a dummy ID for direct call.\n",
    "    )\n",
    "    # Add an AIMessage to provide immediate feedback to the user.\n",
    "    messages_to_add.append(\n",
    "        AIMessage(content=f\"You answered: {user_answer}. The correct answer was {correct_answer}. {tool_output_content}\")\n",
    "    )\n",
    "\n",
    "    print(f\"[DEBUG] process_mcq_answer: Result - {tool_output_content}, New struggle count: {state_updates.get('user_struggle_count')}\")\n",
    "    \n",
    "    # Return the new messages and any state updates.\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue_socratic(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Determines if the Socratic LLM (after its `call_llm` node) needs to call a tool\n",
    "    or if its turn is complete (i.e., it has generated a direct response).\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering should_continue_socratic edge logic.\")\n",
    "    # If the last message from the LLM contains tool calls, transition to `call_tool`.\n",
    "    if isinstance(state[\"messages\"][-1], AIMessage) and state[\"messages\"][-1].tool_calls:\n",
    "        print(\"[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\")\n",
    "        return \"call_tool\"\n",
    "    # Otherwise, the LLM's turn is complete, and the graph can end for this iteration.\n",
    "    print(\"[DEBUG] should_continue_socratic: Socratic agent's turn ends (no tool calls).\")\n",
    "    return END\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Initialize the StateGraph with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add all the nodes to the workflow.\n",
    "workflow.add_node(\"router\", router) # The new supervisor node.\n",
    "workflow.add_node(\"call_llm\", call_llm) # The Socratic agent's LLM logic.\n",
    "workflow.add_node(\"call_tool\", call_tool) # The tool execution logic.\n",
    "workflow.add_node(\"process_mcq_answer\", process_mcq_answer) # Node for direct MCQ answer processing.\n",
    "\n",
    "# Set the `router` node as the starting point of the graph.\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Define conditional edges from the `router` node.\n",
    "# The `router` function itself determines the next node based on the state.\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_node_decision\"], # Use the decision stored in state by the router function.\n",
    "    {\n",
    "        \"call_llm\": \"call_llm\",         # If router decides, go to the Socratic LLM.\n",
    "        \"process_mcq_answer\": \"process_mcq_answer\" # If router detects MCQ answer, go to process it.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define conditional edges from the `call_llm` node (Socratic agent's LLM).\n",
    "# `should_continue_socratic` determines if a tool needs to be called or if the turn ends.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue_socratic,\n",
    "    {\"call_tool\": \"call_tool\", END: END} # If tool calls, go to `call_tool`; otherwise, end.\n",
    ")\n",
    "\n",
    "# Define a regular edge from `call_tool` back to `call_llm`.\n",
    "# After a tool is executed, the Socratic LLM needs to process the tool's output and generate a response.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Define a regular edge from `process_mcq_answer` back to `call_llm`.\n",
    "# After an MCQ answer is processed, the Socratic LLM needs to provide feedback and potentially a new question.\n",
    "workflow.add_edge(\"process_mcq_answer\", \"call_llm\")\n",
    "\n",
    "# Compile the workflow into a runnable graph.\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\", # Topic will be set by the tool if not provided by user\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\" # Initialize this for the router\n",
    "}\n",
    "result_llm_mcq = socratic_graph.invoke(llm_mcq_state)\n",
    "import pprint\n",
    "pprint.pprint(result_llm_mcq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e173d06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--44ef6222-2b3c-4e4a-bd41-507b75ea8720-0' tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1267, 'output_tokens': 22, 'total_tokens': 1375, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 86}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='47ea534d-9f7d-49f1-b206-5cd5a6bc6a6b', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}]), ToolMessage(content='No predefined MCQ found for this topic.', id='3a06a2ac-9c94-4ad5-a54b-640fed0719df', tool_call_id='04aca61f-8333-447c-91f4-0a32de1a12fd')]\n",
      "[DEBUG] LLM Response: content='Thought: The user wants an MCQ on Python decorators, and the `mcq_agent` indicated that there isn\\'t a predefined one. I should use the `llm_mcq_generator` to create a new MCQ for them, maintaining the \"intermediate\" difficulty level.' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--4ce39ecc-e3e2-48c0-b5cb-7392448c9562-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '047b41d1-acd0-4221-a8b4-b0285513ab26', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1312, 'output_tokens': 83, 'total_tokens': 1462, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 67}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[ERROR] Unexpected error in llm_mcq_generator: '\\n            \"question\"'\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='47ea534d-9f7d-49f1-b206-5cd5a6bc6a6b', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}]), ToolMessage(content='No predefined MCQ found for this topic.', id='3a06a2ac-9c94-4ad5-a54b-640fed0719df', tool_call_id='04aca61f-8333-447c-91f4-0a32de1a12fd'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ea66e548-b4ae-4382-82e5-5607dccf2ca8', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '047b41d1-acd0-4221-a8b4-b0285513ab26', 'type': 'tool_call'}]), ToolMessage(content='Error generating MCQ: An unexpected error occurred during MCQ generation: \\'\\\\n            \"question\"\\'', id='1cbdb3a5-b7da-44f8-a2c3-3018a7f39a48', tool_call_id='047b41d1-acd0-4221-a8b4-b0285513ab26')]\n",
      "[DEBUG] LLM Response: content='I apologize, but I encountered an error while trying to generate an MCQ on Python decorators at this moment.\\n\\nHowever, we can still explore the topic! To get started, what do you understand a Python decorator to be, in your own words?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--b4d4e97b-9659-4f7e-bfcc-99973bb7bfa7-0' usage_metadata={'input_tokens': 1375, 'output_tokens': 50, 'total_tokens': 1544, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 119}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent's turn ends (no tool calls).\n",
      "{'agent_thought': '',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='47ea534d-9f7d-49f1-b206-5cd5a6bc6a6b', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='No predefined MCQ found for this topic.', id='3a06a2ac-9c94-4ad5-a54b-640fed0719df', tool_call_id='04aca61f-8333-447c-91f4-0a32de1a12fd'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ea66e548-b4ae-4382-82e5-5607dccf2ca8', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '047b41d1-acd0-4221-a8b4-b0285513ab26', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='Error generating MCQ: An unexpected error occurred during MCQ generation: \\'\\\\n            \"question\"\\'', id='1cbdb3a5-b7da-44f8-a2c3-3018a7f39a48', tool_call_id='047b41d1-acd0-4221-a8b4-b0285513ab26'),\n",
      "              AIMessage(content='I apologize, but I encountered an error while trying to generate an MCQ on Python decorators at this moment.\\n\\nHowever, we can still explore the topic! To get started, what do you understand a Python decorator to be, in your own words?', additional_kwargs={}, response_metadata={}, id='a1765dc0-9e10-48d4-bef8-d349d89efa3c')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': '',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- Test ---\n",
    "# This section provides example invocations to test the graph's behavior.\n",
    "# It includes tests for LLM-generated MCQs, correct/incorrect MCQ answers,\n",
    "# and regular conversation turns.\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    # Attempt to draw and display the graph for visualization.\n",
    "    display(Image(socratic_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw graph: {e}. This is often due to missing graphviz or pydot.\")\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\", # Topic will be set by the tool if not provided by user\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\" # Initialize this for the router\n",
    "}\n",
    "result_llm_mcq = socratic_graph.invoke(llm_mcq_state)\n",
    "import pprint\n",
    "pprint.pprint(result_llm_mcq)\n",
    "\n",
    "# Test answering the MCQ (Correct Answer)\n",
    "# print(\"\\n--- Testing answering the MCQ (Correct Answer) ---\")\n",
    "# # Use the state from the previous step which now contains the active MCQ.\n",
    "# mcq_answer_state_correct = result_llm_mcq \n",
    "# mcq_answer_state_correct[\"messages\"].append(HumanMessage(content=\"B\")) # User answers 'B'\n",
    "# mcq_answer_state_correct[\"next_node_decision\"] = \"\" # Reset for router to make a fresh decision\n",
    "# result_mcq_answer_correct = socratic_graph.invoke(mcq_answer_state_correct)\n",
    "# pprint.pprint(result_mcq_answer_correct)\n",
    "\n",
    "# # Test answering the MCQ (Incorrect Answer)\n",
    "# print(\"\\n--- Testing answering the MCQ (Incorrect Answer) ---\")\n",
    "# # Reset state for a new MCQ test to ensure independence.\n",
    "# llm_mcq_state_2 = {\n",
    "#     \"messages\": [HumanMessage(content=\"Can you give me another MCQ, this time on Python generators?\")],\n",
    "#     \"difficulty_level\": \"advanced\",\n",
    "#     \"user_struggle_count\": 0,\n",
    "#     \"topic\": \"\",\n",
    "#     \"sub_topic\": \"\",\n",
    "#     \"mcq_active\": False,\n",
    "#     \"mcq_question\": \"\",\n",
    "#     \"mcq_options\": [],\n",
    "#     \"mcq_correct_answer\": \"\",\n",
    "#     \"agent_thought\": \"\",\n",
    "#     \"next_node_decision\": \"\"\n",
    "# }\n",
    "# result_llm_mcq_2 = socratic_graph.invoke(llm_mcq_state_2)\n",
    "# pprint.pprint(result_llm_mcq_2)\n",
    "\n",
    "# mcq_answer_state_incorrect = result_llm_mcq_2 # Use the state from the previous step.\n",
    "# mcq_answer_state_incorrect[\"messages\"].append(HumanMessage(content=\"A\")) # User answers 'A'\n",
    "# mcq_answer_state_incorrect[\"next_node_decision\"] = \"\" # Reset for router\n",
    "# result_mcq_answer_incorrect = socratic_graph.invoke(mcq_answer_state_incorrect)\n",
    "# pprint.pprint(result_mcq_answer_incorrect)\n",
    "\n",
    "# print(\"\\n--- Testing a regular conversation turn after MCQ ---\")\n",
    "# # Continue conversation after an MCQ has been answered.\n",
    "# regular_turn_state = result_mcq_answer_incorrect # Use the state from the previous step.\n",
    "# regular_turn_state[\"messages\"].append(HumanMessage(content=\"Can you explain more about generators?\"))\n",
    "# regular_turn_state[\"next_node_decision\"] = \"\" # Reset for router\n",
    "# result_regular_turn = socratic_graph.invoke(regular_turn_state)\n",
    "# pprint.pprint(result_regular_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ccd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--40cc2315-b1a2-4e50-94ba-966a1f5077c4-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1139, 'output_tokens': 25, 'total_tokens': 1228, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 64}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] Raw LLM MCQ Response: '```json\\n{\\n    \"question\": \"Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\\\\\"Executing {func.__name__}...\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\\\\\"Finished {func.__name__}.\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```\",\\n    \"options\": [\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\",\\n        \"8\",\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\",\\n        \"Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\"\\n    ],\\n    \"answer_index\": 2,\\n    \"explanation\": \"The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\\\\\"Executing {func.__name__}...\\\\\")` is called first, printing \\\\\"Executing add_numbers...\\\\\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\\\\\"Finished {func.__name__}.\\\\\")` is called next, printing \\\\\"Finished add_numbers.\\\\\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\"\\n}\\n```'\n",
      "[DEBUG] Cleaned LLM MCQ Response: '{\\n    \"question\": \"Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\\\\\"Executing {func.__name__}...\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\\\\\"Finished {func.__name__}.\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```\",\\n    \"options\": [\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\",\\n        \"8\",\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\",\\n        \"Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\"\\n    ],\\n    \"answer_index\": 2,\\n    \"explanation\": \"The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\\\\\"Executing {func.__name__}...\\\\\")` is called first, printing \\\\\"Executing add_numbers...\\\\\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\\\\\"Finished {func.__name__}.\\\\\")` is called next, printing \\\\\"Finished add_numbers.\\\\\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\"\\n}\\n'\n",
      "[DEBUG] call_tool: State updates - {'mcq_active': True, 'mcq_question': '**Consider the following Python code:', 'mcq_options': ['Executing add_numbers...\\\\nFinished add_numbers.', '8', 'Executing add_numbers...\\\\nFinished add_numbers.\\\\n8', 'Executing add_numbers...\\\\n8\\\\nFinished add_numbers.'], 'mcq_correct_answer': 'C', 'mcq_explanation': 'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', 'topic': 'Python decorators'}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7')]\n",
      "[DEBUG] LLM Response: content='Please select an option (A, B, C, or D).' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--3a563456-983d-4d74-bc41-0ce04d0da051-0' usage_metadata={'input_tokens': 1985, 'output_tokens': 14, 'total_tokens': 2023, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 24}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\n",
      "{'agent_thought': '',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': True,\n",
      " 'mcq_correct_answer': 'C',\n",
      " 'mcq_explanation': 'The `@log_execution` decorator wraps the `add_numbers` '\n",
      "                    'function. When `add_numbers(5, 3)` is called, the '\n",
      "                    '`wrapper` function inside `log_execution` is executed.\\n'\n",
      "                    '1. `print(f\"Executing {func.__name__}...\")` is called '\n",
      "                    'first, printing \"Executing add_numbers...\".\\n'\n",
      "                    '2. `result = func(*args, **kwargs)` then calls '\n",
      "                    '`add_numbers(5, 3)`, which returns `8`. This `8` is '\n",
      "                    'stored in the `result` variable.\\n'\n",
      "                    '3. `print(f\"Finished {func.__name__}.\")` is called next, '\n",
      "                    'printing \"Finished add_numbers.\".\\n'\n",
      "                    '4. Finally, `return result` returns `8`. If this call is '\n",
      "                    'made in an interactive Python session (like a REPL) or if '\n",
      "                    'its return value is implicitly or explicitly printed '\n",
      "                    '(e.g., `print(add_numbers(5, 3))`), the value `8` will be '\n",
      "                    \"displayed after the decorator's print statements, \"\n",
      "                    'resulting in the combined output:\\n'\n",
      "                    '```\\n'\n",
      "                    'Executing add_numbers...\\n'\n",
      "                    'Finished add_numbers.\\n'\n",
      "                    '8\\n'\n",
      "                    '```',\n",
      " 'mcq_options': ['Executing add_numbers...\\\\nFinished add_numbers.',\n",
      "                 '8',\n",
      "                 'Executing add_numbers...\\\\nFinished add_numbers.\\\\n8',\n",
      "                 'Executing add_numbers...\\\\n8\\\\nFinished add_numbers.'],\n",
      " 'mcq_question': '**Consider the following Python code:',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 0}\n",
      "\n",
      "--- Testing MCQ answer ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\n",
      "[DEBUG] Entering process_mcq_answer node.\n",
      "[DEBUG] process_mcq_answer: Result - Incorrect.\n",
      "\n",
      "Explanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\n",
      "1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\n",
      "2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\n",
      "3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\n",
      "4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator's print statements, resulting in the combined output:\n",
      "```\n",
      "Executing add_numbers...\n",
      "Finished add_numbers.\n",
      "8\n",
      "```, New struggle count: 1\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5')]\n",
      "[DEBUG] LLM Response: content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--47b5e59d-f8c2-41d8-9a64-c2375fdde7a2-0' usage_metadata={'input_tokens': 2205, 'output_tokens': 118, 'total_tokens': 2323, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 1/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 500 An internal error has '\n",
      "                  'occurred. Please retry or report in '\n",
      "                  'https://developers.generativeai.google/guide/troubleshooting',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'),\n",
      "              AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb')],\n",
      " 'next_node_decision': 'process_mcq_answer',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n",
      "\n",
      "--- Testing follow-up question response ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610')]\n",
      "[DEBUG] LLM Response: content=\"Thought: The user has explained the purpose of `*args` and `**kwargs` very well, showing a good understanding of their role in making decorators flexible. This indicates they understand a key aspect of decorator implementation. Now, I should guide them to think about the *return value* of the `wrapper` function and why it's important for the decorated function to behave as expected.\\nThat's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--68242563-b31d-4d7f-b33f-6882bb62d290-0' usage_metadata={'input_tokens': 2402, 'output_tokens': 161, 'total_tokens': 2563, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'), AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 1/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 500 An internal error has '\n",
      "                  'occurred. Please retry or report in '\n",
      "                  'https://developers.generativeai.google/guide/troubleshooting',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'),\n",
      "              AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'),\n",
      "              AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n",
      "\n",
      "--- Testing follow-up question response ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'), AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='eed2f46d-fa5f-4320-95cd-e74ea9d80b6b')]\n",
      "[DEBUG] LLM Response: content=\"That's a great point about `*args` and `**kwargs` making decorators versatile!\\n\\nLet's revisit the previous question to ensure we cover all aspects. In the `log_execution` decorator, we have this line: `return result`.\\n\\nWhat would be the practical consequence if we *removed* that line, and the `wrapper` function didn't explicitly return `result`? How would that change what `add_numbers(5, 3)` gives us?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--bf0cb6f8-a59c-4708-a768-0d0e3a49af4c-0' usage_metadata={'input_tokens': 2561, 'output_tokens': 101, 'total_tokens': 2760, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 98}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'), AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='eed2f46d-fa5f-4320-95cd-e74ea9d80b6b'), AIMessage(content=\"That's a great point about `*args` and `**kwargs` making decorators versatile!\\n\\nLet's revisit the previous question to ensure we cover all aspects. In the `log_execution` decorator, we have this line: `return result`.\\n\\nWhat would be the practical consequence if we *removed* that line, and the `wrapper` function didn't explicitly return `result`? How would that change what `add_numbers(5, 3)` gives us?\", additional_kwargs={}, response_metadata={}, id='63c94b00-523b-41c4-91a8-3ccdb760f599')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 1\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 429 You exceeded your current '\n",
      "                  'quota, please check your plan and billing details. For more '\n",
      "                  'information on this error, head to: '\n",
      "                  'https://ai.google.dev/gemini-api/docs/rate-limits. '\n",
      "                  '[violations {\\n'\n",
      "                  '  quota_metric: '\n",
      "                  '\"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\\n'\n",
      "                  '  quota_id: '\n",
      "                  '\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"model\"\\n'\n",
      "                  '    value: \"gemini-2.5-flash\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"location\"\\n'\n",
      "                  '    value: \"global\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_value: 10\\n'\n",
      "                  '}\\n'\n",
      "                  ', links {\\n'\n",
      "                  '  description: \"Learn more about Gemini API quotas\"\\n'\n",
      "                  '  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n'\n",
      "                  '}\\n'\n",
      "                  ', retry_delay {\\n'\n",
      "                  '  seconds: 57\\n'\n",
      "                  '}\\n'\n",
      "                  ']',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'),\n",
      "              AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'),\n",
      "              AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='eed2f46d-fa5f-4320-95cd-e74ea9d80b6b'),\n",
      "              AIMessage(content=\"That's a great point about `*args` and `**kwargs` making decorators versatile!\\n\\nLet's revisit the previous question to ensure we cover all aspects. In the `log_execution` decorator, we have this line: `return result`.\\n\\nWhat would be the practical consequence if we *removed* that line, and the `wrapper` function didn't explicitly return `result`? How would that change what `add_numbers(5, 3)` gives us?\", additional_kwargs={}, response_metadata={}, id='63c94b00-523b-41c4-91a8-3ccdb760f599'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='d4d777a9-0903-406f-9ff3-6ad686d4bdfe')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n",
      "\n",
      "--- Testing follow-up question response ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='d19206c2-7eaf-41af-b92a-99e94a219d4b'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ba820959-b482-4d15-a54b-cf02589ef3f1', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '0b4310dd-58ed-4ed2-9bdf-21eff844f751', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef uppercase_args_decorator(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\\\n        return func(*processed_args, **processed_kwargs)\\\\n    return wrapper\\\\n\\\\n@uppercase_args_decorator\\\\ndef concatenate_strings(str1, str2, separator=\" \"):\\\\n    return f\"{str1}{separator}{str2}\"\\\\n\\\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\\\n```\\\\nWhat will be the output of this code?**\\\\n\\\\nA) `hello-world`\\\\nB) `HELLO-WORLD`\\\\nC) `HELLO-world`\\\\nD) A `TypeError` will be raised because `separator` is not a string.\\', \\'options\\': [\\'A) `hello-world`\\', \\'B) `HELLO-WORLD`\\', \\'C) `HELLO-world`\\', \\'D) A `TypeError` will be raised because `separator` is not a string.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\\\\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\\\n\\\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\\\n\\\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.\\'}', id='3bc8499b-6aeb-47dc-b3bf-9872546027e6', tool_call_id='0b4310dd-58ed-4ed2-9bdf-21eff844f751'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef uppercase_args_decorator(func):\\n    def wrapper(*args, **kwargs):\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\n        return func(*processed_args, **processed_kwargs)\\n    return wrapper\\n\\n@uppercase_args_decorator\\ndef concatenate_strings(str1, str2, separator=\" \"):\\n    return f\"{str1}{separator}{str2}\"\\n\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\n```\\nWhat will be the output of this code?**\\n\\nA) `hello-world`\\nB) `HELLO-WORLD`\\nC) `HELLO-world`\\nD) A `TypeError` will be raised because `separator` is not a string.\\n\\nOptions:\\nA. A) `hello-world`\\nB. B) `HELLO-WORLD`\\nC. C) `HELLO-world`\\nD. D) A `TypeError` will be raised because `separator` is not a string.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='90319614-1fad-414a-89fd-b50f721a26fa'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='f2f075ea-03de-4b36-874e-8e47c953a828'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='3ea5f733-c90a-4179-a8c1-47e439ecc9d2'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', id='aa6ce6fd-98a8-4fde-836d-079b3948007e', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was B. Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', additional_kwargs={}, response_metadata={}, id='2e9a9499-3d20-42c9-8dd6-af1a6c4fde59'), AIMessage(content=\"\\n\\nThought: The user answered the MCQ and it was incorrect. I have provided the explanation for the correct answer. Now I should ask a follow-up question to check their understanding of the explanation or the concept. I can ask them to explain why their initial answer was incorrect, or to re-explain a specific part of the decorator's behavior. I will ask them to explain why the `separator` argument was not converted to uppercase.\\nBased on the explanation, can you tell me why the `separator` argument in the example code was not converted to uppercase, even though it was a string?\", additional_kwargs={}, response_metadata={}, id='579bf0ef-1d9d-4b56-bcc5-2214e847dac1'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b8365fb4-508e-4800-95b8-110a9b5fe43a'), AIMessage(content='That\\'s a great explanation of the general purpose of `*args` and `**kwargs` in decorators! You\\'re absolutely right that they make decorators much more flexible.\\n\\nHowever, let\\'s re-focus on the specific behavior of *this particular* `uppercase_args_decorator` and the `separator` argument.\\n\\nLook closely at this line within the `wrapper` function:\\n`processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}`\\n\\nWhen `separator=\"-\"` is passed, what is the value of `v` for the key `\"separator\"`? And what does `v.upper()` evaluate to in that specific case?', additional_kwargs={}, response_metadata={}, id='c8c9105b-85de-4d5d-a465-0eda19e2125b'), HumanMessage(content='The value of `v` for the key `\"separator\"` is `\"-\"`, which is a string. When `v.upper()` is called on `\"-\"`, it evaluates to `\"-\"` because the hyphen has no uppercase equivalent in Python. This is why the `separator` argument remains `\"-\"` and is not changed.', additional_kwargs={}, response_metadata={}, id='3f84ec95-30a5-4e8d-acdf-b4a3061ee8f0')]\n",
      "[ERROR] LLM invocation failed (attempt 1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "]\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 429 You exceeded your current '\n",
      "                  'quota, please check your plan and billing details. For more '\n",
      "                  'information on this error, head to: '\n",
      "                  'https://ai.google.dev/gemini-api/docs/rate-limits. '\n",
      "                  '[violations {\\n'\n",
      "                  '  quota_metric: '\n",
      "                  '\"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\\n'\n",
      "                  '  quota_id: '\n",
      "                  '\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"model\"\\n'\n",
      "                  '    value: \"gemini-2.5-flash\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"location\"\\n'\n",
      "                  '    value: \"global\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_value: 10\\n'\n",
      "                  '}\\n'\n",
      "                  ', links {\\n'\n",
      "                  '  description: \"Learn more about Gemini API quotas\"\\n'\n",
      "                  '  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n'\n",
      "                  '}\\n'\n",
      "                  ', retry_delay {\\n'\n",
      "                  '  seconds: 46\\n'\n",
      "                  '}\\n'\n",
      "                  ']',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='d19206c2-7eaf-41af-b92a-99e94a219d4b'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ba820959-b482-4d15-a54b-cf02589ef3f1', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '0b4310dd-58ed-4ed2-9bdf-21eff844f751', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef uppercase_args_decorator(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\\\n        return func(*processed_args, **processed_kwargs)\\\\n    return wrapper\\\\n\\\\n@uppercase_args_decorator\\\\ndef concatenate_strings(str1, str2, separator=\" \"):\\\\n    return f\"{str1}{separator}{str2}\"\\\\n\\\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\\\n```\\\\nWhat will be the output of this code?**\\\\n\\\\nA) `hello-world`\\\\nB) `HELLO-WORLD`\\\\nC) `HELLO-world`\\\\nD) A `TypeError` will be raised because `separator` is not a string.\\', \\'options\\': [\\'A) `hello-world`\\', \\'B) `HELLO-WORLD`\\', \\'C) `HELLO-world`\\', \\'D) A `TypeError` will be raised because `separator` is not a string.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\\\\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\\\n\\\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\\\n\\\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.\\'}', id='3bc8499b-6aeb-47dc-b3bf-9872546027e6', tool_call_id='0b4310dd-58ed-4ed2-9bdf-21eff844f751'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef uppercase_args_decorator(func):\\n    def wrapper(*args, **kwargs):\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\n        return func(*processed_args, **processed_kwargs)\\n    return wrapper\\n\\n@uppercase_args_decorator\\ndef concatenate_strings(str1, str2, separator=\" \"):\\n    return f\"{str1}{separator}{str2}\"\\n\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\n```\\nWhat will be the output of this code?**\\n\\nA) `hello-world`\\nB) `HELLO-WORLD`\\nC) `HELLO-world`\\nD) A `TypeError` will be raised because `separator` is not a string.\\n\\nOptions:\\nA. A) `hello-world`\\nB. B) `HELLO-WORLD`\\nC. C) `HELLO-world`\\nD. D) A `TypeError` will be raised because `separator` is not a string.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='90319614-1fad-414a-89fd-b50f721a26fa'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='f2f075ea-03de-4b36-874e-8e47c953a828'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='3ea5f733-c90a-4179-a8c1-47e439ecc9d2'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', id='aa6ce6fd-98a8-4fde-836d-079b3948007e', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was B. Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', additional_kwargs={}, response_metadata={}, id='2e9a9499-3d20-42c9-8dd6-af1a6c4fde59'),\n",
      "              AIMessage(content=\"\\n\\nThought: The user answered the MCQ and it was incorrect. I have provided the explanation for the correct answer. Now I should ask a follow-up question to check their understanding of the explanation or the concept. I can ask them to explain why their initial answer was incorrect, or to re-explain a specific part of the decorator's behavior. I will ask them to explain why the `separator` argument was not converted to uppercase.\\nBased on the explanation, can you tell me why the `separator` argument in the example code was not converted to uppercase, even though it was a string?\", additional_kwargs={}, response_metadata={}, id='579bf0ef-1d9d-4b56-bcc5-2214e847dac1'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b8365fb4-508e-4800-95b8-110a9b5fe43a'),\n",
      "              AIMessage(content='That\\'s a great explanation of the general purpose of `*args` and `**kwargs` in decorators! You\\'re absolutely right that they make decorators much more flexible.\\n\\nHowever, let\\'s re-focus on the specific behavior of *this particular* `uppercase_args_decorator` and the `separator` argument.\\n\\nLook closely at this line within the `wrapper` function:\\n`processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}`\\n\\nWhen `separator=\"-\"` is passed, what is the value of `v` for the key `\"separator\"`? And what does `v.upper()` evaluate to in that specific case?', additional_kwargs={}, response_metadata={}, id='c8c9105b-85de-4d5d-a465-0eda19e2125b'),\n",
      "              HumanMessage(content='The value of `v` for the key `\"separator\"` is `\"-\"`, which is a string. When `v.upper()` is called on `\"-\"`, it evaluates to `\"-\"` because the hyphen has no uppercase equivalent in Python. This is why the `separator` argument remains `\"-\"` and is not changed.', additional_kwargs={}, response_metadata={}, id='3f84ec95-30a5-4e8d-acdf-b4a3061ee8f0'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='ce21528f-677d-4419-9065-e2b6179dcb4d')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "import re # Import regex for answer parsing\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic tutoring agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: A list of BaseMessage objects representing the conversation history.\n",
    "                  Annotated with add_messages to automatically append new messages.\n",
    "        difficulty_level: The current difficulty level of the tutoring session (e.g., \"beginner\", \"intermediate\").\n",
    "        user_struggle_count: An integer tracking how many times the user has struggled or answered incorrectly.\n",
    "        topic: The main Python topic currently being discussed.\n",
    "        sub_topic: A more specific sub-topic within the main topic.\n",
    "        mcq_active: A boolean indicating if a Multiple Choice Question is currently active.\n",
    "        mcq_question: The full text of the active MCQ, including options.\n",
    "        mcq_options: A list of strings, each representing an option for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer (e.g., \"A\", \"B\", \"C\", \"D\") for the active MCQ.\n",
    "        mcq_explaination: The explaination for the mcq answer\n",
    "        agent_thought: The internal thought process of the Socratic LLM before generating a response.\n",
    "        next_node_decision: A string indicating the next node the router should transition to.\n",
    "                            Used by the supervisor/router to control graph flow.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    mcq_explanation: str \n",
    "    agent_thought: str\n",
    "    next_node_decision: str\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the main Socratic LLM for general conversation and tool binding.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", max_retries=3, temperature=0.7)\n",
    "\n",
    "# Initialize a separate LLM for generating MCQs. This allows for different\n",
    "# temperature or model settings specifically for MCQ generation.\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_retries=3, temperature=0.5)\n",
    "\n",
    "# System prompt for the Socratic LLM, guiding its behavior and principles.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1. **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2. **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3. **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4. **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5. **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides code and asks for feedback.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge.\n",
    "    * `mcq_agent`: Use this only for well-known topics like \"variables\", \"functions\", \"classes\", \"conditional statements\", \"comparisons\", or \"maximum of three numbers\".\n",
    "    * `llm_mcq_generator`: Use this for all other topics (e.g., \"Python decorators\", \"context managers\") or when a custom MCQ is needed.\n",
    "    * `mcq_answer_processor`: Use this when the user submits an answer to an active MCQ.\n",
    "6. **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7. **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8. **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "9. **MCQ Answer Format:** When presenting an MCQ, instruct the user to respond with a single letter (A, B, C, or D) to indicate their answer choice.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "MCQ Question (internal): {mcq_question}\n",
    "MCQ Options (internal): {mcq_options}\n",
    "MCQ Correct Answer (internal): {mcq_correct_answer}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Chat prompt template for the Socratic LLM, including system prompt and message history.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Tools ---\n",
    "# These tools simulate external functionalities that the Socratic LLM can call.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code.\n",
    "    This is a simulated tool. In a real application, it would run static analysis, linters, etc.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept.\n",
    "    This is a simulated tool. In a real application, it would provide detailed explanations.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge based on a topic and difficulty level.\n",
    "    This is a simulated tool. In a real application, it would generate a specific coding problem.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level\n",
    "    from a predefined list. The output will be a JSON string containing the question,\n",
    "    options, and correct answer. The 'question' field will be pre-formatted to include\n",
    "    options for direct display.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ\n",
    "    and a predefined question is available for the topic.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the exact topic exists in our predefined list (case-insensitive)\n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        # Format the question to include options for direct display in chat\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        # If topic not found, return a special string to indicate that the LLM should\n",
    "        # consider using the `llm_mcq_generator` tool instead.\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an MCQ using an LLM based on a topic and difficulty level.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python tutor who generates multiple choice questions (MCQs) for practice.\n",
    "Generate an MCQ on the topic \"{topic}\" at a \"{difficulty}\" level.\n",
    "\n",
    "The MCQ must follow this format strictly as a JSON object:\n",
    "{{\n",
    "    \"question\": \"string\",\n",
    "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "    \"answer_index\": 1,\n",
    "    \"explanation\": \"string\"\n",
    "}}\n",
    "\n",
    "For questions involving code snippets, format the code within triple backticks (```) to preserve readability, and ensure all strings are JSON-compatible (newlines escaped as \\\\n).\n",
    "DO NOT include outer markdown code fences like ```json or ```python\n",
    "Respond with raw valid JSON only. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    raw_llm_content = llm_response.content.strip()\n",
    "    print(\"[DEBUG] Raw LLM MCQ Response:\", repr(raw_llm_content))\n",
    "\n",
    "    # Strip outer markdown fences\n",
    "    cleaned_content = re.sub(r'^```(json|python)?\\n?', '', raw_llm_content, flags=re.MULTILINE)\n",
    "    cleaned_content = re.sub(r'\\n?```$', '', cleaned_content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace problematic characters, but preserve code formatting\n",
    "    cleaned_content = (\n",
    "        cleaned_content\n",
    "        .replace('“', '\"').replace('”', '\"')  # Replace smart quotes\n",
    "        .replace('‘', \"'\").replace('’', \"'\")  # Replace smart single quotes\n",
    "        .replace('\\u201c', '\"').replace('\\u201d', '\"')  # Replace Unicode quotes\n",
    "        .replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Replace Unicode single quotes\n",
    "        .replace('\\t', '    ')  # Replace tabs with spaces for code readability\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Cleaned LLM MCQ Response:\", repr(cleaned_content))\n",
    "\n",
    "    try:\n",
    "        mcq_data = json.loads(cleaned_content)\n",
    "        # Validate JSON structure\n",
    "        required_keys = {\"question\", \"options\", \"answer_index\", \"explanation\"}\n",
    "        if not all(key in mcq_data for key in required_keys):\n",
    "            raise ValueError(\"Invalid MCQ format: Missing required keys\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"Invalid MCQ format: Options must be a list of 4 strings\")\n",
    "        if not isinstance(mcq_data[\"answer_index\"], int) or mcq_data[\"answer_index\"] not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid MCQ format: answer_index must be an integer between 0 and 3\")\n",
    "        \n",
    "        # Format question for display with options\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data[\"options\"])\n",
    "        mcq_data[\"question\"] = formatted_question\n",
    "        return mcq_data\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(\"[ERROR] JSON parsing or validation failed:\", str(e))\n",
    "        print(\"[ERROR] Cleaned content:\", repr(cleaned_content))\n",
    "        default_mcq = {\n",
    "            \"question\": \"**What is a Python decorator?**\\n\\n\" + \n",
    "                        \"\\n\".join([\n",
    "                            \"A) A function that modifies another function or method\",\n",
    "                            \"B) A type of class inheritance\",\n",
    "                            \"C) A syntax for defining variables\",\n",
    "                            \"D) A loop construct\"\n",
    "                        ]),\n",
    "            \"options\": [\n",
    "                \"A) A function that modifies another function or method\",\n",
    "                \"B) A type of class inheritance\",\n",
    "                \"C) A syntax for defining variables\",\n",
    "                \"D) A loop construct\"\n",
    "            ],\n",
    "            \"answer_index\": 0,\n",
    "            \"explanation\": \"A Python decorator is a function that wraps another function or method to extend or modify its behavior.\"\n",
    "        }\n",
    "        return {\"error\": f\"Failed to parse or validate MCQ JSON: {str(e)}\", **default_mcq}\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str, explanation: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ.\n",
    "    Compares the user's answer with the correct answer and returns feedback with explanation.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return f\"Correct!\\n\\nExplanation: {explanation}\"\n",
    "    else:\n",
    "        return f\"Incorrect.\\n\\nExplanation: {explanation}\"\n",
    "\n",
    "\n",
    "# List of all tools available to the Socratic LLM\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent, llm_mcq_generator, mcq_answer_processor]\n",
    "# Bind the tools to the main Socratic LLM, allowing it to call them.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "# Combine the prompt and the LLM with tools into a runnable for the Socratic agent.\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Invokes the Socratic LLM with the current conversation history and state.\n",
    "    This node represents the core logic of the Socratic tutoring agent.\n",
    "    It extracts the LLM's \"thought\" and formats the content for display.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_llm node.\")\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    \n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = socratic_agent_runnable.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                **{k: v for k, v in state.items() if k not in ['messages', 'next_node_decision']}\n",
    "            })\n",
    "            print(\"[DEBUG] LLM Response:\", response)\n",
    "            \n",
    "            content = response.content\n",
    "            if isinstance(content, list):\n",
    "                content = \"\\n\".join(str(item) for item in content)\n",
    "            elif not isinstance(content, str):\n",
    "                print(\"[ERROR] Unexpected content type:\", type(content))\n",
    "                content = \"\"\n",
    "\n",
    "            thought = \"\"\n",
    "            display_content = \"\"\n",
    "            if content and content.startswith(\"Thought:\"):\n",
    "                parts = content.split(\"Thought:\", 1)\n",
    "                thought_and_content = parts[1].strip()\n",
    "                thought_lines = thought_and_content.split('\\n', 1)\n",
    "                thought = thought_lines[0].strip()\n",
    "                display_content = thought_lines[1].strip() if len(thought_lines) > 1 else \"\"\n",
    "            else:\n",
    "                display_content = content\n",
    "\n",
    "            new_ai_message = AIMessage(\n",
    "                content=display_content,\n",
    "                tool_calls=response.tool_calls\n",
    "            )\n",
    "\n",
    "            return {\"messages\": [new_ai_message], \"agent_thought\": thought}\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] LLM invocation failed (attempt {attempt + 1}/{max_attempts}):\", str(e))\n",
    "            if attempt == max_attempts - 1:\n",
    "                return {\n",
    "                    \"messages\": [AIMessage(content=\"An issue occurred, but you can still answer the MCQ with A, B, C, or D.\")],\n",
    "                    \"agent_thought\": f\"Error during LLM invocation: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2.0)\n",
    "\n",
    "# A dictionary mapping tool names to their corresponding Python functions.\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "    \"llm_mcq_generator\": llm_mcq_generator,\n",
    "    \"mcq_answer_processor\": mcq_answer_processor,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering call_tool node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    messages_to_add = []\n",
    "    state_updates = {}\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            \n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_content = str(response)\n",
    "\n",
    "                existing_tool_message = any(\n",
    "                    isinstance(msg, ToolMessage) and msg.tool_call_id == tool_call[\"id\"]\n",
    "                    for msg in state[\"messages\"]\n",
    "                )\n",
    "                \n",
    "                if not existing_tool_message:\n",
    "                    messages_to_add.append(\n",
    "                        ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "                    )\n",
    "\n",
    "                if tool_name in [\"mcq_agent\", \"llm_mcq_generator\"]:\n",
    "                    if tool_output_content == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"No predefined MCQ found for this topic. Generating a new one...\")\n",
    "                        )\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content) if isinstance(response, str) else response\n",
    "                        if \"error\" in mcq_data:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                        else:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"Error: MCQ agent returned invalid JSON. Please try again.\")\n",
    "                        )\n",
    "                        continue\n",
    "                elif tool_name == \"mcq_answer_processor\":\n",
    "                    if \"Correct!\" in tool_output_content:\n",
    "                        state_updates[\"user_struggle_count\"] = 0\n",
    "                    else:\n",
    "                        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "                    state_updates[\"mcq_active\"] = False\n",
    "                    state_updates[\"mcq_question\"] = \"\"\n",
    "                    state_updates[\"mcq_options\"] = []\n",
    "                    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "                    state_updates[\"mcq_explanation\"] = \"\"\n",
    "            else:\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "    print(f\"[DEBUG] call_tool: State updates - {state_updates}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "def router(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    The supervisor node that decides the next action based on the current state and user input.\n",
    "    It primarily routes between processing an MCQ answer directly or letting the Socratic LLM respond.\n",
    "    \n",
    "    Returns a dictionary containing 'next_node_decision' to control graph flow.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering router node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    next_decision = \"\"\n",
    "    if state.get(\"mcq_active\", False) and isinstance(last_message, HumanMessage):\n",
    "        user_input = last_message.content.strip().upper()\n",
    "        if re.match(r\"^[ABCD](\\.|\\))?$\", user_input):\n",
    "            print(\"[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\")\n",
    "            next_decision = \"process_mcq_answer\"\n",
    "        else:\n",
    "            print(\"[DEBUG] Router: MCQ active but invalid answer, prompting for valid input.\")\n",
    "            return {\n",
    "                \"next_node_decision\": \"call_llm\",\n",
    "                \"messages\": [AIMessage(content=\"Please respond with a single letter (A, B, C, or D) to select your answer.\")]\n",
    "            }\n",
    "    else:\n",
    "        print(\"[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\")\n",
    "        next_decision = \"call_llm\"\n",
    "    \n",
    "    return {\"next_node_decision\": next_decision}\n",
    "\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering process_mcq_answer node.\")\n",
    "    last_human_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        print(\"[ERROR] process_mcq_answer: Could not find a HumanMessage to process.\")\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    user_answer = last_human_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    explanation = state.get(\"mcq_explanation\", \"No explanation available.\")\n",
    "    \n",
    "    tool_output_content = mcq_answer_processor.invoke({\n",
    "        \"user_answer\": user_answer,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "    \n",
    "    state_updates = {}\n",
    "    messages_to_add = []\n",
    "\n",
    "    if \"Correct!\" in tool_output_content:\n",
    "        state_updates[\"user_struggle_count\"] = 0\n",
    "    else:\n",
    "        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    state_updates[\"mcq_active\"] = False\n",
    "    state_updates[\"mcq_question\"] = \"\"\n",
    "    state_updates[\"mcq_options\"] = []\n",
    "    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "    state_updates[\"mcq_explanation\"] = \"\"\n",
    "\n",
    "    messages_to_add.append(\n",
    "        ToolMessage(content=tool_output_content, tool_call_id=\"mcq_processor_direct_call\")\n",
    "    )\n",
    "    messages_to_add.append(\n",
    "        AIMessage(content=f\"You answered: {user_answer}. The correct answer was {correct_answer}. {tool_output_content}\")\n",
    "    )\n",
    "\n",
    "    print(f\"[DEBUG] process_mcq_answer: Result - {tool_output_content}, New struggle count: {state_updates.get('user_struggle_count')}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue_socratic(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering should_continue_socratic edge logic.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        print(\"[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\")\n",
    "        return \"call_tool\"\n",
    "    elif state.get(\"mcq_active\", False) and isinstance(last_message, AIMessage) and \"Please select an option (A, B, C, or D)\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\")\n",
    "        return \"END\"\n",
    "    elif isinstance(last_message, AIMessage) and \"An issue occurred\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\")\n",
    "        return \"END\"\n",
    "    else:\n",
    "        print(\"[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Initialize the StateGraph with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add all the nodes to the workflow.\n",
    "workflow.add_node(\"router\", router) # The new supervisor node.\n",
    "workflow.add_node(\"call_llm\", call_llm) # The Socratic agent's LLM logic.\n",
    "workflow.add_node(\"call_tool\", call_tool) # The tool execution logic.\n",
    "workflow.add_node(\"process_mcq_answer\", process_mcq_answer) # Node for direct MCQ answer processing.\n",
    "\n",
    "# Set the `router` node as the starting point of the graph.\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Define conditional edges from the `router` node.\n",
    "# The `router` function itself determines the next node based on the state.\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_node_decision\"], # Use the decision stored in state by the router function.\n",
    "    {\n",
    "        \"call_llm\": \"call_llm\",         # If router decides, go to the Socratic LLM.\n",
    "        \"process_mcq_answer\": \"process_mcq_answer\" # If router detects MCQ answer, go to process it.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define conditional edges from the `call_llm` node (Socratic agent's LLM).\n",
    "# `should_continue_socratic` determines if a tool needs to be called or if the turn ends.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue_socratic,\n",
    "    {\"call_tool\": \"call_tool\", \"END\": END, \"call_llm\": \"call_llm\"} # If tool calls, go to `call_tool`; otherwise, end.\n",
    ")\n",
    "\n",
    "# Define a regular edge from `call_tool` back to `call_llm`.\n",
    "# After a tool is executed, the Socratic LLM needs to process the tool's output and generate a response.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Define a regular edge from `process_mcq_answer` back to `call_llm`.\n",
    "# After an MCQ answer is processed, the Socratic LLM needs to provide feedback and potentially a new question.\n",
    "workflow.add_edge(\"process_mcq_answer\", \"call_llm\")\n",
    "\n",
    "# Compile the workflow into a runnable graph.\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"mcq_explanation\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\"\n",
    "}\n",
    "result_llm_mcq = socratic_graph.invoke(llm_mcq_state)\n",
    "import pprint\n",
    "pprint.pprint(result_llm_mcq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465fe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade99b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7132ac307b30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Memory to the LangGraph\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "import re # Import regex for answer parsing\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic tutoring agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: A list of BaseMessage objects representing the conversation history.\n",
    "                  Annotated with add_messages to automatically append new messages.\n",
    "        difficulty_level: The current difficulty level of the tutoring session (e.g., \"beginner\", \"intermediate\").\n",
    "        user_struggle_count: An integer tracking how many times the user has struggled or answered incorrectly.\n",
    "        topic: The main Python topic currently being discussed.\n",
    "        sub_topic: A more specific sub-topic within the main topic.\n",
    "        mcq_active: A boolean indicating if a Multiple Choice Question is currently active.\n",
    "        mcq_question: The full text of the active MCQ, including options.\n",
    "        mcq_options: A list of strings, each representing an option for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer (e.g., \"A\", \"B\", \"C\", \"D\") for the active MCQ.\n",
    "        mcq_explaination: The explaination for the mcq answer\n",
    "        agent_thought: The internal thought process of the Socratic LLM before generating a response.\n",
    "        next_node_decision: A string indicating the next node the router should transition to.\n",
    "                            Used by the supervisor/router to control graph flow.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    mcq_explanation: str \n",
    "    agent_thought: str\n",
    "    next_node_decision: str\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the main Socratic LLM for general conversation and tool binding.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", max_retries=3, temperature=0.7)\n",
    "\n",
    "# Initialize a separate LLM for generating MCQs. This allows for different\n",
    "# temperature or model settings specifically for MCQ generation.\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_retries=3, temperature=0.5)\n",
    "\n",
    "# System prompt for the Socratic LLM, guiding its behavior and principles.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1. **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2. **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3. **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4. **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5. **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides code and asks for feedback.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge.\n",
    "    * `mcq_agent`: Use this only for well-known topics like \"variables\", \"functions\", \"classes\", \"conditional statements\", \"comparisons\", or \"maximum of three numbers\".\n",
    "    * `llm_mcq_generator`: Use this for all other topics (e.g., \"Python decorators\", \"context managers\") or when a custom MCQ is needed.\n",
    "    * `mcq_answer_processor`: Use this when the user submits an answer to an active MCQ.\n",
    "6. **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7. **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8. **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "9. **MCQ Answer Format:** When presenting an MCQ, instruct the user to respond with a single letter (A, B, C, or D) to indicate their answer choice.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "MCQ Question (internal): {mcq_question}\n",
    "MCQ Options (internal): {mcq_options}\n",
    "MCQ Correct Answer (internal): {mcq_correct_answer}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Chat prompt template for the Socratic LLM, including system prompt and message history.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Tools ---\n",
    "# These tools simulate external functionalities that the Socratic LLM can call.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code.\n",
    "    This is a simulated tool. In a real application, it would run static analysis, linters, etc.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept.\n",
    "    This is a simulated tool. In a real application, it would provide detailed explanations.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge based on a topic and difficulty level.\n",
    "    This is a simulated tool. In a real application, it would generate a specific coding problem.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level\n",
    "    from a predefined list. The output will be a JSON string containing the question,\n",
    "    options, and correct answer. The 'question' field will be pre-formatted to include\n",
    "    options for direct display.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ\n",
    "    and a predefined question is available for the topic.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the exact topic exists in our predefined list (case-insensitive)\n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        # Format the question to include options for direct display in chat\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        # If topic not found, return a special string to indicate that the LLM should\n",
    "        # consider using the `llm_mcq_generator` tool instead.\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an MCQ using an LLM based on a topic and difficulty level.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python tutor who generates multiple choice questions (MCQs) for practice.\n",
    "Generate an MCQ on the topic \"{topic}\" at a \"{difficulty}\" level.\n",
    "\n",
    "The MCQ must follow this format strictly as a JSON object:\n",
    "{{\n",
    "    \"question\": \"string\",\n",
    "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "    \"answer_index\": 1,\n",
    "    \"explanation\": \"string\"\n",
    "}}\n",
    "\n",
    "For questions involving code snippets, format the code within triple backticks (```) to preserve readability, and ensure all strings are JSON-compatible (newlines escaped as \\\\n).\n",
    "DO NOT include outer markdown code fences like ```json or ```python\n",
    "Respond with raw valid JSON only. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    raw_llm_content = llm_response.content.strip()\n",
    "    print(\"[DEBUG] Raw LLM MCQ Response:\", repr(raw_llm_content))\n",
    "\n",
    "    # Strip outer markdown fences\n",
    "    cleaned_content = re.sub(r'^```(json|python)?\\n?', '', raw_llm_content, flags=re.MULTILINE)\n",
    "    cleaned_content = re.sub(r'\\n?```$', '', cleaned_content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace problematic characters, but preserve code formatting\n",
    "    cleaned_content = (\n",
    "        cleaned_content\n",
    "        .replace('“', '\"').replace('”', '\"')  # Replace smart quotes\n",
    "        .replace('‘', \"'\").replace('’', \"'\")  # Replace smart single quotes\n",
    "        .replace('\\u201c', '\"').replace('\\u201d', '\"')  # Replace Unicode quotes\n",
    "        .replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Replace Unicode single quotes\n",
    "        .replace('\\t', '    ')  # Replace tabs with spaces for code readability\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Cleaned LLM MCQ Response:\", repr(cleaned_content))\n",
    "\n",
    "    try:\n",
    "        mcq_data = json.loads(cleaned_content)\n",
    "        # Validate JSON structure\n",
    "        required_keys = {\"question\", \"options\", \"answer_index\", \"explanation\"}\n",
    "        if not all(key in mcq_data for key in required_keys):\n",
    "            raise ValueError(\"Invalid MCQ format: Missing required keys\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"Invalid MCQ format: Options must be a list of 4 strings\")\n",
    "        if not isinstance(mcq_data[\"answer_index\"], int) or mcq_data[\"answer_index\"] not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid MCQ format: answer_index must be an integer between 0 and 3\")\n",
    "        \n",
    "        # Format question for display with options\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data[\"options\"])\n",
    "        mcq_data[\"question\"] = formatted_question\n",
    "        return mcq_data\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(\"[ERROR] JSON parsing or validation failed:\", str(e))\n",
    "        print(\"[ERROR] Cleaned content:\", repr(cleaned_content))\n",
    "        default_mcq = {\n",
    "            \"question\": \"**What is a Python decorator?**\\n\\n\" + \n",
    "                        \"\\n\".join([\n",
    "                            \"A) A function that modifies another function or method\",\n",
    "                            \"B) A type of class inheritance\",\n",
    "                            \"C) A syntax for defining variables\",\n",
    "                            \"D) A loop construct\"\n",
    "                        ]),\n",
    "            \"options\": [\n",
    "                \"A) A function that modifies another function or method\",\n",
    "                \"B) A type of class inheritance\",\n",
    "                \"C) A syntax for defining variables\",\n",
    "                \"D) A loop construct\"\n",
    "            ],\n",
    "            \"answer_index\": 0,\n",
    "            \"explanation\": \"A Python decorator is a function that wraps another function or method to extend or modify its behavior.\"\n",
    "        }\n",
    "        return {\"error\": f\"Failed to parse or validate MCQ JSON: {str(e)}\", **default_mcq}\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str, explanation: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ.\n",
    "    Compares the user's answer with the correct answer and returns feedback with explanation.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return f\"Correct!\\n\\nExplanation: {explanation}\"\n",
    "    else:\n",
    "        return f\"Incorrect.\\n\\nExplanation: {explanation}\"\n",
    "\n",
    "\n",
    "# List of all tools available to the Socratic LLM\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent, llm_mcq_generator, mcq_answer_processor]\n",
    "# Bind the tools to the main Socratic LLM, allowing it to call them.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "# Combine the prompt and the LLM with tools into a runnable for the Socratic agent.\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Invokes the Socratic LLM with the current conversation history and state.\n",
    "    This node represents the core logic of the Socratic tutoring agent.\n",
    "    It extracts the LLM's \"thought\" and formats the content for display.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_llm node.\")\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    \n",
    "    try:\n",
    "        response = socratic_agent_runnable.invoke({\n",
    "            \"messages\": state[\"messages\"],\n",
    "            **{k: v for k, v in state.items() if k not in ['messages', 'next_node_decision']}\n",
    "        })\n",
    "        print(\"[DEBUG] LLM Response:\", response)\n",
    "        \n",
    "        content = response.content\n",
    "        if isinstance(content, list):\n",
    "            content = \"\\n\".join(str(item) for item in content)\n",
    "        elif not isinstance(content, str):\n",
    "            print(\"[ERROR] Unexpected content type:\", type(content))\n",
    "            content = \"\"\n",
    "\n",
    "        thought = \"\"\n",
    "        display_content = \"\"\n",
    "        if content and content.startswith(\"Thought:\"):\n",
    "            parts = content.split(\"Thought:\", 1)\n",
    "            thought_and_content = parts[1].strip()\n",
    "            thought_lines = thought_and_content.split('\\n', 1)\n",
    "            thought = thought_lines[0].strip()\n",
    "            display_content = thought_lines[1].strip() if len(thought_lines) > 1 else \"\"\n",
    "        else:\n",
    "            display_content = content\n",
    "\n",
    "        new_ai_message = AIMessage(\n",
    "            content=display_content,\n",
    "            tool_calls=response.tool_calls\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [new_ai_message], \"agent_thought\": thought}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] LLM invocation failed\", str(e))\n",
    "\n",
    "# A dictionary mapping tool names to their corresponding Python functions.\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "    \"llm_mcq_generator\": llm_mcq_generator,\n",
    "    \"mcq_answer_processor\": mcq_answer_processor,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering call_tool node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    messages_to_add = []\n",
    "    state_updates = {}\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            \n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_content = str(response)\n",
    "\n",
    "                existing_tool_message = any(\n",
    "                    isinstance(msg, ToolMessage) and msg.tool_call_id == tool_call[\"id\"]\n",
    "                    for msg in state[\"messages\"]\n",
    "                )\n",
    "                \n",
    "                if not existing_tool_message:\n",
    "                    messages_to_add.append(\n",
    "                        ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "                    )\n",
    "\n",
    "                if tool_name in [\"mcq_agent\", \"llm_mcq_generator\"]:\n",
    "                    if tool_output_content == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"No predefined MCQ found for this topic. Generating a new one...\")\n",
    "                        )\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content) if isinstance(response, str) else response\n",
    "                        if \"error\" in mcq_data:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                        else:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"Error: MCQ agent returned invalid JSON. Please try again.\")\n",
    "                        )\n",
    "                        continue\n",
    "                elif tool_name == \"mcq_answer_processor\":\n",
    "                    if \"Correct!\" in tool_output_content:\n",
    "                        state_updates[\"user_struggle_count\"] = 0\n",
    "                    else:\n",
    "                        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "                    state_updates[\"mcq_active\"] = False\n",
    "                    state_updates[\"mcq_question\"] = \"\"\n",
    "                    state_updates[\"mcq_options\"] = []\n",
    "                    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "                    state_updates[\"mcq_explanation\"] = \"\"\n",
    "            else:\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "    print(f\"[DEBUG] call_tool: State updates - {state_updates}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "def router(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    The supervisor node that decides the next action based on the current state and user input.\n",
    "    It primarily routes between processing an MCQ answer directly or letting the Socratic LLM respond.\n",
    "    \n",
    "    Returns a dictionary containing 'next_node_decision' to control graph flow.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering router node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    next_decision = \"\"\n",
    "    if state.get(\"mcq_active\", False) and isinstance(last_message, HumanMessage):\n",
    "        user_input = last_message.content.strip().upper()\n",
    "        if re.match(r\"^[ABCD](\\.|\\))?$\", user_input):\n",
    "            print(\"[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\")\n",
    "            next_decision = \"process_mcq_answer\"\n",
    "        else:\n",
    "            print(\"[DEBUG] Router: MCQ active but invalid answer, prompting for valid input.\")\n",
    "            return {\n",
    "                \"next_node_decision\": \"call_llm\",\n",
    "                \"messages\": [AIMessage(content=\"Please respond with a single letter (A, B, C, or D) to select your answer.\")]\n",
    "            }\n",
    "    else:\n",
    "        print(\"[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\")\n",
    "        next_decision = \"call_llm\"\n",
    "    \n",
    "    return {\"next_node_decision\": next_decision}\n",
    "\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering process_mcq_answer node.\")\n",
    "    last_human_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        print(\"[ERROR] process_mcq_answer: Could not find a HumanMessage to process.\")\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    user_answer = last_human_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    explanation = state.get(\"mcq_explanation\", \"No explanation available.\")\n",
    "    \n",
    "    tool_output_content = mcq_answer_processor.invoke({\n",
    "        \"user_answer\": user_answer,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "    \n",
    "    state_updates = {}\n",
    "    messages_to_add = []\n",
    "\n",
    "    if \"Correct!\" in tool_output_content:\n",
    "        state_updates[\"user_struggle_count\"] = 0\n",
    "    else:\n",
    "        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    state_updates[\"mcq_active\"] = False\n",
    "    state_updates[\"mcq_question\"] = \"\"\n",
    "    state_updates[\"mcq_options\"] = []\n",
    "    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "    state_updates[\"mcq_explanation\"] = \"\"\n",
    "\n",
    "    messages_to_add.append(\n",
    "        ToolMessage(content=tool_output_content, tool_call_id=\"mcq_processor_direct_call\")\n",
    "    )\n",
    "    messages_to_add.append(\n",
    "        AIMessage(content=f\"You answered: {user_answer}. The correct answer was {correct_answer}. {tool_output_content}\")\n",
    "    )\n",
    "\n",
    "    print(f\"[DEBUG] process_mcq_answer: Result - {tool_output_content}, New struggle count: {state_updates.get('user_struggle_count')}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue_socratic(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering should_continue_socratic edge logic.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        print(\"[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\")\n",
    "        return \"call_tool\"\n",
    "    elif state.get(\"mcq_active\", False) and isinstance(last_message, AIMessage) and \"Please select an option (A, B, C, or D)\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\")\n",
    "        return \"END\"\n",
    "    elif isinstance(last_message, AIMessage) and \"An issue occurred\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\")\n",
    "        return \"END\"\n",
    "    else:\n",
    "        print(\"[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Initialize the StateGraph with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add all the nodes to the workflow.\n",
    "workflow.add_node(\"router\", router) # The new supervisor node.\n",
    "workflow.add_node(\"call_llm\", call_llm) # The Socratic agent's LLM logic.\n",
    "workflow.add_node(\"call_tool\", call_tool) # The tool execution logic.\n",
    "workflow.add_node(\"process_mcq_answer\", process_mcq_answer) # Node for direct MCQ answer processing.\n",
    "\n",
    "# Set the `router` node as the starting point of the graph.\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Define conditional edges from the `router` node.\n",
    "# The `router` function itself determines the next node based on the state.\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_node_decision\"], # Use the decision stored in state by the router function.\n",
    "    {\n",
    "        \"call_llm\": \"call_llm\",         # If router decides, go to the Socratic LLM.\n",
    "        \"process_mcq_answer\": \"process_mcq_answer\" # If router detects MCQ answer, go to process it.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define conditional edges from the `call_llm` node (Socratic agent's LLM).\n",
    "# `should_continue_socratic` determines if a tool needs to be called or if the turn ends.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue_socratic,\n",
    "    {\"call_tool\": \"call_tool\", \"END\": END, \"call_llm\": \"call_llm\"} # If tool calls, go to `call_tool`; otherwise, end.\n",
    ")\n",
    "\n",
    "# Define a regular edge from `call_tool` back to `call_llm`.\n",
    "# After a tool is executed, the Socratic LLM needs to process the tool's output and generate a response.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Define a regular edge from `process_mcq_answer` back to `call_llm`.\n",
    "# After an MCQ answer is processed, the Socratic LLM needs to provide feedback and potentially a new question.\n",
    "workflow.add_edge(\"process_mcq_answer\", \"call_llm\")\n",
    "\n",
    "# Compile the workflow into a runnable graph.\n",
    "socratic_graph = workflow.compile(checkpointer=memory_saver)\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "thread_id = \"user_session_123\"\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"mcq_explanation\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\"\n",
    "}\n",
    "\n",
    "# Invoke the graph with the thread_id to persist state\n",
    "result = socratic_graph.invoke(llm_mcq_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f378f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='75c0607f-9423-49c1-88ff-e7b48cac15e3')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--07d1f8b7-d6c4-4cfd-b268-cbbe4c534136-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '2fe15ace-05f1-49bc-8728-5957249c2576', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1139, 'output_tokens': 25, 'total_tokens': 1234, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 70}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] Raw LLM MCQ Response: '```json\\n{\\n    \"question\": \"What is the primary purpose of using `functools.wraps` when creating decorators in Python?\",\\n    \"options\": [\\n        \"To enable the decorator to accept arguments.\",\\n        \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\",\\n        \"To automatically apply `try-except` blocks around the decorated function for error handling.\",\\n        \"To convert a function-based decorator into a class-based decorator.\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\"\\n}\\n```'\n",
      "[DEBUG] Cleaned LLM MCQ Response: '{\\n    \"question\": \"What is the primary purpose of using `functools.wraps` when creating decorators in Python?\",\\n    \"options\": [\\n        \"To enable the decorator to accept arguments.\",\\n        \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\",\\n        \"To automatically apply `try-except` blocks around the decorated function for error handling.\",\\n        \"To convert a function-based decorator into a class-based decorator.\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\"\\n}\\n'\n",
      "[DEBUG] call_tool: State updates - {'mcq_active': True, 'mcq_question': \"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\n\\nTo enable the decorator to accept arguments.\\nTo preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\nTo convert a function-based decorator into a class-based decorator.\", 'mcq_options': ['To enable the decorator to accept arguments.', \"To preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\", 'To automatically apply `try-except` blocks around the decorated function for error handling.', 'To convert a function-based decorator into a class-based decorator.'], 'mcq_correct_answer': 'B', 'mcq_explanation': 'When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.', 'topic': 'Python decorators'}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='75c0607f-9423-49c1-88ff-e7b48cac15e3'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='eb5f266f-0bff-4b3f-bacf-53bb048eadf9', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '2fe15ace-05f1-49bc-8728-5957249c2576', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\\\n\\\\nTo enable the decorator to accept arguments.\\\\nTo preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\\\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\\\nTo convert a function-based decorator into a class-based decorator.\", \\'options\\': [\\'To enable the decorator to accept arguments.\\', \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\", \\'To automatically apply `try-except` blocks around the decorated function for error handling.\\', \\'To convert a function-based decorator into a class-based decorator.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\\'}', id='98f679ba-44fb-4f3e-bd7b-bcb203759739', tool_call_id='2fe15ace-05f1-49bc-8728-5957249c2576'), AIMessage(content=\"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\n\\nTo enable the decorator to accept arguments.\\nTo preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\nTo convert a function-based decorator into a class-based decorator.\\n\\nOptions:\\nA. To enable the decorator to accept arguments.\\nB. To preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nC. To automatically apply `try-except` blocks around the decorated function for error handling.\\nD. To convert a function-based decorator into a class-based decorator.\\n\\nPlease select an option (A, B, C, or D).\", additional_kwargs={}, response_metadata={}, id='6dd098b0-d70a-44dd-b0ca-decdbbdd558a')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\n",
      "{'agent_thought': '',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': True,\n",
      " 'mcq_correct_answer': 'B',\n",
      " 'mcq_explanation': 'When you use a decorator without `functools.wraps`, the '\n",
      "                    'decorated function is replaced by the wrapper function '\n",
      "                    'returned by the decorator. This means that introspection '\n",
      "                    'tools (like `help()` or accessing attributes like '\n",
      "                    '`__name__` or `__doc__`) would report details of the '\n",
      "                    'wrapper function, not the original function. '\n",
      "                    '`functools.wraps` is a decorator itself that copies the '\n",
      "                    'name, docstring, module, and other attributes from the '\n",
      "                    'original function to the wrapper function, making the '\n",
      "                    'decorated function behave more like the original for '\n",
      "                    'debugging, documentation, and other introspection '\n",
      "                    'purposes. It does not relate to accepting decorator '\n",
      "                    'arguments, error handling, or converting decorator types.',\n",
      " 'mcq_options': ['To enable the decorator to accept arguments.',\n",
      "                 \"To preserve the original function's metadata (like \"\n",
      "                 '`__name__`, `__doc__`, `__module__`).',\n",
      "                 'To automatically apply `try-except` blocks around the '\n",
      "                 'decorated function for error handling.',\n",
      "                 'To convert a function-based decorator into a class-based '\n",
      "                 'decorator.'],\n",
      " 'mcq_question': '**What is the primary purpose of using `functools.wraps` '\n",
      "                 'when creating decorators in Python?**\\n'\n",
      "                 '\\n'\n",
      "                 'To enable the decorator to accept arguments.\\n'\n",
      "                 \"To preserve the original function's metadata (like \"\n",
      "                 '`__name__`, `__doc__`, `__module__`).\\n'\n",
      "                 'To automatically apply `try-except` blocks around the '\n",
      "                 'decorated function for error handling.\\n'\n",
      "                 'To convert a function-based decorator into a class-based '\n",
      "                 'decorator.',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='75c0607f-9423-49c1-88ff-e7b48cac15e3'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='eb5f266f-0bff-4b3f-bacf-53bb048eadf9', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '2fe15ace-05f1-49bc-8728-5957249c2576', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\\\n\\\\nTo enable the decorator to accept arguments.\\\\nTo preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\\\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\\\nTo convert a function-based decorator into a class-based decorator.\", \\'options\\': [\\'To enable the decorator to accept arguments.\\', \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\", \\'To automatically apply `try-except` blocks around the decorated function for error handling.\\', \\'To convert a function-based decorator into a class-based decorator.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\\'}', id='98f679ba-44fb-4f3e-bd7b-bcb203759739', tool_call_id='2fe15ace-05f1-49bc-8728-5957249c2576'),\n",
      "              AIMessage(content=\"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\n\\nTo enable the decorator to accept arguments.\\nTo preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\nTo convert a function-based decorator into a class-based decorator.\\n\\nOptions:\\nA. To enable the decorator to accept arguments.\\nB. To preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nC. To automatically apply `try-except` blocks around the decorated function for error handling.\\nD. To convert a function-based decorator into a class-based decorator.\\n\\nPlease select an option (A, B, C, or D).\", additional_kwargs={}, response_metadata={}, id='6dd098b0-d70a-44dd-b0ca-decdbbdd558a')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "thread_id = \"user_session_123\"\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"mcq_explanation\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\"\n",
    "}\n",
    "\n",
    "# Invoke the graph with the thread_id to persist state\n",
    "result = socratic_graph.invoke(llm_mcq_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "import pprint\n",
    "pprint.pprint(result)\n",
    "\n",
    "# # Simulate a follow-up interaction to demonstrate memory\n",
    "# follow_up_state = {\n",
    "#     \"messages\": [HumanMessage(content=\"B\")],  # User responds to the MCQ\n",
    "#     \"difficulty_level\": result[\"difficulty_level\"],\n",
    "#     \"user_struggle_count\": result[\"user_struggle_count\"],\n",
    "#     \"topic\": result[\"topic\"],\n",
    "#     \"sub_topic\": result[\"sub_topic\"],\n",
    "#     \"mcq_active\": result[\"mcq_active\"],\n",
    "#     \"mcq_question\": result[\"mcq_question\"],\n",
    "#     \"mcq_options\": result[\"mcq_options\"],\n",
    "#     \"mcq_correct_answer\": result[\"mcq_correct_answer\"],\n",
    "#     \"mcq_explanation\": result[\"mcq_explanation\"],\n",
    "#     \"agent_thought\": result[\"agent_thought\"],\n",
    "#     \"next_node_decision\": \"\"\n",
    "# }\n",
    "\n",
    "# # Invoke the graph again with the same thread_id to retrieve the previous state\n",
    "# result_follow_up = socratic_graph.invoke(follow_up_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "# pprint.pprint(result_follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be33bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e50563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dee076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f6ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587f007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socra-bot-streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

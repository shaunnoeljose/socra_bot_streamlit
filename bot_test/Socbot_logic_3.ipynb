{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eebbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--0fb0e40d-3e73-41a5-8652-4141a84de7e0-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1188, 'output_tokens': 25, 'total_tokens': 1269, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 56}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] Raw LLM MCQ Response: '```json\\n{\\n    \"question\": \"Given the following Python code:\\\\n\\\\ndef decorator_a(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(\\\\\"Wrapper A: Before\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(\\\\\"Wrapper A: After\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\ndef decorator_b(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(\\\\\"Wrapper B: Before\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(\\\\\"Wrapper B: After\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@decorator_a\\\\n@decorator_b\\\\ndef my_function():\\\\n    print(\\\\\"Original Function\\\\\")\\\\n\\\\nWhat will be the output when `my_function()` is called?\",\\n    \"options\": [\\n        \"Wrapper A: Before\\\\nOriginal Function\\\\nWrapper A: After\",\\n        \"Wrapper A: Before\\\\nWrapper B: Before\\\\nOriginal Function\\\\nWrapper B: After\\\\nWrapper A: After\",\\n        \"Wrapper B: Before\\\\nWrapper A: Before\\\\nOriginal Function\\\\nWrapper A: After\\\\nWrapper B: After\",\\n        \"Original Function\\\\nWrapper B: Before\\\\nWrapper A: Before\\\\nWrapper A: After\\\\nWrapper B: After\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"Decorators are applied from bottom to top. This means `@decorator_b` is applied first to `my_function`, and then `@decorator_a` is applied to the result of `@decorator_b` (which is `decorator_b`\\'s `wrapper` function).\\\\n\\\\nWhen `my_function()` is called, the execution flow is as follows:\\\\n1.  The outermost decorator\\'s wrapper (`decorator_a`\\'s `wrapper`) executes first. It prints \\\\\"Wrapper A: Before\\\\\".\\\\n2.  `decorator_a`\\'s `wrapper` then calls the function it wrapped, which is `decorator_b`\\'s `wrapper`.\\\\n3.  `decorator_b`\\'s `wrapper` executes. It prints \\\\\"Wrapper B: Before\\\\\".\\\\n4.  `decorator_b`\\'s `wrapper` then calls the function it wrapped, which is the original `my_function`.\\\\n5.  The original `my_function` executes, printing \\\\\"Original Function\\\\\".\\\\n6.  Execution returns to `decorator_b`\\'s `wrapper`, which then prints \\\\\"Wrapper B: After\\\\\".\\\\n7.  Execution returns to `decorator_a`\\'s `wrapper`, which then prints \\\\\"Wrapper A: After\\\\\".\"\\n}\\n```'\n",
      "[DEBUG] Cleaned LLM MCQ Response: '{\\n    \"question\": \"Given the following Python code:  def decorator_a(func):     def wrapper(*args, **kwargs):         print(\"Wrapper A: Before\")         result = func(*args, **kwargs)         print(\"Wrapper A: After\")         return result     return wrapper  def decorator_b(func):     def wrapper(*args, **kwargs):         print(\"Wrapper B: Before\")         result = func(*args, **kwargs)         print(\"Wrapper B: After\")         return result     return wrapper  @decorator_a @decorator_b def my_function():     print(\"Original Function\")  What will be the output when `my_function()` is called?\",\\n    \"options\": [\\n        \"Wrapper A: Before Original Function Wrapper A: After\",\\n        \"Wrapper A: Before Wrapper B: Before Original Function Wrapper B: After Wrapper A: After\",\\n        \"Wrapper B: Before Wrapper A: Before Original Function Wrapper A: After Wrapper B: After\",\\n        \"Original Function Wrapper B: Before Wrapper A: Before Wrapper A: After Wrapper B: After\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"Decorators are applied from bottom to top. This means `@decorator_b` is applied first to `my_function`, and then `@decorator_a` is applied to the result of `@decorator_b` (which is `decorator_b`\\'s `wrapper` function).  When `my_function()` is called, the execution flow is as follows: 1.  The outermost decorator\\'s wrapper (`decorator_a`\\'s `wrapper`) executes first. It prints \"Wrapper A: Before\". 2.  `decorator_a`\\'s `wrapper` then calls the function it wrapped, which is `decorator_b`\\'s `wrapper`. 3.  `decorator_b`\\'s `wrapper` executes. It prints \"Wrapper B: Before\". 4.  `decorator_b`\\'s `wrapper` then calls the function it wrapped, which is the original `my_function`. 5.  The original `my_function` executes, printing \"Original Function\". 6.  Execution returns to `decorator_b`\\'s `wrapper`, which then prints \"Wrapper B: After\". 7.  Execution returns to `decorator_a`\\'s `wrapper`, which then prints \"Wrapper A: After\".\"\\n}\\n'\n",
      "[ERROR] Could not parse cleaned JSON: '{\\n    \"question\": \"Given the following Python code:  def decorator_a(func):     def wrapper(*args, **kwargs):         print(\"Wrapper A: Before\")         result = func(*args, **kwargs)         print(\"Wrapper A: After\")         return result     return wrapper  def decorator_b(func):     def wrapper(*args, **kwargs):         print(\"Wrapper B: Before\")         result = func(*args, **kwargs)         print(\"Wrapper B: After\")         return result     return wrapper  @decorator_a @decorator_b def my_function():     print(\"Original Function\")  What will be the output when `my_function()` is called?\",\\n    \"options\": [\\n        \"Wrapper A: Before Original Function Wrapper A: After\",\\n        \"Wrapper A: Before Wrapper B: Before Original Function Wrapper B: After Wrapper A: After\",\\n        \"Wrapper B: Before Wrapper A: Before Original Function Wrapper A: After Wrapper B: After\",\\n        \"Original Function Wrapper B: Before Wrapper A: Before Wrapper A: After Wrapper B: After\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"Decorators are applied from bottom to top. This means `@decorator_b` is applied first to `my_function`, and then `@decorator_a` is applied to the result of `@decorator_b` (which is `decorator_b`\\'s `wrapper` function).  When `my_function()` is called, the execution flow is as follows: 1.  The outermost decorator\\'s wrapper (`decorator_a`\\'s `wrapper`) executes first. It prints \"Wrapper A: Before\". 2.  `decorator_a`\\'s `wrapper` then calls the function it wrapped, which is `decorator_b`\\'s `wrapper`. 3.  `decorator_b`\\'s `wrapper` executes. It prints \"Wrapper B: Before\". 4.  `decorator_b`\\'s `wrapper` then calls the function it wrapped, which is the original `my_function`. 5.  The original `my_function` executes, printing \"Original Function\". 6.  Execution returns to `decorator_b`\\'s `wrapper`, which then prints \"Wrapper B: After\". 7.  Execution returns to `decorator_a`\\'s `wrapper`, which then prints \"Wrapper A: After\".\"\\n}\\n'\n",
      "[ERROR] JSONDecodeError: Expecting ',' delimiter: line 2 column 124 (char 125)\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c4cbc01d-17b7-432d-83e8-9f55f9ca87df', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}]), ToolMessage(content='Error: MCQ agent returned invalid JSON.', id='5de91f05-70f4-481d-a8d2-7bb65022c9c2', tool_call_id='7e61c2d9-05b3-4859-973a-d90c18da6fbb')]\n",
      "[DEBUG] LLM Response: content='Thought: The user wants an MCQ on Python decorators. My previous attempt to use `llm_mcq_generator` failed. I should try the `mcq_agent` tool, as it might have a predefined question for this topic. I will set the difficulty to \"intermediate\".' additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--d3a36985-6b74-4b55-9adb-420734428ac4-0' tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '8c802060-dc6b-4b3b-b601-4666b4d6c398', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1239, 'output_tokens': 81, 'total_tokens': 1392, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 72}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c4cbc01d-17b7-432d-83e8-9f55f9ca87df', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}]), ToolMessage(content='Error: MCQ agent returned invalid JSON.', id='5de91f05-70f4-481d-a8d2-7bb65022c9c2', tool_call_id='7e61c2d9-05b3-4859-973a-d90c18da6fbb'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='3a816d94-8c57-4bc5-a290-83b508482a68', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '8c802060-dc6b-4b3b-b601-4666b4d6c398', 'type': 'tool_call'}]), ToolMessage(content='No predefined MCQ found for this topic.', id='7c7c2d18-a87c-401f-af05-e6c557f6e86e', tool_call_id='8c802060-dc6b-4b3b-b601-4666b4d6c398')]\n",
      "[DEBUG] LLM Response: content=\"Thought: I tried to generate an MCQ on Python decorators using both the `llm_mcq_generator` and `mcq_agent` tools, but neither was successful. The `llm_mcq_generator` returned an error, and the `mcq_agent` indicated that there are no predefined MCQs for decorators. Therefore, I cannot provide an MCQ on that topic right now. I should let the user know and offer an alternative.\\n\\nIt seems I don't have a pre-built MCQ on Python decorators at the moment. Would you like to try an MCQ on a different Python topic, or perhaps a coding challenge related to decorators?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--2e6f596e-b845-42ee-82ca-2dfa78cf735c-0' usage_metadata={'input_tokens': 1284, 'output_tokens': 136, 'total_tokens': 1521, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 101}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent's turn ends (no tool calls).\n",
      "{'agent_thought': 'I tried to generate an MCQ on Python decorators using both '\n",
      "                  'the `llm_mcq_generator` and `mcq_agent` tools, but neither '\n",
      "                  'was successful. The `llm_mcq_generator` returned an error, '\n",
      "                  'and the `mcq_agent` indicated that there are no predefined '\n",
      "                  'MCQs for decorators. Therefore, I cannot provide an MCQ on '\n",
      "                  'that topic right now. I should let the user know and offer '\n",
      "                  'an alternative.',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='a8e565cf-c1ed-4977-8989-760b78f97cfe'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c4cbc01d-17b7-432d-83e8-9f55f9ca87df', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '7e61c2d9-05b3-4859-973a-d90c18da6fbb', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='Error: MCQ agent returned invalid JSON.', id='5de91f05-70f4-481d-a8d2-7bb65022c9c2', tool_call_id='7e61c2d9-05b3-4859-973a-d90c18da6fbb'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='3a816d94-8c57-4bc5-a290-83b508482a68', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '8c802060-dc6b-4b3b-b601-4666b4d6c398', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='No predefined MCQ found for this topic.', id='7c7c2d18-a87c-401f-af05-e6c557f6e86e', tool_call_id='8c802060-dc6b-4b3b-b601-4666b4d6c398'),\n",
      "              AIMessage(content=\"It seems I don't have a pre-built MCQ on Python decorators at the moment. Would you like to try an MCQ on a different Python topic, or perhaps a coding challenge related to decorators?\", additional_kwargs={}, response_metadata={}, id='89b864a8-eef3-4c2d-8f3c-ea41232009d6')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': '',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "import time\n",
    "import re # Import regex for answer parsing\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic tutoring agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: A list of BaseMessage objects representing the conversation history.\n",
    "                  Annotated with add_messages to automatically append new messages.\n",
    "        difficulty_level: The current difficulty level of the tutoring session (e.g., \"beginner\", \"intermediate\").\n",
    "        user_struggle_count: An integer tracking how many times the user has struggled or answered incorrectly.\n",
    "        topic: The main Python topic currently being discussed.\n",
    "        sub_topic: A more specific sub-topic within the main topic.\n",
    "        mcq_active: A boolean indicating if a Multiple Choice Question is currently active.\n",
    "        mcq_question: The full text of the active MCQ, including options.\n",
    "        mcq_options: A list of strings, each representing an option for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer (e.g., \"A\", \"B\", \"C\", \"D\") for the active MCQ.\n",
    "        agent_thought: The internal thought process of the Socratic LLM before generating a response.\n",
    "        next_node_decision: A string indicating the next node the router should transition to.\n",
    "                            Used by the supervisor/router to control graph flow.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_node_decision: str\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the main Socratic LLM for general conversation and tool binding.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "\n",
    "# Initialize a separate LLM for generating MCQs. This allows for different\n",
    "# temperature or model settings specifically for MCQ generation.\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5)\n",
    "\n",
    "# System prompt for the Socratic LLM, guiding its behavior and principles.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1.  **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2.  **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3.  **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4.  **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5.  **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides code and asks for feedback.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge.\n",
    "    * `mcq_agent`: Use this when you want to generate a multiple-choice question for **well-known or predefined topics** (like \"variables\", \"functions\", \"classes\"). This tool has pre-built questions.\n",
    "    * `llm_mcq_generator`: Use this when the user asks for an MCQ on a topic that is **not explicitly covered by the `mcq_agent`'s predefined list**, or if you believe a more custom or nuanced question is needed based on the current discussion. This tool will ask the LLM to create a new MCQ.\n",
    "    * `mcq_answer_processor`: Use this tool when the user submits an answer to an active MCQ. Provide the user's answer and the correct answer to this tool. This tool will handle updating the struggle count and resetting the MCQ state.\n",
    "6.  **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7.  **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8.  **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "MCQ Question (internal): {mcq_question} # Note: This is now the formatted string\n",
    "MCQ Options (internal): {mcq_options}\n",
    "MCQ Correct Answer (internal): {mcq_correct_answer}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Chat prompt template for the Socratic LLM, including system prompt and message history.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Tools ---\n",
    "# These tools simulate external functionalities that the Socratic LLM can call.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code.\n",
    "    This is a simulated tool. In a real application, it would run static analysis, linters, etc.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept.\n",
    "    This is a simulated tool. In a real application, it would provide detailed explanations.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge based on a topic and difficulty level.\n",
    "    This is a simulated tool. In a real application, it would generate a specific coding problem.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level\n",
    "    from a predefined list. The output will be a JSON string containing the question,\n",
    "    options, and correct answer. The 'question' field will be pre-formatted to include\n",
    "    options for direct display.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ\n",
    "    and a predefined question is available for the topic.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the exact topic exists in our predefined list (case-insensitive)\n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        # Format the question to include options for direct display in chat\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        # If topic not found, return a special string to indicate that the LLM should\n",
    "        # consider using the `llm_mcq_generator` tool instead.\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an MCQ using an LLM based on a topic and difficulty level.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python tutor who generates multiple choice questions (MCQs) for practice.\n",
    "Generate an MCQ on the topic \"{topic}\" at a \"{difficulty}\" level.\n",
    "\n",
    "The MCQ must follow this format strictly as a JSON object:\n",
    "{{\n",
    "    \"question\": \"string\",\n",
    "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "    \"answer_index\": 1,\n",
    "    \"explanation\": \"string\"\n",
    "}}\n",
    "\n",
    "For questions involving code snippets, format the code within triple backticks (```) to preserve readability, and ensure all strings are JSON-compatible (newlines escaped as \\\\n).\n",
    "DO NOT include outer markdown code fences like ```json or ```python\n",
    "Respond with raw valid JSON only. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    raw_llm_content = llm_response.content.strip()\n",
    "    print(\"[DEBUG] Raw LLM MCQ Response:\", repr(raw_llm_content))\n",
    "\n",
    "    # Strip outer markdown fences\n",
    "    cleaned_content = re.sub(r'^```(json|python)?\\n?', '', raw_llm_content, flags=re.MULTILINE)\n",
    "    cleaned_content = re.sub(r'\\n?```$', '', cleaned_content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace problematic characters, but preserve code formatting\n",
    "    cleaned_content = (\n",
    "        cleaned_content\n",
    "        .replace('“', '\"').replace('”', '\"')  # Replace smart quotes\n",
    "        .replace('‘', \"'\").replace('’', \"'\")  # Replace smart single quotes\n",
    "        .replace('\\u201c', '\"').replace('\\u201d', '\"')  # Replace Unicode quotes\n",
    "        .replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Replace Unicode single quotes\n",
    "        .replace('\\t', '    ')  # Replace tabs with spaces for code readability\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Cleaned LLM MCQ Response:\", repr(cleaned_content))\n",
    "\n",
    "    try:\n",
    "        mcq_data = json.loads(cleaned_content)\n",
    "        # Validate JSON structure\n",
    "        required_keys = {\"question\", \"options\", \"answer_index\", \"explanation\"}\n",
    "        if not all(key in mcq_data for key in required_keys):\n",
    "            raise ValueError(\"Invalid MCQ format: Missing required keys\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"Invalid MCQ format: Options must be a list of 4 strings\")\n",
    "        if not isinstance(mcq_data[\"answer_index\"], int) or mcq_data[\"answer_index\"] not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid MCQ format: answer_index must be an integer between 0 and 3\")\n",
    "        \n",
    "        # Format question for display with options\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data[\"options\"])\n",
    "        mcq_data[\"question\"] = formatted_question\n",
    "        return mcq_data\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(\"[ERROR] JSON parsing or validation failed:\", str(e))\n",
    "        print(\"[ERROR] Cleaned content:\", repr(cleaned_content))\n",
    "        default_mcq = {\n",
    "            \"question\": \"**What is a Python decorator?**\\n\\n\" + \n",
    "                        \"\\n\".join([\n",
    "                            \"A) A function that modifies another function or method\",\n",
    "                            \"B) A type of class inheritance\",\n",
    "                            \"C) A syntax for defining variables\",\n",
    "                            \"D) A loop construct\"\n",
    "                        ]),\n",
    "            \"options\": [\n",
    "                \"A) A function that modifies another function or method\",\n",
    "                \"B) A type of class inheritance\",\n",
    "                \"C) A syntax for defining variables\",\n",
    "                \"D) A loop construct\"\n",
    "            ],\n",
    "            \"answer_index\": 0,\n",
    "            \"explanation\": \"A Python decorator is a function that wraps another function or method to extend or modify its behavior.\"\n",
    "        }\n",
    "        return {\"error\": f\"Failed to parse or validate MCQ JSON: {str(e)}\", **default_mcq}\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ.\n",
    "    Compares the user's answer with the correct answer and returns feedback.\n",
    "    This tool is used by the `process_mcq_answer` node.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return \"Correct!\"\n",
    "    else:\n",
    "        return \"Incorrect.\"\n",
    "\n",
    "\n",
    "# List of all tools available to the Socratic LLM\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent, llm_mcq_generator, mcq_answer_processor]\n",
    "# Bind the tools to the main Socratic LLM, allowing it to call them.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "# Combine the prompt and the LLM with tools into a runnable for the Socratic agent.\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Invokes the Socratic LLM with the current conversation history and state.\n",
    "    This node represents the core logic of the Socratic tutoring agent.\n",
    "    It extracts the LLM's \"thought\" and formats the content for display.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_llm node.\")\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    \n",
    "    # Invoke the Socratic agent runnable with the current state.\n",
    "    # Exclude 'messages' and 'next_node_decision' as they are handled separately by LangGraph/prompt.\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        **{k: v for k, v in state.items() if k not in ['messages', 'next_node_decision']}\n",
    "    })\n",
    "    print(\"[DEBUG] LLM Response:\", response)\n",
    "    \n",
    "    thought = \"\"\n",
    "    display_content = response.content # Initialize with the full content from LLM\n",
    "\n",
    "    # Extract the internal \"Thought:\" from the LLM's response if present.\n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        thought_and_content = parts[1].strip()\n",
    "        \n",
    "        # The thought is typically the first line after \"Thought:\"\n",
    "        thought_lines = thought_and_content.split('\\n', 1)\n",
    "        thought = thought_lines[0].strip()\n",
    "        \n",
    "        # The display content is the rest, if any\n",
    "        if len(thought_lines) > 1:\n",
    "            display_content = thought_lines[1].strip()\n",
    "        else:\n",
    "            display_content = \"\" # If only thought was present, hide it from user display\n",
    "\n",
    "    # Create a new AIMessage to add to the state, preserving tool calls.\n",
    "    new_ai_message = AIMessage(\n",
    "        content=display_content,\n",
    "        tool_calls=response.tool_calls\n",
    "    )\n",
    "\n",
    "    # Return the updated messages and the extracted thought.\n",
    "    return {\"messages\": [new_ai_message], \"agent_thought\": thought}\n",
    "\n",
    "# A dictionary mapping tool names to their corresponding Python functions.\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "    \"llm_mcq_generator\": llm_mcq_generator,\n",
    "    \"mcq_answer_processor\": mcq_answer_processor,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Executes a tool call requested by the Socratic LLM.\n",
    "    Updates the state with the tool's output and relevant MCQ information if applicable.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_tool node.\")\n",
    "    last_message = state[\"messages\"][-1] # Get the last message, which should contain tool calls.\n",
    "    \n",
    "    messages_to_add = [] # List to accumulate new messages (e.g., ToolMessage)\n",
    "    state_updates = {}   # Dictionary to accumulate state changes\n",
    "\n",
    "    # Check if the last message is an AIMessage and contains tool calls.\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name) # Get the actual function for the tool.\n",
    "            \n",
    "            tool_output_content = \"\"\n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args) # Invoke the tool.\n",
    "                tool_output_content = str(response) # Convert tool response to string.\n",
    "\n",
    "                # Special handling for MCQ generation tools (`mcq_agent`, `llm_mcq_generator`)\n",
    "                if tool_name in [\"mcq_agent\", \"llm_mcq_generator\"]:\n",
    "                    if tool_output_content == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "                        # If mcq_agent couldn't find a predefined MCQ, add a ToolMessage\n",
    "                        # and continue, allowing the LLM to potentially call llm_mcq_generator next.\n",
    "                        messages_to_add.append(\n",
    "                            ToolMessage(content=\"No predefined MCQ found for this topic.\", tool_call_id=tool_call[\"id\"])\n",
    "                        )\n",
    "                        continue # Skip to next tool call or finish this loop.\n",
    "                    \n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content) if isinstance(response, str) else response\n",
    "                        # Check if the MCQ generation tool returned an error.\n",
    "                        if \"error\" in mcq_data:\n",
    "                            messages_to_add.append(\n",
    "                                ToolMessage(content=f\"Error generating MCQ: {mcq_data['error']}\", tool_call_id=tool_call[\"id\"])\n",
    "                            )\n",
    "                            continue # Skip if there was an error.\n",
    "\n",
    "                        # Update state with MCQ details if generation was successful.\n",
    "                        state_updates[\"mcq_active\"] = True\n",
    "                        state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\")\n",
    "                        state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                        state_updates[\"mcq_correct_answer\"] = mcq_data.get(\"correct_answer\", \"\")\n",
    "                        # If a topic wasn't set yet in the state, set it from the tool arguments.\n",
    "                        if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                            state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Handle cases where the tool's output is not valid JSON.\n",
    "                        messages_to_add.append(\n",
    "                            ToolMessage(content=\"Error: MCQ agent returned invalid JSON.\", tool_call_id=tool_call[\"id\"])\n",
    "                        )\n",
    "                        continue # Skip to next tool call or finish this loop.\n",
    "\n",
    "                # Special handling for the `mcq_answer_processor` tool.\n",
    "                elif tool_name == \"mcq_answer_processor\":\n",
    "                    if tool_output_content == \"Correct!\":\n",
    "                        state_updates[\"user_struggle_count\"] = 0 # Reset struggle count on correct answer.\n",
    "                    else:\n",
    "                        # Increment struggle count on incorrect answer.\n",
    "                        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "                    \n",
    "                    # Reset MCQ state after processing an answer.\n",
    "                    state_updates[\"mcq_active\"] = False\n",
    "                    state_updates[\"mcq_question\"] = \"\"\n",
    "                    state_updates[\"mcq_options\"] = []\n",
    "                    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "                \n",
    "                # For all other tools, just append their output as a ToolMessage.\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # If a tool function is not found, add an error message.\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "    print(f\"[DEBUG] call_tool: State updates - {state_updates}\")\n",
    "    # Return the new messages and any state updates.\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "def router(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    The supervisor node that decides the next action based on the current state and user input.\n",
    "    It primarily routes between processing an MCQ answer directly or letting the Socratic LLM respond.\n",
    "    \n",
    "    Returns a dictionary containing 'next_node_decision' to control graph flow.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering router node.\")\n",
    "    last_message = state[\"messages\"][-1] # Get the latest message in the conversation.\n",
    "    \n",
    "    next_decision = \"\"\n",
    "    # Check if an MCQ is active and the user's last message looks like an MCQ answer.\n",
    "    if state.get(\"mcq_active\", False) and isinstance(last_message, HumanMessage):\n",
    "        user_input = last_message.content.strip().upper()\n",
    "        # Use regex to robustly match typical MCQ answer formats (A, B, C, D, A., B), etc.\n",
    "        if re.match(r\"^[ABCD](\\.|\\))?$\", user_input):\n",
    "            print(\"[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\")\n",
    "            next_decision = \"process_mcq_answer\"\n",
    "        else:\n",
    "            print(\"[DEBUG] Router: MCQ active but no valid answer, routing to call_llm (Socratic Agent).\")\n",
    "            next_decision = \"call_llm\" # If MCQ active but answer is not valid, let LLM handle it\n",
    "    else:\n",
    "        print(\"[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\")\n",
    "        next_decision = \"call_llm\"\n",
    "    \n",
    "    # Return a dictionary with the decision. LangGraph nodes must return a dict.\n",
    "    return {\"next_node_decision\": next_decision}\n",
    "\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Processes the user's MCQ answer by calling the mcq_answer_processor tool directly.\n",
    "    This node is invoked by the `router` when an MCQ answer is detected.\n",
    "    It updates the state based on the correctness of the answer and provides feedback.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering process_mcq_answer node.\")\n",
    "    last_human_message = None\n",
    "    # Find the most recent HumanMessage in the history.\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        print(\"[ERROR] process_mcq_answer: Could not find a HumanMessage to process.\")\n",
    "        return {\"messages\": []} # If no human message, nothing to process.\n",
    "\n",
    "    user_answer = last_human_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "\n",
    "    # Directly call the `mcq_answer_processor` tool function.\n",
    "    tool_output_content = mcq_answer_processor.invoke({\"user_answer\": user_answer, \"correct_answer\": correct_answer})\n",
    "    \n",
    "    state_updates = {}\n",
    "    messages_to_add = []\n",
    "\n",
    "    # Update struggle count based on the tool's output.\n",
    "    if tool_output_content == \"Correct!\":\n",
    "        state_updates[\"user_struggle_count\"] = 0\n",
    "    else:\n",
    "        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    # Reset all MCQ-related state variables as the MCQ has been answered.\n",
    "    state_updates[\"mcq_active\"] = False\n",
    "    state_updates[\"mcq_question\"] = \"\"\n",
    "    state_updates[\"mcq_options\"] = []\n",
    "    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "\n",
    "    # Add a ToolMessage to the history to record the tool's execution.\n",
    "    messages_to_add.append(\n",
    "        ToolMessage(content=tool_output_content, tool_call_id=\"mcq_processor_direct_call\") # Using a dummy ID for direct call.\n",
    "    )\n",
    "    # Add an AIMessage to provide immediate feedback to the user.\n",
    "    messages_to_add.append(\n",
    "        AIMessage(content=f\"You answered: {user_answer}. The correct answer was {correct_answer}. {tool_output_content}\")\n",
    "    )\n",
    "\n",
    "    print(f\"[DEBUG] process_mcq_answer: Result - {tool_output_content}, New struggle count: {state_updates.get('user_struggle_count')}\")\n",
    "    \n",
    "    # Return the new messages and any state updates.\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue_socratic(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Determines if the Socratic LLM (after its `call_llm` node) needs to call a tool\n",
    "    or if its turn is complete (i.e., it has generated a direct response).\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering should_continue_socratic edge logic.\")\n",
    "    # If the last message from the LLM contains tool calls, transition to `call_tool`.\n",
    "    if isinstance(state[\"messages\"][-1], AIMessage) and state[\"messages\"][-1].tool_calls:\n",
    "        print(\"[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\")\n",
    "        return \"call_tool\"\n",
    "    # Otherwise, the LLM's turn is complete, and the graph can end for this iteration.\n",
    "    print(\"[DEBUG] should_continue_socratic: Socratic agent's turn ends (no tool calls).\")\n",
    "    return END\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Initialize the StateGraph with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add all the nodes to the workflow.\n",
    "workflow.add_node(\"router\", router) # The new supervisor node.\n",
    "workflow.add_node(\"call_llm\", call_llm) # The Socratic agent's LLM logic.\n",
    "workflow.add_node(\"call_tool\", call_tool) # The tool execution logic.\n",
    "workflow.add_node(\"process_mcq_answer\", process_mcq_answer) # Node for direct MCQ answer processing.\n",
    "\n",
    "# Set the `router` node as the starting point of the graph.\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Define conditional edges from the `router` node.\n",
    "# The `router` function itself determines the next node based on the state.\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_node_decision\"], # Use the decision stored in state by the router function.\n",
    "    {\n",
    "        \"call_llm\": \"call_llm\",         # If router decides, go to the Socratic LLM.\n",
    "        \"process_mcq_answer\": \"process_mcq_answer\" # If router detects MCQ answer, go to process it.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define conditional edges from the `call_llm` node (Socratic agent's LLM).\n",
    "# `should_continue_socratic` determines if a tool needs to be called or if the turn ends.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue_socratic,\n",
    "    {\"call_tool\": \"call_tool\", END: END} # If tool calls, go to `call_tool`; otherwise, end.\n",
    ")\n",
    "\n",
    "# Define a regular edge from `call_tool` back to `call_llm`.\n",
    "# After a tool is executed, the Socratic LLM needs to process the tool's output and generate a response.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Define a regular edge from `process_mcq_answer` back to `call_llm`.\n",
    "# After an MCQ answer is processed, the Socratic LLM needs to provide feedback and potentially a new question.\n",
    "workflow.add_edge(\"process_mcq_answer\", \"call_llm\")\n",
    "\n",
    "# Compile the workflow into a runnable graph.\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\", # Topic will be set by the tool if not provided by user\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\" # Initialize this for the router\n",
    "}\n",
    "result_llm_mcq = socratic_graph.invoke(llm_mcq_state)\n",
    "import pprint\n",
    "pprint.pprint(result_llm_mcq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e173d06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'mcq_agent', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--44ef6222-2b3c-4e4a-bd41-507b75ea8720-0' tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1267, 'output_tokens': 22, 'total_tokens': 1375, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 86}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='47ea534d-9f7d-49f1-b206-5cd5a6bc6a6b', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}]), ToolMessage(content='No predefined MCQ found for this topic.', id='3a06a2ac-9c94-4ad5-a54b-640fed0719df', tool_call_id='04aca61f-8333-447c-91f4-0a32de1a12fd')]\n",
      "[DEBUG] LLM Response: content='Thought: The user wants an MCQ on Python decorators, and the `mcq_agent` indicated that there isn\\'t a predefined one. I should use the `llm_mcq_generator` to create a new MCQ for them, maintaining the \"intermediate\" difficulty level.' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--4ce39ecc-e3e2-48c0-b5cb-7392448c9562-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '047b41d1-acd0-4221-a8b4-b0285513ab26', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1312, 'output_tokens': 83, 'total_tokens': 1462, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 67}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[ERROR] Unexpected error in llm_mcq_generator: '\\n            \"question\"'\n",
      "[DEBUG] call_tool: State updates - {}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='47ea534d-9f7d-49f1-b206-5cd5a6bc6a6b', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}]), ToolMessage(content='No predefined MCQ found for this topic.', id='3a06a2ac-9c94-4ad5-a54b-640fed0719df', tool_call_id='04aca61f-8333-447c-91f4-0a32de1a12fd'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ea66e548-b4ae-4382-82e5-5607dccf2ca8', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '047b41d1-acd0-4221-a8b4-b0285513ab26', 'type': 'tool_call'}]), ToolMessage(content='Error generating MCQ: An unexpected error occurred during MCQ generation: \\'\\\\n            \"question\"\\'', id='1cbdb3a5-b7da-44f8-a2c3-3018a7f39a48', tool_call_id='047b41d1-acd0-4221-a8b4-b0285513ab26')]\n",
      "[DEBUG] LLM Response: content='I apologize, but I encountered an error while trying to generate an MCQ on Python decorators at this moment.\\n\\nHowever, we can still explore the topic! To get started, what do you understand a Python decorator to be, in your own words?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--b4d4e97b-9659-4f7e-bfcc-99973bb7bfa7-0' usage_metadata={'input_tokens': 1375, 'output_tokens': 50, 'total_tokens': 1544, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 119}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent's turn ends (no tool calls).\n",
      "{'agent_thought': '',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='5cca2d54-896d-4a78-abb0-8089f3360139'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='47ea534d-9f7d-49f1-b206-5cd5a6bc6a6b', tool_calls=[{'name': 'mcq_agent', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '04aca61f-8333-447c-91f4-0a32de1a12fd', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='No predefined MCQ found for this topic.', id='3a06a2ac-9c94-4ad5-a54b-640fed0719df', tool_call_id='04aca61f-8333-447c-91f4-0a32de1a12fd'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ea66e548-b4ae-4382-82e5-5607dccf2ca8', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '047b41d1-acd0-4221-a8b4-b0285513ab26', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='Error generating MCQ: An unexpected error occurred during MCQ generation: \\'\\\\n            \"question\"\\'', id='1cbdb3a5-b7da-44f8-a2c3-3018a7f39a48', tool_call_id='047b41d1-acd0-4221-a8b4-b0285513ab26'),\n",
      "              AIMessage(content='I apologize, but I encountered an error while trying to generate an MCQ on Python decorators at this moment.\\n\\nHowever, we can still explore the topic! To get started, what do you understand a Python decorator to be, in your own words?', additional_kwargs={}, response_metadata={}, id='a1765dc0-9e10-48d4-bef8-d349d89efa3c')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': '',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- Test ---\n",
    "# This section provides example invocations to test the graph's behavior.\n",
    "# It includes tests for LLM-generated MCQs, correct/incorrect MCQ answers,\n",
    "# and regular conversation turns.\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    # Attempt to draw and display the graph for visualization.\n",
    "    display(Image(socratic_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw graph: {e}. This is often due to missing graphviz or pydot.\")\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\", # Topic will be set by the tool if not provided by user\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\" # Initialize this for the router\n",
    "}\n",
    "result_llm_mcq = socratic_graph.invoke(llm_mcq_state)\n",
    "import pprint\n",
    "pprint.pprint(result_llm_mcq)\n",
    "\n",
    "# Test answering the MCQ (Correct Answer)\n",
    "# print(\"\\n--- Testing answering the MCQ (Correct Answer) ---\")\n",
    "# # Use the state from the previous step which now contains the active MCQ.\n",
    "# mcq_answer_state_correct = result_llm_mcq \n",
    "# mcq_answer_state_correct[\"messages\"].append(HumanMessage(content=\"B\")) # User answers 'B'\n",
    "# mcq_answer_state_correct[\"next_node_decision\"] = \"\" # Reset for router to make a fresh decision\n",
    "# result_mcq_answer_correct = socratic_graph.invoke(mcq_answer_state_correct)\n",
    "# pprint.pprint(result_mcq_answer_correct)\n",
    "\n",
    "# # Test answering the MCQ (Incorrect Answer)\n",
    "# print(\"\\n--- Testing answering the MCQ (Incorrect Answer) ---\")\n",
    "# # Reset state for a new MCQ test to ensure independence.\n",
    "# llm_mcq_state_2 = {\n",
    "#     \"messages\": [HumanMessage(content=\"Can you give me another MCQ, this time on Python generators?\")],\n",
    "#     \"difficulty_level\": \"advanced\",\n",
    "#     \"user_struggle_count\": 0,\n",
    "#     \"topic\": \"\",\n",
    "#     \"sub_topic\": \"\",\n",
    "#     \"mcq_active\": False,\n",
    "#     \"mcq_question\": \"\",\n",
    "#     \"mcq_options\": [],\n",
    "#     \"mcq_correct_answer\": \"\",\n",
    "#     \"agent_thought\": \"\",\n",
    "#     \"next_node_decision\": \"\"\n",
    "# }\n",
    "# result_llm_mcq_2 = socratic_graph.invoke(llm_mcq_state_2)\n",
    "# pprint.pprint(result_llm_mcq_2)\n",
    "\n",
    "# mcq_answer_state_incorrect = result_llm_mcq_2 # Use the state from the previous step.\n",
    "# mcq_answer_state_incorrect[\"messages\"].append(HumanMessage(content=\"A\")) # User answers 'A'\n",
    "# mcq_answer_state_incorrect[\"next_node_decision\"] = \"\" # Reset for router\n",
    "# result_mcq_answer_incorrect = socratic_graph.invoke(mcq_answer_state_incorrect)\n",
    "# pprint.pprint(result_mcq_answer_incorrect)\n",
    "\n",
    "# print(\"\\n--- Testing a regular conversation turn after MCQ ---\")\n",
    "# # Continue conversation after an MCQ has been answered.\n",
    "# regular_turn_state = result_mcq_answer_incorrect # Use the state from the previous step.\n",
    "# regular_turn_state[\"messages\"].append(HumanMessage(content=\"Can you explain more about generators?\"))\n",
    "# regular_turn_state[\"next_node_decision\"] = \"\" # Reset for router\n",
    "# result_regular_turn = socratic_graph.invoke(regular_turn_state)\n",
    "# pprint.pprint(result_regular_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ccd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--40cc2315-b1a2-4e50-94ba-966a1f5077c4-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1139, 'output_tokens': 25, 'total_tokens': 1228, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 64}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] Raw LLM MCQ Response: '```json\\n{\\n    \"question\": \"Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\\\\\"Executing {func.__name__}...\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\\\\\"Finished {func.__name__}.\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```\",\\n    \"options\": [\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\",\\n        \"8\",\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\",\\n        \"Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\"\\n    ],\\n    \"answer_index\": 2,\\n    \"explanation\": \"The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\\\\\"Executing {func.__name__}...\\\\\")` is called first, printing \\\\\"Executing add_numbers...\\\\\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\\\\\"Finished {func.__name__}.\\\\\")` is called next, printing \\\\\"Finished add_numbers.\\\\\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\"\\n}\\n```'\n",
      "[DEBUG] Cleaned LLM MCQ Response: '{\\n    \"question\": \"Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\\\\\"Executing {func.__name__}...\\\\\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\\\\\"Finished {func.__name__}.\\\\\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```\",\\n    \"options\": [\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\",\\n        \"8\",\\n        \"Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\",\\n        \"Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\"\\n    ],\\n    \"answer_index\": 2,\\n    \"explanation\": \"The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\\\\\"Executing {func.__name__}...\\\\\")` is called first, printing \\\\\"Executing add_numbers...\\\\\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\\\\\"Finished {func.__name__}.\\\\\")` is called next, printing \\\\\"Finished add_numbers.\\\\\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\"\\n}\\n'\n",
      "[DEBUG] call_tool: State updates - {'mcq_active': True, 'mcq_question': '**Consider the following Python code:', 'mcq_options': ['Executing add_numbers...\\\\nFinished add_numbers.', '8', 'Executing add_numbers...\\\\nFinished add_numbers.\\\\n8', 'Executing add_numbers...\\\\n8\\\\nFinished add_numbers.'], 'mcq_correct_answer': 'C', 'mcq_explanation': 'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', 'topic': 'Python decorators'}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7')]\n",
      "[DEBUG] LLM Response: content='Please select an option (A, B, C, or D).' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--3a563456-983d-4d74-bc41-0ce04d0da051-0' usage_metadata={'input_tokens': 1985, 'output_tokens': 14, 'total_tokens': 2023, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 24}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\n",
      "{'agent_thought': '',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': True,\n",
      " 'mcq_correct_answer': 'C',\n",
      " 'mcq_explanation': 'The `@log_execution` decorator wraps the `add_numbers` '\n",
      "                    'function. When `add_numbers(5, 3)` is called, the '\n",
      "                    '`wrapper` function inside `log_execution` is executed.\\n'\n",
      "                    '1. `print(f\"Executing {func.__name__}...\")` is called '\n",
      "                    'first, printing \"Executing add_numbers...\".\\n'\n",
      "                    '2. `result = func(*args, **kwargs)` then calls '\n",
      "                    '`add_numbers(5, 3)`, which returns `8`. This `8` is '\n",
      "                    'stored in the `result` variable.\\n'\n",
      "                    '3. `print(f\"Finished {func.__name__}.\")` is called next, '\n",
      "                    'printing \"Finished add_numbers.\".\\n'\n",
      "                    '4. Finally, `return result` returns `8`. If this call is '\n",
      "                    'made in an interactive Python session (like a REPL) or if '\n",
      "                    'its return value is implicitly or explicitly printed '\n",
      "                    '(e.g., `print(add_numbers(5, 3))`), the value `8` will be '\n",
      "                    \"displayed after the decorator's print statements, \"\n",
      "                    'resulting in the combined output:\\n'\n",
      "                    '```\\n'\n",
      "                    'Executing add_numbers...\\n'\n",
      "                    'Finished add_numbers.\\n'\n",
      "                    '8\\n'\n",
      "                    '```',\n",
      " 'mcq_options': ['Executing add_numbers...\\\\nFinished add_numbers.',\n",
      "                 '8',\n",
      "                 'Executing add_numbers...\\\\nFinished add_numbers.\\\\n8',\n",
      "                 'Executing add_numbers...\\\\n8\\\\nFinished add_numbers.'],\n",
      " 'mcq_question': '**Consider the following Python code:',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 0}\n",
      "\n",
      "--- Testing MCQ answer ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\n",
      "[DEBUG] Entering process_mcq_answer node.\n",
      "[DEBUG] process_mcq_answer: Result - Incorrect.\n",
      "\n",
      "Explanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\n",
      "1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\n",
      "2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\n",
      "3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\n",
      "4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator's print statements, resulting in the combined output:\n",
      "```\n",
      "Executing add_numbers...\n",
      "Finished add_numbers.\n",
      "8\n",
      "```, New struggle count: 1\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5')]\n",
      "[DEBUG] LLM Response: content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--47b5e59d-f8c2-41d8-9a64-c2375fdde7a2-0' usage_metadata={'input_tokens': 2205, 'output_tokens': 118, 'total_tokens': 2323, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 1/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 500 An internal error has '\n",
      "                  'occurred. Please retry or report in '\n",
      "                  'https://developers.generativeai.google/guide/troubleshooting',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'),\n",
      "              AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb')],\n",
      " 'next_node_decision': 'process_mcq_answer',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n",
      "\n",
      "--- Testing follow-up question response ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610')]\n",
      "[DEBUG] LLM Response: content=\"Thought: The user has explained the purpose of `*args` and `**kwargs` very well, showing a good understanding of their role in making decorators flexible. This indicates they understand a key aspect of decorator implementation. Now, I should guide them to think about the *return value* of the `wrapper` function and why it's important for the decorated function to behave as expected.\\nThat's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--68242563-b31d-4d7f-b33f-6882bb62d290-0' usage_metadata={'input_tokens': 2402, 'output_tokens': 161, 'total_tokens': 2563, 'input_token_details': {'cache_read': 0}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'), AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 1/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 500 An internal error has '\n",
      "                  'occurred. Please retry or report in '\n",
      "                  'https://developers.generativeai.google/guide/troubleshooting',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'),\n",
      "              AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'),\n",
      "              AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n",
      "\n",
      "--- Testing follow-up question response ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'), AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='eed2f46d-fa5f-4320-95cd-e74ea9d80b6b')]\n",
      "[DEBUG] LLM Response: content=\"That's a great point about `*args` and `**kwargs` making decorators versatile!\\n\\nLet's revisit the previous question to ensure we cover all aspects. In the `log_execution` decorator, we have this line: `return result`.\\n\\nWhat would be the practical consequence if we *removed* that line, and the `wrapper` function didn't explicitly return `result`? How would that change what `add_numbers(5, 3)` gives us?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--bf0cb6f8-a59c-4708-a768-0d0e3a49af4c-0' usage_metadata={'input_tokens': 2561, 'output_tokens': 101, 'total_tokens': 2760, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 98}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'), AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'), AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'), AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='eed2f46d-fa5f-4320-95cd-e74ea9d80b6b'), AIMessage(content=\"That's a great point about `*args` and `**kwargs` making decorators versatile!\\n\\nLet's revisit the previous question to ensure we cover all aspects. In the `log_execution` decorator, we have this line: `return result`.\\n\\nWhat would be the practical consequence if we *removed* that line, and the `wrapper` function didn't explicitly return `result`? How would that change what `add_numbers(5, 3)` gives us?\", additional_kwargs={}, response_metadata={}, id='63c94b00-523b-41c4-91a8-3ccdb760f599')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 5\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 1\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 429 You exceeded your current '\n",
      "                  'quota, please check your plan and billing details. For more '\n",
      "                  'information on this error, head to: '\n",
      "                  'https://ai.google.dev/gemini-api/docs/rate-limits. '\n",
      "                  '[violations {\\n'\n",
      "                  '  quota_metric: '\n",
      "                  '\"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\\n'\n",
      "                  '  quota_id: '\n",
      "                  '\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"model\"\\n'\n",
      "                  '    value: \"gemini-2.5-flash\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"location\"\\n'\n",
      "                  '    value: \"global\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_value: 10\\n'\n",
      "                  '}\\n'\n",
      "                  ', links {\\n'\n",
      "                  '  description: \"Learn more about Gemini API quotas\"\\n'\n",
      "                  '  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n'\n",
      "                  '}\\n'\n",
      "                  ', retry_delay {\\n'\n",
      "                  '  seconds: 57\\n'\n",
      "                  '}\\n'\n",
      "                  ']',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='cad543fa-8293-4894-a77c-eb17d8629d58'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='66237953-a8b9-4a21-9113-757a4297bc43', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '77210a20-0ce9-418a-ba60-0905ed96a940', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef log_execution(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        print(f\"Executing {func.__name__}...\")\\\\n        result = func(*args, **kwargs)\\\\n        print(f\"Finished {func.__name__}.\")\\\\n        return result\\\\n    return wrapper\\\\n\\\\n@log_execution\\\\ndef add_numbers(a, b):\\\\n    return a + b\\\\n\\\\n# What will be the output when add_numbers(5, 3) is called?\\\\n```**\\\\n\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\n8\\\\nExecuting add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\\\nExecuting add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\', \\'options\\': [\\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\', \\'8\\', \\'Executing add_numbers...\\\\\\\\nFinished add_numbers.\\\\\\\\n8\\', \\'Executing add_numbers...\\\\\\\\n8\\\\\\\\nFinished add_numbers.\\'], \\'answer_index\\': 2, \\'explanation\\': \\'The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\\\\\'s print statements, resulting in the combined output:\\\\n```\\\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\\\n```\\'}', id='ff6a6549-6e15-49bb-8ad3-ead1b4757755', tool_call_id='77210a20-0ce9-418a-ba60-0905ed96a940'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef log_execution(func):\\n    def wrapper(*args, **kwargs):\\n        print(f\"Executing {func.__name__}...\")\\n        result = func(*args, **kwargs)\\n        print(f\"Finished {func.__name__}.\")\\n        return result\\n    return wrapper\\n\\n@log_execution\\ndef add_numbers(a, b):\\n    return a + b\\n\\n# What will be the output when add_numbers(5, 3) is called?\\n```**\\n\\nExecuting add_numbers...\\\\nFinished add_numbers.\\n8\\nExecuting add_numbers...\\\\nFinished add_numbers.\\\\n8\\nExecuting add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nOptions:\\nA. Executing add_numbers...\\\\nFinished add_numbers.\\nB. 8\\nC. Executing add_numbers...\\\\nFinished add_numbers.\\\\n8\\nD. Executing add_numbers...\\\\n8\\\\nFinished add_numbers.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='1f5bcab6-7c0a-49ec-8be9-6620c5844bd7'),\n",
      "              AIMessage(content='Please select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='ab61861c-88ef-4517-9f1e-c6be71440edc'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='7ee07fec-c469-4354-8204-6d7c39cd760d'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', id='bbc13dfe-699d-4b8c-a48d-6e66a8c642b1', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was C. Incorrect.\\n\\nExplanation: The `@log_execution` decorator wraps the `add_numbers` function. When `add_numbers(5, 3)` is called, the `wrapper` function inside `log_execution` is executed.\\n1. `print(f\"Executing {func.__name__}...\")` is called first, printing \"Executing add_numbers...\".\\n2. `result = func(*args, **kwargs)` then calls `add_numbers(5, 3)`, which returns `8`. This `8` is stored in the `result` variable.\\n3. `print(f\"Finished {func.__name__}.\")` is called next, printing \"Finished add_numbers.\".\\n4. Finally, `return result` returns `8`. If this call is made in an interactive Python session (like a REPL) or if its return value is implicitly or explicitly printed (e.g., `print(add_numbers(5, 3))`), the value `8` will be displayed after the decorator\\'s print statements, resulting in the combined output:\\n```\\nExecuting add_numbers...\\nFinished add_numbers.\\n8\\n```', additional_kwargs={}, response_metadata={}, id='efb5c3ce-be34-4125-a2c0-157035f907d5'),\n",
      "              AIMessage(content='\\n\\nThought: The user answered A, which was incorrect. The correct answer was C. The explanation provided by the tool clearly outlines the execution flow of the decorator and the decorated function, including the final return value being printed.\\n\\nNow that the user has seen an MCQ and its explanation, it would be beneficial to ask a follow-up question to ensure they understand the core concept of how decorators modify function behavior. I should ask about the role of the `wrapper` function.\\nWhat do you think is the purpose of the `wrapper` function inside the `log_execution` decorator?', additional_kwargs={}, response_metadata={}, id='014daa96-faa6-4551-b266-e90f76661dca'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='2cd51d1e-cc96-4162-b763-881612d174eb'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b0b3a24b-39b9-4959-bf9b-85cd9e7b4610'),\n",
      "              AIMessage(content=\"That's a fantastic explanation of why `*args` and `**kwargs` are used! You've correctly identified how they contribute to the reusability and robustness of decorators.\\n\\nBuilding on that, what would happen if the `wrapper` function *didn't* return `result`? How would that affect the behavior of `add_numbers` after it's been decorated?\", additional_kwargs={}, response_metadata={}, id='446bb84b-2f02-41ba-9895-df8d385d9f12'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='5b7ab09e-0c43-44bf-9281-18d405f588bd'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='eed2f46d-fa5f-4320-95cd-e74ea9d80b6b'),\n",
      "              AIMessage(content=\"That's a great point about `*args` and `**kwargs` making decorators versatile!\\n\\nLet's revisit the previous question to ensure we cover all aspects. In the `log_execution` decorator, we have this line: `return result`.\\n\\nWhat would be the practical consequence if we *removed* that line, and the `wrapper` function didn't explicitly return `result`? How would that change what `add_numbers(5, 3)` gives us?\", additional_kwargs={}, response_metadata={}, id='63c94b00-523b-41c4-91a8-3ccdb760f599'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='d4d777a9-0903-406f-9ff3-6ad686d4bdfe')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n",
      "\n",
      "--- Testing follow-up question response ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='d19206c2-7eaf-41af-b92a-99e94a219d4b'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ba820959-b482-4d15-a54b-cf02589ef3f1', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '0b4310dd-58ed-4ed2-9bdf-21eff844f751', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef uppercase_args_decorator(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\\\n        return func(*processed_args, **processed_kwargs)\\\\n    return wrapper\\\\n\\\\n@uppercase_args_decorator\\\\ndef concatenate_strings(str1, str2, separator=\" \"):\\\\n    return f\"{str1}{separator}{str2}\"\\\\n\\\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\\\n```\\\\nWhat will be the output of this code?**\\\\n\\\\nA) `hello-world`\\\\nB) `HELLO-WORLD`\\\\nC) `HELLO-world`\\\\nD) A `TypeError` will be raised because `separator` is not a string.\\', \\'options\\': [\\'A) `hello-world`\\', \\'B) `HELLO-WORLD`\\', \\'C) `HELLO-world`\\', \\'D) A `TypeError` will be raised because `separator` is not a string.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\\\\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\\\n\\\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\\\n\\\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.\\'}', id='3bc8499b-6aeb-47dc-b3bf-9872546027e6', tool_call_id='0b4310dd-58ed-4ed2-9bdf-21eff844f751'), AIMessage(content='**Consider the following Python code:\\n```python\\ndef uppercase_args_decorator(func):\\n    def wrapper(*args, **kwargs):\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\n        return func(*processed_args, **processed_kwargs)\\n    return wrapper\\n\\n@uppercase_args_decorator\\ndef concatenate_strings(str1, str2, separator=\" \"):\\n    return f\"{str1}{separator}{str2}\"\\n\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\n```\\nWhat will be the output of this code?**\\n\\nA) `hello-world`\\nB) `HELLO-WORLD`\\nC) `HELLO-world`\\nD) A `TypeError` will be raised because `separator` is not a string.\\n\\nOptions:\\nA. A) `hello-world`\\nB. B) `HELLO-WORLD`\\nC. C) `HELLO-world`\\nD. D) A `TypeError` will be raised because `separator` is not a string.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='90319614-1fad-414a-89fd-b50f721a26fa'), AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='f2f075ea-03de-4b36-874e-8e47c953a828'), HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='3ea5f733-c90a-4179-a8c1-47e439ecc9d2'), ToolMessage(content='Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', id='aa6ce6fd-98a8-4fde-836d-079b3948007e', tool_call_id='mcq_processor_direct_call'), AIMessage(content='You answered: A. The correct answer was B. Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', additional_kwargs={}, response_metadata={}, id='2e9a9499-3d20-42c9-8dd6-af1a6c4fde59'), AIMessage(content=\"\\n\\nThought: The user answered the MCQ and it was incorrect. I have provided the explanation for the correct answer. Now I should ask a follow-up question to check their understanding of the explanation or the concept. I can ask them to explain why their initial answer was incorrect, or to re-explain a specific part of the decorator's behavior. I will ask them to explain why the `separator` argument was not converted to uppercase.\\nBased on the explanation, can you tell me why the `separator` argument in the example code was not converted to uppercase, even though it was a string?\", additional_kwargs={}, response_metadata={}, id='579bf0ef-1d9d-4b56-bcc5-2214e847dac1'), HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b8365fb4-508e-4800-95b8-110a9b5fe43a'), AIMessage(content='That\\'s a great explanation of the general purpose of `*args` and `**kwargs` in decorators! You\\'re absolutely right that they make decorators much more flexible.\\n\\nHowever, let\\'s re-focus on the specific behavior of *this particular* `uppercase_args_decorator` and the `separator` argument.\\n\\nLook closely at this line within the `wrapper` function:\\n`processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}`\\n\\nWhen `separator=\"-\"` is passed, what is the value of `v` for the key `\"separator\"`? And what does `v.upper()` evaluate to in that specific case?', additional_kwargs={}, response_metadata={}, id='c8c9105b-85de-4d5d-a465-0eda19e2125b'), HumanMessage(content='The value of `v` for the key `\"separator\"` is `\"-\"`, which is a string. When `v.upper()` is called on `\"-\"`, it evaluates to `\"-\"` because the hyphen has no uppercase equivalent in Python. This is why the `separator` argument remains `\"-\"` and is not changed.', additional_kwargs={}, response_metadata={}, id='3f84ec95-30a5-4e8d-acdf-b4a3061ee8f0')]\n",
      "[ERROR] LLM invocation failed (attempt 1/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 2/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 48\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed (attempt 3/3): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "]\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\n",
      "{'agent_thought': 'Error during LLM invocation: 429 You exceeded your current '\n",
      "                  'quota, please check your plan and billing details. For more '\n",
      "                  'information on this error, head to: '\n",
      "                  'https://ai.google.dev/gemini-api/docs/rate-limits. '\n",
      "                  '[violations {\\n'\n",
      "                  '  quota_metric: '\n",
      "                  '\"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\\n'\n",
      "                  '  quota_id: '\n",
      "                  '\"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"model\"\\n'\n",
      "                  '    value: \"gemini-2.5-flash\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_dimensions {\\n'\n",
      "                  '    key: \"location\"\\n'\n",
      "                  '    value: \"global\"\\n'\n",
      "                  '  }\\n'\n",
      "                  '  quota_value: 10\\n'\n",
      "                  '}\\n'\n",
      "                  ', links {\\n'\n",
      "                  '  description: \"Learn more about Gemini API quotas\"\\n'\n",
      "                  '  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\\n'\n",
      "                  '}\\n'\n",
      "                  ', retry_delay {\\n'\n",
      "                  '  seconds: 46\\n'\n",
      "                  '}\\n'\n",
      "                  ']',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': False,\n",
      " 'mcq_correct_answer': '',\n",
      " 'mcq_explanation': '',\n",
      " 'mcq_options': [],\n",
      " 'mcq_question': '',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='d19206c2-7eaf-41af-b92a-99e94a219d4b'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='ba820959-b482-4d15-a54b-cf02589ef3f1', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '0b4310dd-58ed-4ed2-9bdf-21eff844f751', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \\'**Consider the following Python code:\\\\n```python\\\\ndef uppercase_args_decorator(func):\\\\n    def wrapper(*args, **kwargs):\\\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\\\n        return func(*processed_args, **processed_kwargs)\\\\n    return wrapper\\\\n\\\\n@uppercase_args_decorator\\\\ndef concatenate_strings(str1, str2, separator=\" \"):\\\\n    return f\"{str1}{separator}{str2}\"\\\\n\\\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\\\n```\\\\nWhat will be the output of this code?**\\\\n\\\\nA) `hello-world`\\\\nB) `HELLO-WORLD`\\\\nC) `HELLO-world`\\\\nD) A `TypeError` will be raised because `separator` is not a string.\\', \\'options\\': [\\'A) `hello-world`\\', \\'B) `HELLO-WORLD`\\', \\'C) `HELLO-world`\\', \\'D) A `TypeError` will be raised because `separator` is not a string.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\\\\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\\\n\\\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\\\n\\\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.\\'}', id='3bc8499b-6aeb-47dc-b3bf-9872546027e6', tool_call_id='0b4310dd-58ed-4ed2-9bdf-21eff844f751'),\n",
      "              AIMessage(content='**Consider the following Python code:\\n```python\\ndef uppercase_args_decorator(func):\\n    def wrapper(*args, **kwargs):\\n        processed_args = tuple(arg.upper() if isinstance(arg, str) else arg for arg in args)\\n        processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}\\n        return func(*processed_args, **processed_kwargs)\\n    return wrapper\\n\\n@uppercase_args_decorator\\ndef concatenate_strings(str1, str2, separator=\" \"):\\n    return f\"{str1}{separator}{str2}\"\\n\\nprint(concatenate_strings(\"hello\", \"world\", separator=\"-\"))\\n```\\nWhat will be the output of this code?**\\n\\nA) `hello-world`\\nB) `HELLO-WORLD`\\nC) `HELLO-world`\\nD) A `TypeError` will be raised because `separator` is not a string.\\n\\nOptions:\\nA. A) `hello-world`\\nB. B) `HELLO-WORLD`\\nC. C) `HELLO-world`\\nD. D) A `TypeError` will be raised because `separator` is not a string.\\n\\nPlease select an option (A, B, C, or D).', additional_kwargs={}, response_metadata={}, id='90319614-1fad-414a-89fd-b50f721a26fa'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='f2f075ea-03de-4b36-874e-8e47c953a828'),\n",
      "              HumanMessage(content='A', additional_kwargs={}, response_metadata={}, id='3ea5f733-c90a-4179-a8c1-47e439ecc9d2'),\n",
      "              ToolMessage(content='Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', id='aa6ce6fd-98a8-4fde-836d-079b3948007e', tool_call_id='mcq_processor_direct_call'),\n",
      "              AIMessage(content='You answered: A. The correct answer was B. Incorrect.\\n\\nExplanation: The `uppercase_args_decorator` wraps the `concatenate_strings` function. Inside the `wrapper` function, it processes all positional (`*args`) and keyword (`**kwargs`) arguments. If an argument\\'s value is a string, it converts it to uppercase. Non-string values are passed through unchanged.\\n\\nWhen `concatenate_strings(\"hello\", \"world\", separator=\"-\")` is called:\\n1. `*args` will be `(\"hello\", \"world\")`. These are processed to `(\"HELLO\", \"WORLD\")`.\\n2. `**kwargs` will be `{\"separator\": \"-\"}`. The value `\"-\"` is a string, and `\"-\".upper()` also results in `\"-\"`. So, the `separator` remains `\"-\"`.\\n\\nThe `concatenate_strings` function then effectively receives `str1=\"HELLO\"`, `str2=\"WORLD\"`, and `separator=\"-\"`.\\nThe final return value is `f\"HELLO-WORLD\"`. Therefore, option B is correct.', additional_kwargs={}, response_metadata={}, id='2e9a9499-3d20-42c9-8dd6-af1a6c4fde59'),\n",
      "              AIMessage(content=\"\\n\\nThought: The user answered the MCQ and it was incorrect. I have provided the explanation for the correct answer. Now I should ask a follow-up question to check their understanding of the explanation or the concept. I can ask them to explain why their initial answer was incorrect, or to re-explain a specific part of the decorator's behavior. I will ask them to explain why the `separator` argument was not converted to uppercase.\\nBased on the explanation, can you tell me why the `separator` argument in the example code was not converted to uppercase, even though it was a string?\", additional_kwargs={}, response_metadata={}, id='579bf0ef-1d9d-4b56-bcc5-2214e847dac1'),\n",
      "              HumanMessage(content=\"`*args` and `**kwargs` make the decorator reusable across functions with any signature, avoiding the need to write specific decorators for each function. They prevent signature mismatch errors and ensure the original function's arguments are passed correctly, even if its signature changes later.\", additional_kwargs={}, response_metadata={}, id='b8365fb4-508e-4800-95b8-110a9b5fe43a'),\n",
      "              AIMessage(content='That\\'s a great explanation of the general purpose of `*args` and `**kwargs` in decorators! You\\'re absolutely right that they make decorators much more flexible.\\n\\nHowever, let\\'s re-focus on the specific behavior of *this particular* `uppercase_args_decorator` and the `separator` argument.\\n\\nLook closely at this line within the `wrapper` function:\\n`processed_kwargs = {k: v.upper() if isinstance(v, str) else v for k, v in kwargs.items()}`\\n\\nWhen `separator=\"-\"` is passed, what is the value of `v` for the key `\"separator\"`? And what does `v.upper()` evaluate to in that specific case?', additional_kwargs={}, response_metadata={}, id='c8c9105b-85de-4d5d-a465-0eda19e2125b'),\n",
      "              HumanMessage(content='The value of `v` for the key `\"separator\"` is `\"-\"`, which is a string. When `v.upper()` is called on `\"-\"`, it evaluates to `\"-\"` because the hyphen has no uppercase equivalent in Python. This is why the `separator` argument remains `\"-\"` and is not changed.', additional_kwargs={}, response_metadata={}, id='3f84ec95-30a5-4e8d-acdf-b4a3061ee8f0'),\n",
      "              AIMessage(content='An issue occurred, but you can still answer the MCQ with A, B, C, or D.', additional_kwargs={}, response_metadata={}, id='ce21528f-677d-4419-9065-e2b6179dcb4d')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "import re # Import regex for answer parsing\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic tutoring agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: A list of BaseMessage objects representing the conversation history.\n",
    "                  Annotated with add_messages to automatically append new messages.\n",
    "        difficulty_level: The current difficulty level of the tutoring session (e.g., \"beginner\", \"intermediate\").\n",
    "        user_struggle_count: An integer tracking how many times the user has struggled or answered incorrectly.\n",
    "        topic: The main Python topic currently being discussed.\n",
    "        sub_topic: A more specific sub-topic within the main topic.\n",
    "        mcq_active: A boolean indicating if a Multiple Choice Question is currently active.\n",
    "        mcq_question: The full text of the active MCQ, including options.\n",
    "        mcq_options: A list of strings, each representing an option for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer (e.g., \"A\", \"B\", \"C\", \"D\") for the active MCQ.\n",
    "        mcq_explaination: The explaination for the mcq answer\n",
    "        agent_thought: The internal thought process of the Socratic LLM before generating a response.\n",
    "        next_node_decision: A string indicating the next node the router should transition to.\n",
    "                            Used by the supervisor/router to control graph flow.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    mcq_explanation: str \n",
    "    agent_thought: str\n",
    "    next_node_decision: str\n",
    "\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the main Socratic LLM for general conversation and tool binding.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", max_retries=3, temperature=0.7)\n",
    "\n",
    "# Initialize a separate LLM for generating MCQs. This allows for different\n",
    "# temperature or model settings specifically for MCQ generation.\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_retries=3, temperature=0.5)\n",
    "\n",
    "# System prompt for the Socratic LLM, guiding its behavior and principles.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1. **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2. **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3. **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4. **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5. **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides code and asks for feedback.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge.\n",
    "    * `mcq_agent`: Use this only for well-known topics like \"variables\", \"functions\", \"classes\", \"conditional statements\", \"comparisons\", or \"maximum of three numbers\".\n",
    "    * `llm_mcq_generator`: Use this for all other topics (e.g., \"Python decorators\", \"context managers\") or when a custom MCQ is needed.\n",
    "    * `mcq_answer_processor`: Use this when the user submits an answer to an active MCQ.\n",
    "6. **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7. **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8. **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "9. **MCQ Answer Format:** When presenting an MCQ, instruct the user to respond with a single letter (A, B, C, or D) to indicate their answer choice.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "MCQ Question (internal): {mcq_question}\n",
    "MCQ Options (internal): {mcq_options}\n",
    "MCQ Correct Answer (internal): {mcq_correct_answer}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Chat prompt template for the Socratic LLM, including system prompt and message history.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Tools ---\n",
    "# These tools simulate external functionalities that the Socratic LLM can call.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code.\n",
    "    This is a simulated tool. In a real application, it would run static analysis, linters, etc.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept.\n",
    "    This is a simulated tool. In a real application, it would provide detailed explanations.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge based on a topic and difficulty level.\n",
    "    This is a simulated tool. In a real application, it would generate a specific coding problem.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level\n",
    "    from a predefined list. The output will be a JSON string containing the question,\n",
    "    options, and correct answer. The 'question' field will be pre-formatted to include\n",
    "    options for direct display.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ\n",
    "    and a predefined question is available for the topic.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the exact topic exists in our predefined list (case-insensitive)\n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        # Format the question to include options for direct display in chat\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        # If topic not found, return a special string to indicate that the LLM should\n",
    "        # consider using the `llm_mcq_generator` tool instead.\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an MCQ using an LLM based on a topic and difficulty level.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python tutor who generates multiple choice questions (MCQs) for practice.\n",
    "Generate an MCQ on the topic \"{topic}\" at a \"{difficulty}\" level.\n",
    "\n",
    "The MCQ must follow this format strictly as a JSON object:\n",
    "{{\n",
    "    \"question\": \"string\",\n",
    "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "    \"answer_index\": 1,\n",
    "    \"explanation\": \"string\"\n",
    "}}\n",
    "\n",
    "For questions involving code snippets, format the code within triple backticks (```) to preserve readability, and ensure all strings are JSON-compatible (newlines escaped as \\\\n).\n",
    "DO NOT include outer markdown code fences like ```json or ```python\n",
    "Respond with raw valid JSON only. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    raw_llm_content = llm_response.content.strip()\n",
    "    print(\"[DEBUG] Raw LLM MCQ Response:\", repr(raw_llm_content))\n",
    "\n",
    "    # Strip outer markdown fences\n",
    "    cleaned_content = re.sub(r'^```(json|python)?\\n?', '', raw_llm_content, flags=re.MULTILINE)\n",
    "    cleaned_content = re.sub(r'\\n?```$', '', cleaned_content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace problematic characters, but preserve code formatting\n",
    "    cleaned_content = (\n",
    "        cleaned_content\n",
    "        .replace('“', '\"').replace('”', '\"')  # Replace smart quotes\n",
    "        .replace('‘', \"'\").replace('’', \"'\")  # Replace smart single quotes\n",
    "        .replace('\\u201c', '\"').replace('\\u201d', '\"')  # Replace Unicode quotes\n",
    "        .replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Replace Unicode single quotes\n",
    "        .replace('\\t', '    ')  # Replace tabs with spaces for code readability\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Cleaned LLM MCQ Response:\", repr(cleaned_content))\n",
    "\n",
    "    try:\n",
    "        mcq_data = json.loads(cleaned_content)\n",
    "        # Validate JSON structure\n",
    "        required_keys = {\"question\", \"options\", \"answer_index\", \"explanation\"}\n",
    "        if not all(key in mcq_data for key in required_keys):\n",
    "            raise ValueError(\"Invalid MCQ format: Missing required keys\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"Invalid MCQ format: Options must be a list of 4 strings\")\n",
    "        if not isinstance(mcq_data[\"answer_index\"], int) or mcq_data[\"answer_index\"] not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid MCQ format: answer_index must be an integer between 0 and 3\")\n",
    "        \n",
    "        # Format question for display with options\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data[\"options\"])\n",
    "        mcq_data[\"question\"] = formatted_question\n",
    "        return mcq_data\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(\"[ERROR] JSON parsing or validation failed:\", str(e))\n",
    "        print(\"[ERROR] Cleaned content:\", repr(cleaned_content))\n",
    "        default_mcq = {\n",
    "            \"question\": \"**What is a Python decorator?**\\n\\n\" + \n",
    "                        \"\\n\".join([\n",
    "                            \"A) A function that modifies another function or method\",\n",
    "                            \"B) A type of class inheritance\",\n",
    "                            \"C) A syntax for defining variables\",\n",
    "                            \"D) A loop construct\"\n",
    "                        ]),\n",
    "            \"options\": [\n",
    "                \"A) A function that modifies another function or method\",\n",
    "                \"B) A type of class inheritance\",\n",
    "                \"C) A syntax for defining variables\",\n",
    "                \"D) A loop construct\"\n",
    "            ],\n",
    "            \"answer_index\": 0,\n",
    "            \"explanation\": \"A Python decorator is a function that wraps another function or method to extend or modify its behavior.\"\n",
    "        }\n",
    "        return {\"error\": f\"Failed to parse or validate MCQ JSON: {str(e)}\", **default_mcq}\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str, explanation: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ.\n",
    "    Compares the user's answer with the correct answer and returns feedback with explanation.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return f\"Correct!\\n\\nExplanation: {explanation}\"\n",
    "    else:\n",
    "        return f\"Incorrect.\\n\\nExplanation: {explanation}\"\n",
    "\n",
    "\n",
    "# List of all tools available to the Socratic LLM\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent, llm_mcq_generator, mcq_answer_processor]\n",
    "# Bind the tools to the main Socratic LLM, allowing it to call them.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "# Combine the prompt and the LLM with tools into a runnable for the Socratic agent.\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Invokes the Socratic LLM with the current conversation history and state.\n",
    "    This node represents the core logic of the Socratic tutoring agent.\n",
    "    It extracts the LLM's \"thought\" and formats the content for display.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_llm node.\")\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    \n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = socratic_agent_runnable.invoke({\n",
    "                \"messages\": state[\"messages\"],\n",
    "                **{k: v for k, v in state.items() if k not in ['messages', 'next_node_decision']}\n",
    "            })\n",
    "            print(\"[DEBUG] LLM Response:\", response)\n",
    "            \n",
    "            content = response.content\n",
    "            if isinstance(content, list):\n",
    "                content = \"\\n\".join(str(item) for item in content)\n",
    "            elif not isinstance(content, str):\n",
    "                print(\"[ERROR] Unexpected content type:\", type(content))\n",
    "                content = \"\"\n",
    "\n",
    "            thought = \"\"\n",
    "            display_content = \"\"\n",
    "            if content and content.startswith(\"Thought:\"):\n",
    "                parts = content.split(\"Thought:\", 1)\n",
    "                thought_and_content = parts[1].strip()\n",
    "                thought_lines = thought_and_content.split('\\n', 1)\n",
    "                thought = thought_lines[0].strip()\n",
    "                display_content = thought_lines[1].strip() if len(thought_lines) > 1 else \"\"\n",
    "            else:\n",
    "                display_content = content\n",
    "\n",
    "            new_ai_message = AIMessage(\n",
    "                content=display_content,\n",
    "                tool_calls=response.tool_calls\n",
    "            )\n",
    "\n",
    "            return {\"messages\": [new_ai_message], \"agent_thought\": thought}\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] LLM invocation failed (attempt {attempt + 1}/{max_attempts}):\", str(e))\n",
    "            if attempt == max_attempts - 1:\n",
    "                return {\n",
    "                    \"messages\": [AIMessage(content=\"An issue occurred, but you can still answer the MCQ with A, B, C, or D.\")],\n",
    "                    \"agent_thought\": f\"Error during LLM invocation: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2.0)\n",
    "\n",
    "# A dictionary mapping tool names to their corresponding Python functions.\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "    \"llm_mcq_generator\": llm_mcq_generator,\n",
    "    \"mcq_answer_processor\": mcq_answer_processor,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering call_tool node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    messages_to_add = []\n",
    "    state_updates = {}\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            \n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_content = str(response)\n",
    "\n",
    "                existing_tool_message = any(\n",
    "                    isinstance(msg, ToolMessage) and msg.tool_call_id == tool_call[\"id\"]\n",
    "                    for msg in state[\"messages\"]\n",
    "                )\n",
    "                \n",
    "                if not existing_tool_message:\n",
    "                    messages_to_add.append(\n",
    "                        ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "                    )\n",
    "\n",
    "                if tool_name in [\"mcq_agent\", \"llm_mcq_generator\"]:\n",
    "                    if tool_output_content == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"No predefined MCQ found for this topic. Generating a new one...\")\n",
    "                        )\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content) if isinstance(response, str) else response\n",
    "                        if \"error\" in mcq_data:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                        else:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"Error: MCQ agent returned invalid JSON. Please try again.\")\n",
    "                        )\n",
    "                        continue\n",
    "                elif tool_name == \"mcq_answer_processor\":\n",
    "                    if \"Correct!\" in tool_output_content:\n",
    "                        state_updates[\"user_struggle_count\"] = 0\n",
    "                    else:\n",
    "                        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "                    state_updates[\"mcq_active\"] = False\n",
    "                    state_updates[\"mcq_question\"] = \"\"\n",
    "                    state_updates[\"mcq_options\"] = []\n",
    "                    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "                    state_updates[\"mcq_explanation\"] = \"\"\n",
    "            else:\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "    print(f\"[DEBUG] call_tool: State updates - {state_updates}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "def router(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    The supervisor node that decides the next action based on the current state and user input.\n",
    "    It primarily routes between processing an MCQ answer directly or letting the Socratic LLM respond.\n",
    "    \n",
    "    Returns a dictionary containing 'next_node_decision' to control graph flow.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering router node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    next_decision = \"\"\n",
    "    if state.get(\"mcq_active\", False) and isinstance(last_message, HumanMessage):\n",
    "        user_input = last_message.content.strip().upper()\n",
    "        if re.match(r\"^[ABCD](\\.|\\))?$\", user_input):\n",
    "            print(\"[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\")\n",
    "            next_decision = \"process_mcq_answer\"\n",
    "        else:\n",
    "            print(\"[DEBUG] Router: MCQ active but invalid answer, prompting for valid input.\")\n",
    "            return {\n",
    "                \"next_node_decision\": \"call_llm\",\n",
    "                \"messages\": [AIMessage(content=\"Please respond with a single letter (A, B, C, or D) to select your answer.\")]\n",
    "            }\n",
    "    else:\n",
    "        print(\"[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\")\n",
    "        next_decision = \"call_llm\"\n",
    "    \n",
    "    return {\"next_node_decision\": next_decision}\n",
    "\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering process_mcq_answer node.\")\n",
    "    last_human_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        print(\"[ERROR] process_mcq_answer: Could not find a HumanMessage to process.\")\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    user_answer = last_human_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    explanation = state.get(\"mcq_explanation\", \"No explanation available.\")\n",
    "    \n",
    "    tool_output_content = mcq_answer_processor.invoke({\n",
    "        \"user_answer\": user_answer,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "    \n",
    "    state_updates = {}\n",
    "    messages_to_add = []\n",
    "\n",
    "    if \"Correct!\" in tool_output_content:\n",
    "        state_updates[\"user_struggle_count\"] = 0\n",
    "    else:\n",
    "        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    state_updates[\"mcq_active\"] = False\n",
    "    state_updates[\"mcq_question\"] = \"\"\n",
    "    state_updates[\"mcq_options\"] = []\n",
    "    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "    state_updates[\"mcq_explanation\"] = \"\"\n",
    "\n",
    "    messages_to_add.append(\n",
    "        ToolMessage(content=tool_output_content, tool_call_id=\"mcq_processor_direct_call\")\n",
    "    )\n",
    "    messages_to_add.append(\n",
    "        AIMessage(content=f\"You answered: {user_answer}. The correct answer was {correct_answer}. {tool_output_content}\")\n",
    "    )\n",
    "\n",
    "    print(f\"[DEBUG] process_mcq_answer: Result - {tool_output_content}, New struggle count: {state_updates.get('user_struggle_count')}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue_socratic(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering should_continue_socratic edge logic.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        print(\"[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\")\n",
    "        return \"call_tool\"\n",
    "    elif state.get(\"mcq_active\", False) and isinstance(last_message, AIMessage) and \"Please select an option (A, B, C, or D)\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\")\n",
    "        return \"END\"\n",
    "    elif isinstance(last_message, AIMessage) and \"An issue occurred\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\")\n",
    "        return \"END\"\n",
    "    else:\n",
    "        print(\"[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Initialize the StateGraph with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add all the nodes to the workflow.\n",
    "workflow.add_node(\"router\", router) # The new supervisor node.\n",
    "workflow.add_node(\"call_llm\", call_llm) # The Socratic agent's LLM logic.\n",
    "workflow.add_node(\"call_tool\", call_tool) # The tool execution logic.\n",
    "workflow.add_node(\"process_mcq_answer\", process_mcq_answer) # Node for direct MCQ answer processing.\n",
    "\n",
    "# Set the `router` node as the starting point of the graph.\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Define conditional edges from the `router` node.\n",
    "# The `router` function itself determines the next node based on the state.\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_node_decision\"], # Use the decision stored in state by the router function.\n",
    "    {\n",
    "        \"call_llm\": \"call_llm\",         # If router decides, go to the Socratic LLM.\n",
    "        \"process_mcq_answer\": \"process_mcq_answer\" # If router detects MCQ answer, go to process it.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define conditional edges from the `call_llm` node (Socratic agent's LLM).\n",
    "# `should_continue_socratic` determines if a tool needs to be called or if the turn ends.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue_socratic,\n",
    "    {\"call_tool\": \"call_tool\", \"END\": END, \"call_llm\": \"call_llm\"} # If tool calls, go to `call_tool`; otherwise, end.\n",
    ")\n",
    "\n",
    "# Define a regular edge from `call_tool` back to `call_llm`.\n",
    "# After a tool is executed, the Socratic LLM needs to process the tool's output and generate a response.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Define a regular edge from `process_mcq_answer` back to `call_llm`.\n",
    "# After an MCQ answer is processed, the Socratic LLM needs to provide feedback and potentially a new question.\n",
    "workflow.add_edge(\"process_mcq_answer\", \"call_llm\")\n",
    "\n",
    "# Compile the workflow into a runnable graph.\n",
    "socratic_graph = workflow.compile()\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"mcq_explanation\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\"\n",
    "}\n",
    "result_llm_mcq = socratic_graph.invoke(llm_mcq_state)\n",
    "import pprint\n",
    "pprint.pprint(result_llm_mcq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465fe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade99b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7132ac307b30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Memory to the LangGraph\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "import re # Import regex for answer parsing\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Socratic tutoring agent.\n",
    "    \n",
    "    Attributes:\n",
    "        messages: A list of BaseMessage objects representing the conversation history.\n",
    "                  Annotated with add_messages to automatically append new messages.\n",
    "        difficulty_level: The current difficulty level of the tutoring session (e.g., \"beginner\", \"intermediate\").\n",
    "        user_struggle_count: An integer tracking how many times the user has struggled or answered incorrectly.\n",
    "        topic: The main Python topic currently being discussed.\n",
    "        sub_topic: A more specific sub-topic within the main topic.\n",
    "        mcq_active: A boolean indicating if a Multiple Choice Question is currently active.\n",
    "        mcq_question: The full text of the active MCQ, including options.\n",
    "        mcq_options: A list of strings, each representing an option for the active MCQ.\n",
    "        mcq_correct_answer: The correct answer (e.g., \"A\", \"B\", \"C\", \"D\") for the active MCQ.\n",
    "        mcq_explaination: The explaination for the mcq answer\n",
    "        agent_thought: The internal thought process of the Socratic LLM before generating a response.\n",
    "        next_node_decision: A string indicating the next node the router should transition to.\n",
    "                            Used by the supervisor/router to control graph flow.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    mcq_explanation: str \n",
    "    agent_thought: str\n",
    "    next_node_decision: str\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "# --- 2. Initialize the Socratic LLM and Tools ---\n",
    "\n",
    "# Initialize the main Socratic LLM for general conversation and tool binding.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", max_retries=3, temperature=0.7)\n",
    "\n",
    "# Initialize a separate LLM for generating MCQs. This allows for different\n",
    "# temperature or model settings specifically for MCQ generation.\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_retries=3, temperature=0.5)\n",
    "\n",
    "# System prompt for the Socratic LLM, guiding its behavior and principles.\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Here are your core principles:\n",
    "1. **Ask Questions:** Always respond with a question, unless explicitly providing feedback on code or an MCQ answer.\n",
    "2. **Socratic Method:** Break down complex problems into smaller, manageable questions.\n",
    "3. **Encourage Exploration:** Prompt the user to experiment, research, or think critically.\n",
    "4. **Adapt to User Understanding:**\n",
    "    * **Struggle Detection:** If the user seems confused, provides incorrect answers, or asks for direct solutions, simplify your questions, rephrase, or offer a hint. You can also suggest taking a multiple-choice question (MCQ) to assess their understanding differently.\n",
    "    * **Progression:** If the user demonstrates understanding, subtly move to a slightly more advanced sub-concept or a related new topic. Avoid repetitive questioning on the same point.\n",
    "5. **Tool Usage:** You have access to several specialized tools. Use them judiciously based on the user's query:\n",
    "    * `code_analysis_agent`: Use this when the user provides code and asks for feedback.\n",
    "    * `code_explanation_agent`: Use this when the user asks for an explanation.\n",
    "    * `challenge_generator_agent`: Use this when the user wants a coding challenge.\n",
    "    * `mcq_agent`: Use this only for well-known topics like \"variables\", \"functions\", \"classes\", \"conditional statements\", \"comparisons\", or \"maximum of three numbers\".\n",
    "    * `llm_mcq_generator`: Use this for all other topics (e.g., \"Python decorators\", \"context managers\") or when a custom MCQ is needed.\n",
    "    * `mcq_answer_processor`: Use this when the user submits an answer to an active MCQ.\n",
    "6. **Maintain Context:** Keep track of the current topic and sub_topic.\n",
    "7. **Be Patient and Encouraging:** Foster a positive learning environment.\n",
    "8. **ReAct Architecture:** Before responding or calling a tool, always articulate your thought process. Start your response with \"Thought: [Your reasoning here]\". Then, proceed with your question or tool call. If you are calling a tool, the tool call should follow your thought. If you are directly asking a question, the question should follow your thought.\n",
    "9. **MCQ Answer Format:** When presenting an MCQ, instruct the user to respond with a single letter (A, B, C, or D) to indicate their answer choice.\n",
    "\n",
    "Current difficulty level: {difficulty_level}\n",
    "Current topic: {topic}\n",
    "Current sub_topic: {sub_topic}\n",
    "User struggle count: {user_struggle_count}\n",
    "MCQ active: {mcq_active}\n",
    "MCQ Question (internal): {mcq_question}\n",
    "MCQ Options (internal): {mcq_options}\n",
    "MCQ Correct Answer (internal): {mcq_correct_answer}\n",
    "\n",
    "Begin the conversation by asking the user what Python topic they'd like to learn or practice, or if they'd like to test their knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Chat prompt template for the Socratic LLM, including system prompt and message history.\n",
    "socratic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", socratic_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Define Tools ---\n",
    "# These tools simulate external functionalities that the Socratic LLM can call.\n",
    "\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the provided Python code.\n",
    "    This is a simulated tool. In a real application, it would run static analysis, linters, etc.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Analysis: Your code snippet '{code}' looks interesting. What were you trying to achieve with this code?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Explains a given Python concept.\n",
    "    This is a simulated tool. In a real application, it would provide detailed explanations.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Code Explanation: Ah, you're curious about '{concept}'. Can you tell me what you already know or suspect about it?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a Python coding challenge based on a topic and difficulty level.\n",
    "    This is a simulated tool. In a real application, it would generate a specific coding problem.\n",
    "    \"\"\"\n",
    "    return f\"Simulated Challenge for '{topic}': 'Write a function that sums even numbers in a list.' How would you start?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level\n",
    "    from a predefined list. The output will be a JSON string containing the question,\n",
    "    options, and correct answer. The 'question' field will be pre-formatted to include\n",
    "    options for direct display.\n",
    "    This tool is called when the Socratic agent decides to test understanding via MCQ\n",
    "    and a predefined question is available for the topic.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check if the exact topic exists in our predefined list (case-insensitive)\n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        # Format the question to include options for direct display in chat\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        # If topic not found, return a special string to indicate that the LLM should\n",
    "        # consider using the `llm_mcq_generator` tool instead.\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates an MCQ using an LLM based on a topic and difficulty level.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Python tutor who generates multiple choice questions (MCQs) for practice.\n",
    "Generate an MCQ on the topic \"{topic}\" at a \"{difficulty}\" level.\n",
    "\n",
    "The MCQ must follow this format strictly as a JSON object:\n",
    "{{\n",
    "    \"question\": \"string\",\n",
    "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "    \"answer_index\": 1,\n",
    "    \"explanation\": \"string\"\n",
    "}}\n",
    "\n",
    "For questions involving code snippets, format the code within triple backticks (```) to preserve readability, and ensure all strings are JSON-compatible (newlines escaped as \\\\n).\n",
    "DO NOT include outer markdown code fences like ```json or ```python\n",
    "Respond with raw valid JSON only. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    raw_llm_content = llm_response.content.strip()\n",
    "    print(\"[DEBUG] Raw LLM MCQ Response:\", repr(raw_llm_content))\n",
    "\n",
    "    # Strip outer markdown fences\n",
    "    cleaned_content = re.sub(r'^```(json|python)?\\n?', '', raw_llm_content, flags=re.MULTILINE)\n",
    "    cleaned_content = re.sub(r'\\n?```$', '', cleaned_content, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace problematic characters, but preserve code formatting\n",
    "    cleaned_content = (\n",
    "        cleaned_content\n",
    "        .replace('“', '\"').replace('”', '\"')  # Replace smart quotes\n",
    "        .replace('‘', \"'\").replace('’', \"'\")  # Replace smart single quotes\n",
    "        .replace('\\u201c', '\"').replace('\\u201d', '\"')  # Replace Unicode quotes\n",
    "        .replace('\\u2018', \"'\").replace('\\u2019', \"'\")  # Replace Unicode single quotes\n",
    "        .replace('\\t', '    ')  # Replace tabs with spaces for code readability\n",
    "    )\n",
    "\n",
    "    print(\"[DEBUG] Cleaned LLM MCQ Response:\", repr(cleaned_content))\n",
    "\n",
    "    try:\n",
    "        mcq_data = json.loads(cleaned_content)\n",
    "        # Validate JSON structure\n",
    "        required_keys = {\"question\", \"options\", \"answer_index\", \"explanation\"}\n",
    "        if not all(key in mcq_data for key in required_keys):\n",
    "            raise ValueError(\"Invalid MCQ format: Missing required keys\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"Invalid MCQ format: Options must be a list of 4 strings\")\n",
    "        if not isinstance(mcq_data[\"answer_index\"], int) or mcq_data[\"answer_index\"] not in [0, 1, 2, 3]:\n",
    "            raise ValueError(\"Invalid MCQ format: answer_index must be an integer between 0 and 3\")\n",
    "        \n",
    "        # Format question for display with options\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data[\"options\"])\n",
    "        mcq_data[\"question\"] = formatted_question\n",
    "        return mcq_data\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(\"[ERROR] JSON parsing or validation failed:\", str(e))\n",
    "        print(\"[ERROR] Cleaned content:\", repr(cleaned_content))\n",
    "        default_mcq = {\n",
    "            \"question\": \"**What is a Python decorator?**\\n\\n\" + \n",
    "                        \"\\n\".join([\n",
    "                            \"A) A function that modifies another function or method\",\n",
    "                            \"B) A type of class inheritance\",\n",
    "                            \"C) A syntax for defining variables\",\n",
    "                            \"D) A loop construct\"\n",
    "                        ]),\n",
    "            \"options\": [\n",
    "                \"A) A function that modifies another function or method\",\n",
    "                \"B) A type of class inheritance\",\n",
    "                \"C) A syntax for defining variables\",\n",
    "                \"D) A loop construct\"\n",
    "            ],\n",
    "            \"answer_index\": 0,\n",
    "            \"explanation\": \"A Python decorator is a function that wraps another function or method to extend or modify its behavior.\"\n",
    "        }\n",
    "        return {\"error\": f\"Failed to parse or validate MCQ JSON: {str(e)}\", **default_mcq}\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str, explanation: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ.\n",
    "    Compares the user's answer with the correct answer and returns feedback with explanation.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return f\"Correct!\\n\\nExplanation: {explanation}\"\n",
    "    else:\n",
    "        return f\"Incorrect.\\n\\nExplanation: {explanation}\"\n",
    "\n",
    "\n",
    "# List of all tools available to the Socratic LLM\n",
    "tools = [code_analysis_agent, code_explanation_agent, challenge_generator_agent, mcq_agent, llm_mcq_generator, mcq_answer_processor]\n",
    "# Bind the tools to the main Socratic LLM, allowing it to call them.\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "# Combine the prompt and the LLM with tools into a runnable for the Socratic agent.\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "# --- 3. Define the Graph Nodes ---\n",
    "\n",
    "def call_llm(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Invokes the Socratic LLM with the current conversation history and state.\n",
    "    This node represents the core logic of the Socratic tutoring agent.\n",
    "    It extracts the LLM's \"thought\" and formats the content for display.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering call_llm node.\")\n",
    "    print(\"[DEBUG] Messages sent to LLM:\", state[\"messages\"])\n",
    "    \n",
    "    try:\n",
    "        response = socratic_agent_runnable.invoke({\n",
    "            \"messages\": state[\"messages\"],\n",
    "            **{k: v for k, v in state.items() if k not in ['messages', 'next_node_decision']}\n",
    "        })\n",
    "        print(\"[DEBUG] LLM Response:\", response)\n",
    "        \n",
    "        content = response.content\n",
    "        if isinstance(content, list):\n",
    "            content = \"\\n\".join(str(item) for item in content)\n",
    "        elif not isinstance(content, str):\n",
    "            print(\"[ERROR] Unexpected content type:\", type(content))\n",
    "            content = \"\"\n",
    "\n",
    "        thought = \"\"\n",
    "        display_content = \"\"\n",
    "        if content and content.startswith(\"Thought:\"):\n",
    "            parts = content.split(\"Thought:\", 1)\n",
    "            thought_and_content = parts[1].strip()\n",
    "            thought_lines = thought_and_content.split('\\n', 1)\n",
    "            thought = thought_lines[0].strip()\n",
    "            display_content = thought_lines[1].strip() if len(thought_lines) > 1 else \"\"\n",
    "        else:\n",
    "            display_content = content\n",
    "\n",
    "        new_ai_message = AIMessage(\n",
    "            content=display_content,\n",
    "            tool_calls=response.tool_calls\n",
    "        )\n",
    "\n",
    "        return {\"messages\": [new_ai_message], \"agent_thought\": thought}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] LLM invocation failed\", str(e))\n",
    "\n",
    "# A dictionary mapping tool names to their corresponding Python functions.\n",
    "TOOLS_USED = {\n",
    "    \"code_analysis_agent\": code_analysis_agent,\n",
    "    \"code_explanation_agent\": code_explanation_agent,\n",
    "    \"challenge_generator_agent\": challenge_generator_agent,\n",
    "    \"mcq_agent\": mcq_agent,\n",
    "    \"llm_mcq_generator\": llm_mcq_generator,\n",
    "    \"mcq_answer_processor\": mcq_answer_processor,\n",
    "}\n",
    "\n",
    "def call_tool(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering call_tool node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    messages_to_add = []\n",
    "    state_updates = {}\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call[\"args\"]\n",
    "            tool_function = TOOLS_USED.get(tool_name)\n",
    "            \n",
    "            if tool_function:\n",
    "                response = tool_function.invoke(tool_args)\n",
    "                tool_output_content = str(response)\n",
    "\n",
    "                existing_tool_message = any(\n",
    "                    isinstance(msg, ToolMessage) and msg.tool_call_id == tool_call[\"id\"]\n",
    "                    for msg in state[\"messages\"]\n",
    "                )\n",
    "                \n",
    "                if not existing_tool_message:\n",
    "                    messages_to_add.append(\n",
    "                        ToolMessage(content=tool_output_content, tool_call_id=tool_call[\"id\"])\n",
    "                    )\n",
    "\n",
    "                if tool_name in [\"mcq_agent\", \"llm_mcq_generator\"]:\n",
    "                    if tool_output_content == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"No predefined MCQ found for this topic. Generating a new one...\")\n",
    "                        )\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        mcq_data = json.loads(tool_output_content) if isinstance(response, str) else response\n",
    "                        if \"error\" in mcq_data:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                        else:\n",
    "                            formatted_question = f\"{mcq_data['question']}\\n\\nOptions:\\n\" + \"\\n\".join(\n",
    "                                f\"{chr(65 + i)}. {opt}\" for i, opt in enumerate(mcq_data.get(\"options\", []))\n",
    "                            ) + \"\\n\\nPlease select an option (A, B, C, or D).\"\n",
    "                            messages_to_add.append(\n",
    "                                AIMessage(content=formatted_question)\n",
    "                            )\n",
    "                            state_updates[\"mcq_active\"] = True\n",
    "                            state_updates[\"mcq_question\"] = mcq_data.get(\"question\", \"\").split(\"```\")[0].strip()  # Store only question text\n",
    "                            state_updates[\"mcq_options\"] = mcq_data.get(\"options\", [])\n",
    "                            answer_index = mcq_data.get(\"answer_index\")\n",
    "                            correct_answer = chr(65 + answer_index) if isinstance(answer_index, int) and 0 <= answer_index <= 3 else \"\"\n",
    "                            state_updates[\"mcq_correct_answer\"] = correct_answer\n",
    "                            state_updates[\"mcq_explanation\"] = mcq_data.get(\"explanation\", \"\")\n",
    "                            if not state.get(\"topic\") and tool_args.get(\"topic\"):\n",
    "                                state_updates[\"topic\"] = tool_args[\"topic\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        messages_to_add.append(\n",
    "                            AIMessage(content=\"Error: MCQ agent returned invalid JSON. Please try again.\")\n",
    "                        )\n",
    "                        continue\n",
    "                elif tool_name == \"mcq_answer_processor\":\n",
    "                    if \"Correct!\" in tool_output_content:\n",
    "                        state_updates[\"user_struggle_count\"] = 0\n",
    "                    else:\n",
    "                        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "                    state_updates[\"mcq_active\"] = False\n",
    "                    state_updates[\"mcq_question\"] = \"\"\n",
    "                    state_updates[\"mcq_options\"] = []\n",
    "                    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "                    state_updates[\"mcq_explanation\"] = \"\"\n",
    "            else:\n",
    "                messages_to_add.append(\n",
    "                    ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
    "                )\n",
    "    print(f\"[DEBUG] call_tool: State updates - {state_updates}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "\n",
    "def router(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    The supervisor node that decides the next action based on the current state and user input.\n",
    "    It primarily routes between processing an MCQ answer directly or letting the Socratic LLM respond.\n",
    "    \n",
    "    Returns a dictionary containing 'next_node_decision' to control graph flow.\n",
    "    \"\"\"\n",
    "    print(\"[DEBUG] Entering router node.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    next_decision = \"\"\n",
    "    if state.get(\"mcq_active\", False) and isinstance(last_message, HumanMessage):\n",
    "        user_input = last_message.content.strip().upper()\n",
    "        if re.match(r\"^[ABCD](\\.|\\))?$\", user_input):\n",
    "            print(\"[DEBUG] Router: Detected MCQ answer, routing to process_mcq_answer.\")\n",
    "            next_decision = \"process_mcq_answer\"\n",
    "        else:\n",
    "            print(\"[DEBUG] Router: MCQ active but invalid answer, prompting for valid input.\")\n",
    "            return {\n",
    "                \"next_node_decision\": \"call_llm\",\n",
    "                \"messages\": [AIMessage(content=\"Please respond with a single letter (A, B, C, or D) to select your answer.\")]\n",
    "            }\n",
    "    else:\n",
    "        print(\"[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\")\n",
    "        next_decision = \"call_llm\"\n",
    "    \n",
    "    return {\"next_node_decision\": next_decision}\n",
    "\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering process_mcq_answer node.\")\n",
    "    last_human_message = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_human_message = msg\n",
    "            break\n",
    "    \n",
    "    if not last_human_message:\n",
    "        print(\"[ERROR] process_mcq_answer: Could not find a HumanMessage to process.\")\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    user_answer = last_human_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    explanation = state.get(\"mcq_explanation\", \"No explanation available.\")\n",
    "    \n",
    "    tool_output_content = mcq_answer_processor.invoke({\n",
    "        \"user_answer\": user_answer,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "    \n",
    "    state_updates = {}\n",
    "    messages_to_add = []\n",
    "\n",
    "    if \"Correct!\" in tool_output_content:\n",
    "        state_updates[\"user_struggle_count\"] = 0\n",
    "    else:\n",
    "        state_updates[\"user_struggle_count\"] = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    state_updates[\"mcq_active\"] = False\n",
    "    state_updates[\"mcq_question\"] = \"\"\n",
    "    state_updates[\"mcq_options\"] = []\n",
    "    state_updates[\"mcq_correct_answer\"] = \"\"\n",
    "    state_updates[\"mcq_explanation\"] = \"\"\n",
    "\n",
    "    messages_to_add.append(\n",
    "        ToolMessage(content=tool_output_content, tool_call_id=\"mcq_processor_direct_call\")\n",
    "    )\n",
    "    messages_to_add.append(\n",
    "        AIMessage(content=f\"You answered: {user_answer}. The correct answer was {correct_answer}. {tool_output_content}\")\n",
    "    )\n",
    "\n",
    "    print(f\"[DEBUG] process_mcq_answer: Result - {tool_output_content}, New struggle count: {state_updates.get('user_struggle_count')}\")\n",
    "    return {\"messages\": messages_to_add, **state_updates}\n",
    "\n",
    "# --- 4. Define the Graph Edges ---\n",
    "\n",
    "def should_continue_socratic(state: SocraticAgentState):\n",
    "    print(\"[DEBUG] Entering should_continue_socratic edge logic.\")\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        print(\"[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\")\n",
    "        return \"call_tool\"\n",
    "    elif state.get(\"mcq_active\", False) and isinstance(last_message, AIMessage) and \"Please select an option (A, B, C, or D)\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\")\n",
    "        return \"END\"\n",
    "    elif isinstance(last_message, AIMessage) and \"An issue occurred\" in last_message.content:\n",
    "        print(\"[DEBUG] should_continue_socratic: Error message detected, ending to wait for user input.\")\n",
    "        return \"END\"\n",
    "    else:\n",
    "        print(\"[DEBUG] should_continue_socratic: Routing to call_llm for further interaction.\")\n",
    "        return \"call_llm\"\n",
    "\n",
    "# --- 5. Build the LangGraph ---\n",
    "\n",
    "# Initialize the StateGraph with our defined state.\n",
    "workflow = StateGraph(SocraticAgentState)\n",
    "\n",
    "# Add all the nodes to the workflow.\n",
    "workflow.add_node(\"router\", router) # The new supervisor node.\n",
    "workflow.add_node(\"call_llm\", call_llm) # The Socratic agent's LLM logic.\n",
    "workflow.add_node(\"call_tool\", call_tool) # The tool execution logic.\n",
    "workflow.add_node(\"process_mcq_answer\", process_mcq_answer) # Node for direct MCQ answer processing.\n",
    "\n",
    "# Set the `router` node as the starting point of the graph.\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "# Define conditional edges from the `router` node.\n",
    "# The `router` function itself determines the next node based on the state.\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_node_decision\"], # Use the decision stored in state by the router function.\n",
    "    {\n",
    "        \"call_llm\": \"call_llm\",         # If router decides, go to the Socratic LLM.\n",
    "        \"process_mcq_answer\": \"process_mcq_answer\" # If router detects MCQ answer, go to process it.\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define conditional edges from the `call_llm` node (Socratic agent's LLM).\n",
    "# `should_continue_socratic` determines if a tool needs to be called or if the turn ends.\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    should_continue_socratic,\n",
    "    {\"call_tool\": \"call_tool\", \"END\": END, \"call_llm\": \"call_llm\"} # If tool calls, go to `call_tool`; otherwise, end.\n",
    ")\n",
    "\n",
    "# Define a regular edge from `call_tool` back to `call_llm`.\n",
    "# After a tool is executed, the Socratic LLM needs to process the tool's output and generate a response.\n",
    "workflow.add_edge(\"call_tool\", \"call_llm\")\n",
    "\n",
    "# Define a regular edge from `process_mcq_answer` back to `call_llm`.\n",
    "# After an MCQ answer is processed, the Socratic LLM needs to provide feedback and potentially a new question.\n",
    "workflow.add_edge(\"process_mcq_answer\", \"call_llm\")\n",
    "\n",
    "# Compile the workflow into a runnable graph.\n",
    "socratic_graph = workflow.compile(checkpointer=memory_saver)\n",
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "thread_id = \"user_session_123\"\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"mcq_explanation\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\"\n",
    "}\n",
    "\n",
    "# Invoke the graph with the thread_id to persist state\n",
    "result = socratic_graph.invoke(llm_mcq_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f378f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\n",
      "[DEBUG] Entering router node.\n",
      "[DEBUG] Router: No MCQ active, routing to call_llm (Socratic Agent).\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='75c0607f-9423-49c1-88ff-e7b48cac15e3')]\n",
      "[DEBUG] LLM Response: content='' additional_kwargs={'function_call': {'name': 'llm_mcq_generator', 'arguments': '{\"topic\": \"Python decorators\", \"difficulty\": \"intermediate\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--07d1f8b7-d6c4-4cfd-b268-cbbe4c534136-0' tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '2fe15ace-05f1-49bc-8728-5957249c2576', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1139, 'output_tokens': 25, 'total_tokens': 1234, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 70}}\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: Socratic agent has tool calls, routing to call_tool.\n",
      "[DEBUG] Entering call_tool node.\n",
      "[DEBUG] Raw LLM MCQ Response: '```json\\n{\\n    \"question\": \"What is the primary purpose of using `functools.wraps` when creating decorators in Python?\",\\n    \"options\": [\\n        \"To enable the decorator to accept arguments.\",\\n        \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\",\\n        \"To automatically apply `try-except` blocks around the decorated function for error handling.\",\\n        \"To convert a function-based decorator into a class-based decorator.\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\"\\n}\\n```'\n",
      "[DEBUG] Cleaned LLM MCQ Response: '{\\n    \"question\": \"What is the primary purpose of using `functools.wraps` when creating decorators in Python?\",\\n    \"options\": [\\n        \"To enable the decorator to accept arguments.\",\\n        \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\",\\n        \"To automatically apply `try-except` blocks around the decorated function for error handling.\",\\n        \"To convert a function-based decorator into a class-based decorator.\"\\n    ],\\n    \"answer_index\": 1,\\n    \"explanation\": \"When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\"\\n}\\n'\n",
      "[DEBUG] call_tool: State updates - {'mcq_active': True, 'mcq_question': \"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\n\\nTo enable the decorator to accept arguments.\\nTo preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\nTo convert a function-based decorator into a class-based decorator.\", 'mcq_options': ['To enable the decorator to accept arguments.', \"To preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\", 'To automatically apply `try-except` blocks around the decorated function for error handling.', 'To convert a function-based decorator into a class-based decorator.'], 'mcq_correct_answer': 'B', 'mcq_explanation': 'When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.', 'topic': 'Python decorators'}\n",
      "[DEBUG] Entering call_llm node.\n",
      "[DEBUG] Messages sent to LLM: [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='75c0607f-9423-49c1-88ff-e7b48cac15e3'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='eb5f266f-0bff-4b3f-bacf-53bb048eadf9', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '2fe15ace-05f1-49bc-8728-5957249c2576', 'type': 'tool_call'}]), ToolMessage(content='{\\'question\\': \"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\\\n\\\\nTo enable the decorator to accept arguments.\\\\nTo preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\\\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\\\nTo convert a function-based decorator into a class-based decorator.\", \\'options\\': [\\'To enable the decorator to accept arguments.\\', \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\", \\'To automatically apply `try-except` blocks around the decorated function for error handling.\\', \\'To convert a function-based decorator into a class-based decorator.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\\'}', id='98f679ba-44fb-4f3e-bd7b-bcb203759739', tool_call_id='2fe15ace-05f1-49bc-8728-5957249c2576'), AIMessage(content=\"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\n\\nTo enable the decorator to accept arguments.\\nTo preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\nTo convert a function-based decorator into a class-based decorator.\\n\\nOptions:\\nA. To enable the decorator to accept arguments.\\nB. To preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nC. To automatically apply `try-except` blocks around the decorated function for error handling.\\nD. To convert a function-based decorator into a class-based decorator.\\n\\nPlease select an option (A, B, C, or D).\", additional_kwargs={}, response_metadata={}, id='6dd098b0-d70a-44dd-b0ca-decdbbdd558a')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] LLM invocation failed 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting\n",
      "[DEBUG] Entering should_continue_socratic edge logic.\n",
      "[DEBUG] should_continue_socratic: MCQ presented, waiting for user answer.\n",
      "{'agent_thought': '',\n",
      " 'difficulty_level': 'intermediate',\n",
      " 'mcq_active': True,\n",
      " 'mcq_correct_answer': 'B',\n",
      " 'mcq_explanation': 'When you use a decorator without `functools.wraps`, the '\n",
      "                    'decorated function is replaced by the wrapper function '\n",
      "                    'returned by the decorator. This means that introspection '\n",
      "                    'tools (like `help()` or accessing attributes like '\n",
      "                    '`__name__` or `__doc__`) would report details of the '\n",
      "                    'wrapper function, not the original function. '\n",
      "                    '`functools.wraps` is a decorator itself that copies the '\n",
      "                    'name, docstring, module, and other attributes from the '\n",
      "                    'original function to the wrapper function, making the '\n",
      "                    'decorated function behave more like the original for '\n",
      "                    'debugging, documentation, and other introspection '\n",
      "                    'purposes. It does not relate to accepting decorator '\n",
      "                    'arguments, error handling, or converting decorator types.',\n",
      " 'mcq_options': ['To enable the decorator to accept arguments.',\n",
      "                 \"To preserve the original function's metadata (like \"\n",
      "                 '`__name__`, `__doc__`, `__module__`).',\n",
      "                 'To automatically apply `try-except` blocks around the '\n",
      "                 'decorated function for error handling.',\n",
      "                 'To convert a function-based decorator into a class-based '\n",
      "                 'decorator.'],\n",
      " 'mcq_question': '**What is the primary purpose of using `functools.wraps` '\n",
      "                 'when creating decorators in Python?**\\n'\n",
      "                 '\\n'\n",
      "                 'To enable the decorator to accept arguments.\\n'\n",
      "                 \"To preserve the original function's metadata (like \"\n",
      "                 '`__name__`, `__doc__`, `__module__`).\\n'\n",
      "                 'To automatically apply `try-except` blocks around the '\n",
      "                 'decorated function for error handling.\\n'\n",
      "                 'To convert a function-based decorator into a class-based '\n",
      "                 'decorator.',\n",
      " 'messages': [HumanMessage(content='Can you give me an MCQ on Python decorators?', additional_kwargs={}, response_metadata={}, id='75c0607f-9423-49c1-88ff-e7b48cac15e3'),\n",
      "              AIMessage(content='', additional_kwargs={}, response_metadata={}, id='eb5f266f-0bff-4b3f-bacf-53bb048eadf9', tool_calls=[{'name': 'llm_mcq_generator', 'args': {'topic': 'Python decorators', 'difficulty': 'intermediate'}, 'id': '2fe15ace-05f1-49bc-8728-5957249c2576', 'type': 'tool_call'}]),\n",
      "              ToolMessage(content='{\\'question\\': \"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\\\n\\\\nTo enable the decorator to accept arguments.\\\\nTo preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\\\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\\\nTo convert a function-based decorator into a class-based decorator.\", \\'options\\': [\\'To enable the decorator to accept arguments.\\', \"To preserve the original function\\'s metadata (like `__name__`, `__doc__`, `__module__`).\", \\'To automatically apply `try-except` blocks around the decorated function for error handling.\\', \\'To convert a function-based decorator into a class-based decorator.\\'], \\'answer_index\\': 1, \\'explanation\\': \\'When you use a decorator without `functools.wraps`, the decorated function is replaced by the wrapper function returned by the decorator. This means that introspection tools (like `help()` or accessing attributes like `__name__` or `__doc__`) would report details of the wrapper function, not the original function. `functools.wraps` is a decorator itself that copies the name, docstring, module, and other attributes from the original function to the wrapper function, making the decorated function behave more like the original for debugging, documentation, and other introspection purposes. It does not relate to accepting decorator arguments, error handling, or converting decorator types.\\'}', id='98f679ba-44fb-4f3e-bd7b-bcb203759739', tool_call_id='2fe15ace-05f1-49bc-8728-5957249c2576'),\n",
      "              AIMessage(content=\"**What is the primary purpose of using `functools.wraps` when creating decorators in Python?**\\n\\nTo enable the decorator to accept arguments.\\nTo preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nTo automatically apply `try-except` blocks around the decorated function for error handling.\\nTo convert a function-based decorator into a class-based decorator.\\n\\nOptions:\\nA. To enable the decorator to accept arguments.\\nB. To preserve the original function's metadata (like `__name__`, `__doc__`, `__module__`).\\nC. To automatically apply `try-except` blocks around the decorated function for error handling.\\nD. To convert a function-based decorator into a class-based decorator.\\n\\nPlease select an option (A, B, C, or D).\", additional_kwargs={}, response_metadata={}, id='6dd098b0-d70a-44dd-b0ca-decdbbdd558a')],\n",
      " 'next_node_decision': 'call_llm',\n",
      " 'sub_topic': '',\n",
      " 'topic': 'Python decorators',\n",
      " 'user_struggle_count': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example test for the new LLM-generated MCQ\n",
    "thread_id = \"user_session_123\"\n",
    "print(\"\\n--- Testing LLM-generated MCQ for an unknown topic (decorators) ---\")\n",
    "llm_mcq_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Can you give me an MCQ on Python decorators?\")],\n",
    "    \"difficulty_level\": \"intermediate\",\n",
    "    \"user_struggle_count\": 0,\n",
    "    \"topic\": \"\",\n",
    "    \"sub_topic\": \"\",\n",
    "    \"mcq_active\": False,\n",
    "    \"mcq_question\": \"\",\n",
    "    \"mcq_options\": [],\n",
    "    \"mcq_correct_answer\": \"\",\n",
    "    \"mcq_explanation\": \"\",\n",
    "    \"agent_thought\": \"\",\n",
    "    \"next_node_decision\": \"\"\n",
    "}\n",
    "\n",
    "# Invoke the graph with the thread_id to persist state\n",
    "result = socratic_graph.invoke(llm_mcq_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "import pprint\n",
    "pprint.pprint(result)\n",
    "\n",
    "# # Simulate a follow-up interaction to demonstrate memory\n",
    "# follow_up_state = {\n",
    "#     \"messages\": [HumanMessage(content=\"B\")],  # User responds to the MCQ\n",
    "#     \"difficulty_level\": result[\"difficulty_level\"],\n",
    "#     \"user_struggle_count\": result[\"user_struggle_count\"],\n",
    "#     \"topic\": result[\"topic\"],\n",
    "#     \"sub_topic\": result[\"sub_topic\"],\n",
    "#     \"mcq_active\": result[\"mcq_active\"],\n",
    "#     \"mcq_question\": result[\"mcq_question\"],\n",
    "#     \"mcq_options\": result[\"mcq_options\"],\n",
    "#     \"mcq_correct_answer\": result[\"mcq_correct_answer\"],\n",
    "#     \"mcq_explanation\": result[\"mcq_explanation\"],\n",
    "#     \"agent_thought\": result[\"agent_thought\"],\n",
    "#     \"next_node_decision\": \"\"\n",
    "# }\n",
    "\n",
    "# # Invoke the graph again with the same thread_id to retrieve the previous state\n",
    "# result_follow_up = socratic_graph.invoke(follow_up_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
    "# pprint.pprint(result_follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be33bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e50563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7132acc416a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplified Socratic Bot Logic\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, TypedDict, Annotated, Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Simplified State ---\n",
    "class SocraticState(TypedDict):\n",
    "    \"\"\"Simplified state for the Socratic tutor\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    topic: str\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    current_mcq: Optional[Dict]  # None or {\"question\": str, \"options\": List[str], \"correct_answer\": str, \"explanation\": str}\n",
    "\n",
    "# --- Initialize LLM ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", max_retries=3, temperature=0.7)\n",
    "\n",
    "# --- Predefined MCQs ---\n",
    "PREDEFINED_MCQS = {\n",
    "    \"variables\": {\n",
    "        \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "        \"options\": [\"Tuple\", \"String\", \"List\", \"Integer\"],\n",
    "        \"correct_answer\": \"C\",\n",
    "        \"explanation\": \"Lists are mutable in Python, meaning you can change their contents after creation.\"\n",
    "    },\n",
    "    \"functions\": {\n",
    "        \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "        \"options\": [\"func\", \"define\", \"def\", \"function\"],\n",
    "        \"correct_answer\": \"C\",\n",
    "        \"explanation\": \"The 'def' keyword is used to define functions in Python.\"\n",
    "    },\n",
    "    \"classes\": {\n",
    "        \"question\": \"What is the primary purpose of the __init__ method in a class?\",\n",
    "        \"options\": [\n",
    "            \"To destroy an object when it's no longer needed\",\n",
    "            \"To define static methods\",\n",
    "            \"To initialize the attributes of an object when it's created\",\n",
    "            \"To define the string representation of an object\"\n",
    "        ],\n",
    "        \"correct_answer\": \"C\",\n",
    "        \"explanation\": \"The __init__ method is called when an object is created and is used to initialize its attributes.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- System Prompt ---\n",
    "SYSTEM_PROMPT = \"\"\"You are a Socratic Python programming tutor. Your core principle is to guide students to discover answers through thoughtful questions rather than giving direct answers.\n",
    "\n",
    "**Your approach:**\n",
    "1. Ask guiding questions to help students think through problems\n",
    "2. Break complex topics into smaller, manageable concepts\n",
    "3. Encourage experimentation and exploration\n",
    "4. Adapt your questioning based on student responses\n",
    "5. Be patient and encouraging\n",
    "\n",
    "**Current context:**\n",
    "- Topic: {topic}\n",
    "- Difficulty: {difficulty_level}\n",
    "- Student struggles: {user_struggle_count}\n",
    "- Active MCQ: {has_mcq}\n",
    "\n",
    "**Guidelines:**\n",
    "- Always respond with questions unless providing feedback\n",
    "- If a student struggles (wrong answers, confusion), simplify your approach\n",
    "- You can offer MCQs to test understanding\n",
    "- Keep responses concise and focused\n",
    "- Encourage hands-on practice\n",
    "\n",
    "Start by asking what Python topic they'd like to explore or if they want to test their knowledge.\"\"\"\n",
    "\n",
    "# --- Core Functions ---\n",
    "def generate_mcq(topic: str, difficulty: str) -> Dict:\n",
    "    \"\"\"Generate an MCQ for the given topic\"\"\"\n",
    "    # First check predefined MCQs\n",
    "    if topic.lower() in PREDEFINED_MCQS:\n",
    "        return PREDEFINED_MCQS[topic.lower()]\n",
    "    \n",
    "    # Generate using LLM for other topics\n",
    "    prompt = f\"\"\"Generate a multiple choice question about {topic} at {difficulty} level.\n",
    "    \n",
    "    Return ONLY a JSON object with this exact structure:\n",
    "    {{\n",
    "        \"question\": \"Your question here\",\n",
    "        \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
    "        \"correct_answer\": \"C\",\n",
    "        \"explanation\": \"Brief explanation of the correct answer\"\n",
    "    }}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        content = response.content.strip()\n",
    "        \n",
    "        # Clean up the response\n",
    "        content = re.sub(r'^```json\\s*', '', content)\n",
    "        content = re.sub(r'\\s*```$', '', content)\n",
    "        \n",
    "        mcq_data = json.loads(content)\n",
    "        \n",
    "        # Validate structure\n",
    "        required_keys = [\"question\", \"options\", \"correct_answer\", \"explanation\"]\n",
    "        if all(key in mcq_data for key in required_keys):\n",
    "            return mcq_data\n",
    "        else:\n",
    "            raise ValueError(\"Invalid MCQ structure\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating MCQ: {e}\")\n",
    "        # Return a default MCQ\n",
    "        return {\n",
    "            \"question\": f\"What is an important concept in {topic}?\",\n",
    "            \"options\": [\"Syntax\", \"Logic\", \"Practice\", \"All of the above\"],\n",
    "            \"correct_answer\": \"D\",\n",
    "            \"explanation\": \"All aspects are important when learning programming concepts.\"\n",
    "        }\n",
    "\n",
    "def process_user_input(state: SocraticState) -> SocraticState:\n",
    "    \"\"\"Main function to process user input and generate response\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if not isinstance(last_message, HumanMessage):\n",
    "        return state\n",
    "    \n",
    "    user_input = last_message.content.strip()\n",
    "    \n",
    "    # Check if this is an MCQ answer\n",
    "    if state[\"current_mcq\"] and re.match(r'^[ABCD]$', user_input.upper()):\n",
    "        return handle_mcq_answer(state, user_input.upper())\n",
    "    \n",
    "    # Check if user is requesting an MCQ\n",
    "    if any(keyword in user_input.lower() for keyword in [\"mcq\", \"quiz\", \"test\", \"question\"]):\n",
    "        return handle_mcq_request(state)\n",
    "    \n",
    "    # Regular Socratic response\n",
    "    return generate_socratic_response(state)\n",
    "\n",
    "def handle_mcq_answer(state: SocraticState, user_answer: str) -> SocraticState:\n",
    "    \"\"\"Handle MCQ answer submission\"\"\"\n",
    "    mcq = state[\"current_mcq\"]\n",
    "    correct_answer = mcq[\"correct_answer\"]\n",
    "    \n",
    "    is_correct = user_answer == correct_answer\n",
    "    \n",
    "    if is_correct:\n",
    "        response = f\"Correct! 🎉\\n\\n{mcq['explanation']}\\n\\nWhat would you like to explore next?\"\n",
    "        state[\"user_struggle_count\"] = max(0, state[\"user_struggle_count\"] - 1)\n",
    "    else:\n",
    "        response = f\"Not quite. The correct answer is {correct_answer}.\\n\\n{mcq['explanation']}\\n\\nLet's break this down further. What part of this concept would you like to understand better?\"\n",
    "        state[\"user_struggle_count\"] += 1\n",
    "    \n",
    "    state[\"current_mcq\"] = None  # Clear the MCQ\n",
    "    state[\"messages\"].append(AIMessage(content=response))\n",
    "    \n",
    "    return state\n",
    "\n",
    "def handle_mcq_request(state: SocraticState) -> SocraticState:\n",
    "    \"\"\"Handle request for MCQ\"\"\"\n",
    "    topic = state[\"topic\"] or \"Python basics\"\n",
    "    difficulty = state[\"difficulty_level\"]\n",
    "    \n",
    "    mcq = generate_mcq(topic, difficulty)\n",
    "    state[\"current_mcq\"] = mcq\n",
    "    \n",
    "    # Format the MCQ for display\n",
    "    options_text = \"\\n\".join([f\"{chr(65+i)}) {option}\" for i, option in enumerate(mcq[\"options\"])])\n",
    "    mcq_text = f\"**{mcq['question']}**\\n\\n{options_text}\\n\\nPlease answer with A, B, C, or D.\"\n",
    "    \n",
    "    state[\"messages\"].append(AIMessage(content=mcq_text))\n",
    "    return state\n",
    "\n",
    "def generate_socratic_response(state: SocraticState) -> SocraticState:\n",
    "    \"\"\"Generate a Socratic response using the LLM\"\"\"\n",
    "    # Extract topic from conversation if not set\n",
    "    if not state[\"topic\"]:\n",
    "        last_human_msg = state[\"messages\"][-1].content\n",
    "        # Simple topic extraction - you could make this more sophisticated\n",
    "        if any(word in last_human_msg.lower() for word in [\"function\", \"def\"]):\n",
    "            state[\"topic\"] = \"functions\"\n",
    "        elif any(word in last_human_msg.lower() for word in [\"class\", \"object\"]):\n",
    "            state[\"topic\"] = \"classes\"\n",
    "        elif any(word in last_human_msg.lower() for word in [\"variable\", \"assignment\"]):\n",
    "            state[\"topic\"] = \"variables\"\n",
    "        else:\n",
    "            state[\"topic\"] = \"Python basics\"\n",
    "    \n",
    "    # Create prompt with current context\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = prompt.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"difficulty_level\": state[\"difficulty_level\"],\n",
    "        \"user_struggle_count\": state[\"user_struggle_count\"],\n",
    "        \"has_mcq\": state[\"current_mcq\"] is not None\n",
    "    })\n",
    "    \n",
    "    ai_response = llm.invoke(response.messages)\n",
    "    state[\"messages\"].append(AIMessage(content=ai_response.content))\n",
    "    \n",
    "    return state\n",
    "\n",
    "# --- Graph Setup ---\n",
    "def create_socratic_graph():\n",
    "    \"\"\"Create the simplified Socratic tutor graph\"\"\"\n",
    "    workflow = StateGraph(SocraticState)\n",
    "    \n",
    "    # Single node that handles all logic\n",
    "    workflow.add_node(\"process\", process_user_input)\n",
    "    \n",
    "    # Simple flow: process -> end\n",
    "    workflow.set_entry_point(\"process\")\n",
    "    workflow.add_edge(\"process\", END)\n",
    "    \n",
    "    # Compile with memory\n",
    "    memory_saver = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory_saver)\n",
    "\n",
    "# Create the graph instance\n",
    "socratic_graph = create_socratic_graph()\n",
    "\n",
    "socratic_graph\n",
    "# # --- Helper function for Streamlit ---\n",
    "# def run_socratic_session(user_input: str, thread_id: str, current_state: SocraticState = None) -> SocraticState:\n",
    "#     \"\"\"Run a single interaction with the Socratic tutor\"\"\"\n",
    "#     if current_state is None:\n",
    "#         current_state = SocraticState(\n",
    "#             messages=[],\n",
    "#             topic=\"\",\n",
    "#             difficulty_level=\"beginner\",\n",
    "#             user_struggle_count=0,\n",
    "#             current_mcq=None\n",
    "#         )\n",
    "    \n",
    "#     # Add user message\n",
    "#     current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "    \n",
    "#     # Process through graph\n",
    "#     result = socratic_graph.invoke(\n",
    "#         current_state,\n",
    "#         config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "#     )\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8dee076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7132ac323b30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# socratic_bot_logic_enhanced.py\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    next_action: str  # Added for supervisor routing\n",
    "\n",
    "# --- 2. Initialize the LLMs ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5)\n",
    "supervisor_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
    "\n",
    "# --- 3. Define Tools ---\n",
    "@tool\n",
    "def code_analysis_agent(code: str) -> str:\n",
    "    \"\"\"Analyzes the provided Python code and provides feedback.\"\"\"\n",
    "    return f\"Code Analysis: Your code '{code}' shows good structure. What specific aspect would you like to improve?\"\n",
    "\n",
    "@tool\n",
    "def code_explanation_agent(concept: str) -> str:\n",
    "    \"\"\"Explains a given Python concept using the Socratic method.\"\"\"\n",
    "    return f\"Let's explore '{concept}' together. What do you think this concept does based on its name?\"\n",
    "\n",
    "@tool\n",
    "def challenge_generator_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"Generates a Python coding challenge based on topic and difficulty.\"\"\"\n",
    "    challenges = {\n",
    "        \"variables\": \"Create a program that swaps two variables without using a third variable.\",\n",
    "        \"functions\": \"Write a function that calculates the factorial of a number.\",\n",
    "        \"loops\": \"Create a function that prints the Fibonacci sequence up to n terms.\",\n",
    "        \"classes\": \"Design a simple class to represent a bank account with deposit and withdraw methods.\"\n",
    "    }\n",
    "    challenge = challenges.get(topic.lower(), f\"Create a simple program demonstrating {topic}.\")\n",
    "    return f\"Challenge: {challenge} How would you approach this problem?\"\n",
    "\n",
    "@tool\n",
    "def mcq_agent(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a multiple-choice question (MCQ) on a given Python topic and difficulty level.\n",
    "    Returns a JSON string containing the question, options, and correct answer.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"class\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed.\",\n",
    "                \"B) To define static methods.\",\n",
    "                \"C) To initialize the attributes of an object when it's created.\",\n",
    "                \"D) To define the string representation of an object.\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditional statements\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"comparisons\": {\n",
    "            \"question\": \"What is the correct operator for 'not equal to' in Python?\",\n",
    "            \"options\": [\"A) ==\", \"B) !=\", \"C) <>\", \"D) =!\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        },\n",
    "        \"maximum of three numbers\": {\n",
    "            \"question\": \"Consider finding the maximum of three numbers (a, b, c). Which of these logical structures is typically used?\",\n",
    "            \"options\": [\n",
    "                \"A) A single 'for' loop\",\n",
    "                \"B) Nested 'if-else' statements or multiple 'if' statements with logical 'and'/'or'\",\n",
    "                \"C) A 'while' loop\",\n",
    "                \"D) A 'try-except' block\"\n",
    "            ],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    selected_mcq_raw = mcqs_raw.get(topic.lower())\n",
    "\n",
    "    if selected_mcq_raw:\n",
    "        formatted_question = f\"**{selected_mcq_raw['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(selected_mcq_raw['options'])\n",
    "\n",
    "        mcq_data = {\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq_raw['options'],\n",
    "            \"correct_answer\": selected_mcq_raw['correct_answer']\n",
    "        }\n",
    "        return json.dumps(mcq_data)\n",
    "    else:\n",
    "        return \"NO_PREDEFINED_MCQ_FOUND\"\n",
    "\n",
    "@tool\n",
    "def llm_mcq_generator(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a new multiple-choice question using LLM for topics not in predefined list.\n",
    "    \"\"\"\n",
    "    mcq_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are an expert Python programming instructor. Generate a challenging\n",
    "        but fair multiple-choice question (MCQ) on the given Python topic and difficulty level.\n",
    "        The MCQ should have exactly four options (A, B, C, D).\n",
    "        Provide the question, the four options, and the single correct answer in JSON format.\n",
    "        \n",
    "        Example Output:\n",
    "        {\n",
    "            \"question\": \"What is the purpose of a 'for' loop in Python?\",\n",
    "            \"options\": [\"A) To define a function.\", \"B) To iterate over a sequence.\", \"C) To handle exceptions.\", \"D) To create a new class.\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "        \"\"\"),\n",
    "        (\"user\", f\"Generate a Python MCQ about: {topic}. Difficulty: {difficulty}.\")\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        response = mcq_generation_llm.invoke(mcq_prompt_template.format(topic=topic, difficulty=difficulty))\n",
    "        mcq_data = json.loads(response.content)\n",
    "\n",
    "        if not all(k in mcq_data for k in [\"question\", \"options\", \"correct_answer\"]):\n",
    "            raise ValueError(\"MCQ JSON missing required keys.\")\n",
    "        if not isinstance(mcq_data[\"options\"], list) or len(mcq_data[\"options\"]) != 4:\n",
    "            raise ValueError(\"MCQ options must be a list of 4 items.\")\n",
    "        if mcq_data[\"correct_answer\"] not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            raise ValueError(\"Correct answer must be A, B, C, or D.\")\n",
    "\n",
    "        formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \\\n",
    "                             \"\\n\".join(mcq_data['options'])\n",
    "\n",
    "        return json.dumps({\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": mcq_data['options'],\n",
    "            \"correct_answer\": mcq_data['correct_answer']\n",
    "        })\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        return json.dumps({\"error\": \"Failed to decode JSON from LLM for MCQ generation.\"})\n",
    "    except ValueError as e:\n",
    "        return json.dumps({\"error\": f\"Invalid MCQ format generated by LLM: {e}\"})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"An unexpected error occurred during MCQ generation: {e}\"})\n",
    "\n",
    "@tool\n",
    "def mcq_answer_processor(user_answer: str, correct_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Processes the user's answer to an MCQ and provides feedback.\n",
    "    \"\"\"\n",
    "    is_correct = user_answer.strip().upper() == correct_answer.strip().upper()\n",
    "    if is_correct:\n",
    "        return \"Correct! Great job understanding the concept.\"\n",
    "    else:\n",
    "        return f\"Incorrect. The correct answer is {correct_answer}. Let's explore why.\"\n",
    "\n",
    "# --- 4. Supervisor System ---\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a supervisor for a Socratic Python tutoring system. Your role is to analyze the user's message \n",
    "and current state to determine the most appropriate action to take.\n",
    "\n",
    "Available actions:\n",
    "1. \"socratic_chat\" - For general Socratic questioning and discussion\n",
    "2. \"code_analysis\" - When user provides code for review\n",
    "3. \"code_explanation\" - When user asks for explanation of concepts\n",
    "4. \"challenge_generation\" - When user wants a coding challenge\n",
    "5. \"mcq_generation\" - When user wants a multiple choice question or when you detect they're struggling\n",
    "6. \"mcq_processing\" - When user is answering an active MCQ\n",
    "7. \"direct_response\" - For simple acknowledgments or when continuing conversation\n",
    "\n",
    "Consider the following factors:\n",
    "- User's current struggle count (higher = more support needed)\n",
    "- Whether an MCQ is currently active\n",
    "- The nature of the user's message (question, code, answer, etc.)\n",
    "- The current topic and difficulty level\n",
    "\n",
    "Current state:\n",
    "- Difficulty level: {difficulty_level}\n",
    "- Topic: {topic}\n",
    "- Sub-topic: {sub_topic}\n",
    "- User struggle count: {user_struggle_count}\n",
    "- MCQ active: {mcq_active}\n",
    "- MCQ question: {mcq_question}\n",
    "\n",
    "Respond with just the action name (e.g., \"socratic_chat\", \"code_analysis\", etc.)\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", supervisor_system_prompt),\n",
    "    (\"user\", \"User message: {user_message}\\n\\nWhat action should be taken?\")\n",
    "])\n",
    "\n",
    "def supervisor_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Supervisor node that determines the next action based on user input and current state.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    user_message = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
    "    \n",
    "    # Check if MCQ is active and user is providing an answer\n",
    "    if state.get(\"mcq_active\") and isinstance(last_message, HumanMessage):\n",
    "        # Check if it looks like an MCQ answer (A, B, C, or D)\n",
    "        if user_message.strip().upper() in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            return {\"next_action\": \"mcq_processing\"}\n",
    "    \n",
    "    # Use LLM to determine the best action\n",
    "    response = supervisor_llm.invoke(supervisor_prompt.format(\n",
    "        user_message=user_message,\n",
    "        difficulty_level=state.get(\"difficulty_level\", \"beginner\"),\n",
    "        topic=state.get(\"topic\", \"\"),\n",
    "        sub_topic=state.get(\"sub_topic\", \"\"),\n",
    "        user_struggle_count=state.get(\"user_struggle_count\", 0),\n",
    "        mcq_active=state.get(\"mcq_active\", False),\n",
    "        mcq_question=state.get(\"mcq_question\", \"\")\n",
    "    ))\n",
    "    \n",
    "    action = response.content.strip().lower()\n",
    "    \n",
    "    # Map the action to valid routing options\n",
    "    action_mapping = {\n",
    "        \"socratic_chat\": \"socratic_chat\",\n",
    "        \"code_analysis\": \"code_analysis\",\n",
    "        \"code_explanation\": \"code_explanation\", \n",
    "        \"challenge_generation\": \"challenge_generation\",\n",
    "        \"mcq_generation\": \"mcq_generation\",\n",
    "        \"mcq_processing\": \"mcq_processing\",\n",
    "        \"direct_response\": \"socratic_chat\"\n",
    "    }\n",
    "    \n",
    "    mapped_action = action_mapping.get(action, \"socratic_chat\")\n",
    "    print(f\"[DEBUG] Supervisor decided: {mapped_action}\")\n",
    "    \n",
    "    return {\"next_action\": mapped_action}\n",
    "\n",
    "# --- 5. Socratic Chat Node ---\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your goal is to guide the user to discover answers\n",
    "and understand concepts through thoughtful questions, rather than directly providing solutions.\n",
    "\n",
    "Core principles:\n",
    "1. Ask Questions: Always respond with a question unless providing feedback\n",
    "2. Socratic Method: Break down complex problems into smaller, manageable questions\n",
    "3. Encourage Exploration: Prompt the user to experiment, research, or think critically\n",
    "4. Adapt to Understanding: If user struggles, simplify questions or offer hints\n",
    "5. Maintain Context: Keep track of current topic and sub_topic\n",
    "6. Be Patient and Encouraging: Foster a positive learning environment\n",
    "\n",
    "Current state:\n",
    "- Difficulty level: {difficulty_level}\n",
    "- Topic: {topic}\n",
    "- Sub_topic: {sub_topic}\n",
    "- User struggle count: {user_struggle_count}\n",
    "- MCQ active: {mcq_active}\n",
    "\n",
    "Always start your response with \"Thought: [your reasoning]\" then provide your question or response.\n",
    "\"\"\"\n",
    "\n",
    "socratic_chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", socratic_system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "def socratic_chat_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Main Socratic chat node for asking questions and guiding learning.\n",
    "    \"\"\"\n",
    "    socratic_chain = socratic_chat_prompt | llm\n",
    "    \n",
    "    response = socratic_chain.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"difficulty_level\": state.get(\"difficulty_level\", \"beginner\"),\n",
    "        \"topic\": state.get(\"topic\", \"\"),\n",
    "        \"sub_topic\": state.get(\"sub_topic\", \"\"),\n",
    "        \"user_struggle_count\": state.get(\"user_struggle_count\", 0),\n",
    "        \"mcq_active\": state.get(\"mcq_active\", False)\n",
    "    })\n",
    "    \n",
    "    # Extract thought and display content\n",
    "    thought = \"\"\n",
    "    display_content = response.content\n",
    "    \n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            thought = parts[1].strip().split('\\n', 1)[0]\n",
    "            if '\\n' in parts[1]:\n",
    "                display_content = parts[1].split('\\n', 1)[1].strip()\n",
    "            else:\n",
    "                display_content = \"\"\n",
    "    \n",
    "    new_message = AIMessage(content=display_content)\n",
    "    return {\"messages\": [new_message], \"agent_thought\": thought}\n",
    "\n",
    "# --- 6. Tool-specific Nodes ---\n",
    "def code_analysis_node(state: SocraticAgentState):\n",
    "    \"\"\"Node for analyzing user's code.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    code_content = last_message.content\n",
    "    \n",
    "    result = code_analysis_agent.invoke({\"code\": code_content})\n",
    "    response_message = AIMessage(content=result)\n",
    "    \n",
    "    return {\"messages\": [response_message]}\n",
    "\n",
    "def code_explanation_node(state: SocraticAgentState):\n",
    "    \"\"\"Node for explaining concepts.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    concept = state.get(\"topic\", last_message.content)\n",
    "    \n",
    "    result = code_explanation_agent.invoke({\"concept\": concept})\n",
    "    response_message = AIMessage(content=result)\n",
    "    \n",
    "    return {\"messages\": [response_message]}\n",
    "\n",
    "def challenge_generation_node(state: SocraticAgentState):\n",
    "    \"\"\"Node for generating coding challenges.\"\"\"\n",
    "    topic = state.get(\"topic\", \"general\")\n",
    "    difficulty = state.get(\"difficulty_level\", \"beginner\")\n",
    "    \n",
    "    result = challenge_generator_agent.invoke({\"topic\": topic, \"difficulty\": difficulty})\n",
    "    response_message = AIMessage(content=result)\n",
    "    \n",
    "    return {\"messages\": [response_message]}\n",
    "\n",
    "def mcq_generation_node(state: SocraticAgentState):\n",
    "    \"\"\"Node for generating MCQs.\"\"\"\n",
    "    topic = state.get(\"topic\", \"variables\")\n",
    "    difficulty = state.get(\"difficulty_level\", \"beginner\")\n",
    "    \n",
    "    # Try predefined MCQ first\n",
    "    result = mcq_agent.invoke({\"topic\": topic, \"difficulty\": difficulty})\n",
    "    \n",
    "    if result == \"NO_PREDEFINED_MCQ_FOUND\":\n",
    "        # Use LLM to generate MCQ\n",
    "        result = llm_mcq_generator.invoke({\"topic\": topic, \"difficulty\": difficulty})\n",
    "    \n",
    "    try:\n",
    "        mcq_data = json.loads(result)\n",
    "        if \"error\" in mcq_data:\n",
    "            response_message = AIMessage(content=f\"Error generating MCQ: {mcq_data['error']}\")\n",
    "            return {\"messages\": [response_message]}\n",
    "        \n",
    "        # Update state with MCQ data\n",
    "        state_updates = {\n",
    "            \"mcq_active\": True,\n",
    "            \"mcq_question\": mcq_data.get(\"question\", \"\"),\n",
    "            \"mcq_options\": mcq_data.get(\"options\", []),\n",
    "            \"mcq_correct_answer\": mcq_data.get(\"correct_answer\", \"\"),\n",
    "            \"messages\": [AIMessage(content=mcq_data.get(\"question\", \"\"))]\n",
    "        }\n",
    "        \n",
    "        return state_updates\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        response_message = AIMessage(content=\"Error: Could not process MCQ data.\")\n",
    "        return {\"messages\": [response_message]}\n",
    "\n",
    "def mcq_processing_node(state: SocraticAgentState):\n",
    "    \"\"\"Node for processing MCQ answers.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    user_answer = last_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    \n",
    "    result = mcq_answer_processor.invoke({\"user_answer\": user_answer, \"correct_answer\": correct_answer})\n",
    "    \n",
    "    # Update struggle count based on result\n",
    "    is_correct = user_answer == correct_answer\n",
    "    new_struggle_count = 0 if is_correct else state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    response_message = AIMessage(content=result)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response_message],\n",
    "        \"user_struggle_count\": new_struggle_count,\n",
    "        \"mcq_active\": False,\n",
    "        \"mcq_question\": \"\",\n",
    "        \"mcq_options\": [],\n",
    "        \"mcq_correct_answer\": \"\"\n",
    "    }\n",
    "\n",
    "# --- 7. Routing Function ---\n",
    "def route_to_node(state: SocraticAgentState) -> Literal[\"socratic_chat\", \"code_analysis\", \"code_explanation\", \"challenge_generation\", \"mcq_generation\", \"mcq_processing\"]:\n",
    "    \"\"\"Routes to the appropriate node based on supervisor decision.\"\"\"\n",
    "    return state.get(\"next_action\", \"socratic_chat\")\n",
    "\n",
    "# --- 8. Build the Enhanced Graph ---\n",
    "def build_enhanced_socratic_graph():\n",
    "    \"\"\"Builds the enhanced Socratic graph with supervisor and memory.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(SocraticAgentState)\n",
    "    \n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"supervisor\", supervisor_node)\n",
    "    workflow.add_node(\"socratic_chat\", socratic_chat_node)\n",
    "    workflow.add_node(\"code_analysis\", code_analysis_node)\n",
    "    workflow.add_node(\"code_explanation\", code_explanation_node)\n",
    "    workflow.add_node(\"challenge_generation\", challenge_generation_node)\n",
    "    workflow.add_node(\"mcq_generation\", mcq_generation_node)\n",
    "    workflow.add_node(\"mcq_processing\", mcq_processing_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "    \n",
    "    # Add conditional edges from supervisor to appropriate nodes\n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        route_to_node,\n",
    "        {\n",
    "            \"socratic_chat\": \"socratic_chat\",\n",
    "            \"code_analysis\": \"code_analysis\",\n",
    "            \"code_explanation\": \"code_explanation\",\n",
    "            \"challenge_generation\": \"challenge_generation\",\n",
    "            \"mcq_generation\": \"mcq_generation\",\n",
    "            \"mcq_processing\": \"mcq_processing\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All nodes (except supervisor) end the conversation\n",
    "    workflow.add_edge(\"socratic_chat\", END)\n",
    "    workflow.add_edge(\"code_analysis\", END)\n",
    "    workflow.add_edge(\"code_explanation\", END)\n",
    "    workflow.add_edge(\"challenge_generation\", END)\n",
    "    workflow.add_edge(\"mcq_generation\", END)\n",
    "    workflow.add_edge(\"mcq_processing\", END)\n",
    "    \n",
    "    # Add memory saver\n",
    "    memory = MemorySaver()\n",
    "    \n",
    "    # Compile the graph with memory\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "# --- 9. Initialize the Enhanced Graph ---\n",
    "enhanced_socratic_graph = build_enhanced_socratic_graph()\n",
    "\n",
    "enhanced_socratic_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d5cf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7132cc10cb90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# socratic_bot_logic_enhanced.py\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    interaction_mode: str  # 'general', 'code_review', 'concept_exploration', 'challenge', 'mcq'\n",
    "    context_data: dict  # Store relevant context like code, concept, etc.\n",
    "\n",
    "# --- 2. Initialize the LLMs ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "mcq_generation_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.5)\n",
    "supervisor_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
    "\n",
    "# --- 3. Socratic Tools (Information Gathering Only) ---\n",
    "@tool\n",
    "def extract_code_context(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts key information about the provided code for Socratic questioning.\n",
    "    This tool analyzes code structure, patterns, and potential issues to inform questions.\n",
    "    \"\"\"\n",
    "    context = {\n",
    "        \"code_length\": len(code.split('\\n')),\n",
    "        \"has_functions\": \"def \" in code,\n",
    "        \"has_classes\": \"class \" in code,\n",
    "        \"has_loops\": any(loop in code for loop in [\"for \", \"while \"]),\n",
    "        \"has_conditionals\": any(cond in code for cond in [\"if \", \"elif \", \"else\"]),\n",
    "        \"imports\": [line.strip() for line in code.split('\\n') if line.strip().startswith('import') or line.strip().startswith('from')],\n",
    "        \"potential_issues\": []\n",
    "    }\n",
    "    \n",
    "    # Check for common issues\n",
    "    if code.count('(') != code.count(')'):\n",
    "        context[\"potential_issues\"].append(\"parentheses_mismatch\")\n",
    "    if \"print(\" in code and code.count(\"print(\") > 3:\n",
    "        context[\"potential_issues\"].append(\"excessive_prints\")\n",
    "    if \"global \" in code:\n",
    "        context[\"potential_issues\"].append(\"global_variables\")\n",
    "    \n",
    "    return json.dumps(context)\n",
    "\n",
    "@tool\n",
    "def analyze_concept_depth(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the depth and complexity of a programming concept for appropriate Socratic questioning.\n",
    "    \"\"\"\n",
    "    concept_mapping = {\n",
    "        \"variables\": {\"level\": \"beginner\", \"prerequisites\": [], \"subtopics\": [\"assignment\", \"naming\", \"types\", \"scope\"]},\n",
    "        \"functions\": {\"level\": \"beginner\", \"prerequisites\": [\"variables\"], \"subtopics\": [\"definition\", \"parameters\", \"return\", \"scope\"]},\n",
    "        \"classes\": {\"level\": \"intermediate\", \"prerequisites\": [\"functions\", \"variables\"], \"subtopics\": [\"attributes\", \"methods\", \"inheritance\", \"encapsulation\"]},\n",
    "        \"loops\": {\"level\": \"beginner\", \"prerequisites\": [\"variables\", \"conditionals\"], \"subtopics\": [\"for\", \"while\", \"iteration\", \"break\", \"continue\"]},\n",
    "        \"conditionals\": {\"level\": \"beginner\", \"prerequisites\": [\"variables\", \"comparisons\"], \"subtopics\": [\"if\", \"elif\", \"else\", \"boolean\", \"logical_operators\"]},\n",
    "        \"decorators\": {\"level\": \"advanced\", \"prerequisites\": [\"functions\", \"closures\"], \"subtopics\": [\"syntax\", \"parameters\", \"multiple\", \"built_in\"]},\n",
    "        \"generators\": {\"level\": \"advanced\", \"prerequisites\": [\"functions\", \"loops\"], \"subtopics\": [\"yield\", \"iterator\", \"memory\", \"lazy_evaluation\"]}\n",
    "    }\n",
    "    \n",
    "    concept_info = concept_mapping.get(concept.lower(), {\n",
    "        \"level\": \"intermediate\", \n",
    "        \"prerequisites\": [\"basic_python\"], \n",
    "        \"subtopics\": [\"definition\", \"usage\", \"examples\"]\n",
    "    })\n",
    "    \n",
    "    return json.dumps(concept_info)\n",
    "\n",
    "@tool\n",
    "def generate_mcq_data(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates MCQ data for Socratic assessment. First tries predefined, then generates new ones.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed\",\n",
    "                \"B) To define static methods\",\n",
    "                \"C) To initialize the attributes of an object when it's created\",\n",
    "                \"D) To define the string representation of an object\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"loops\": {\n",
    "            \"question\": \"What will happen if you don't include a break statement in a while loop with a condition that never becomes False?\",\n",
    "            \"options\": [\"A) The program will end normally\", \"B) An error will occur\", \"C) The loop will run indefinitely\", \"D) Python will automatically break the loop\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditionals\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    selected_mcq = mcqs_raw.get(topic.lower())\n",
    "    \n",
    "    if selected_mcq:\n",
    "        formatted_question = f\"**{selected_mcq['question']}**\\n\\n\" + \"\\n\".join(selected_mcq['options'])\n",
    "        return json.dumps({\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq['options'],\n",
    "            \"correct_answer\": selected_mcq['correct_answer']\n",
    "        })\n",
    "    else:\n",
    "        # Generate using LLM\n",
    "        mcq_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Generate a Python MCQ in JSON format with 'question', 'options' (array of 4 strings A-D), and 'correct_answer' (A/B/C/D).\"\"\"),\n",
    "            (\"user\", f\"Topic: {topic}, Difficulty: {difficulty}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = mcq_generation_llm.invoke(mcq_prompt.format())\n",
    "            mcq_data = json.loads(response.content)\n",
    "            \n",
    "            formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data['options'])\n",
    "            return json.dumps({\n",
    "                \"question\": formatted_question,\n",
    "                \"options\": mcq_data['options'],\n",
    "                \"correct_answer\": mcq_data['correct_answer']\n",
    "            })\n",
    "        except:\n",
    "            return json.dumps({\"error\": \"Could not generate MCQ\"})\n",
    "\n",
    "@tool\n",
    "def create_challenge_context(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates challenge context for Socratic guidance through problem-solving.\n",
    "    \"\"\"\n",
    "    challenges = {\n",
    "        \"variables\": {\n",
    "            \"problem\": \"Swapping two variables without using a third variable\",\n",
    "            \"key_concepts\": [\"assignment\", \"arithmetic\", \"temporary storage\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"What methods do you know for swapping values?\",\n",
    "                \"How might mathematical operations help?\",\n",
    "                \"What happens when you do a = a + b?\"\n",
    "            ]\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"problem\": \"Calculating factorial of a number\",\n",
    "            \"key_concepts\": [\"recursion\", \"iteration\", \"base case\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"What is a factorial mathematically?\",\n",
    "                \"How would you break this down into smaller problems?\",\n",
    "                \"What happens when the number is 0 or 1?\"\n",
    "            ]\n",
    "        },\n",
    "        \"loops\": {\n",
    "            \"problem\": \"Generating Fibonacci sequence\",\n",
    "            \"key_concepts\": [\"iteration\", \"sequence\", \"previous values\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"How does each Fibonacci number relate to previous ones?\",\n",
    "                \"What values do you need to track?\",\n",
    "                \"How would you generate the next number?\"\n",
    "            ]\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"problem\": \"Creating a simple bank account class\",\n",
    "            \"key_concepts\": [\"encapsulation\", \"methods\", \"attributes\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"What data should a bank account store?\",\n",
    "                \"What operations can you perform on an account?\",\n",
    "                \"How would you ensure the balance can't be negative?\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json.dumps(challenges.get(topic.lower(), {\n",
    "        \"problem\": f\"Implementing a solution for {topic}\",\n",
    "        \"key_concepts\": [\"problem_solving\", \"implementation\"],\n",
    "        \"guiding_questions\": [f\"How would you approach solving {topic}?\"]\n",
    "    }))\n",
    "\n",
    "# --- 4. Supervisor Node ---\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a supervisor for a Socratic Python tutoring system. Your role is to determine the interaction mode \n",
    "based on the user's message and current state. Remember: EVERYTHING must remain Socratic - we never give direct answers.\n",
    "\n",
    "Available interaction modes:\n",
    "1. \"general\" - General Socratic questioning and topic exploration\n",
    "2. \"code_review\" - Socratic code review through guided questions\n",
    "3. \"concept_exploration\" - Deep dive into concepts through questioning\n",
    "4. \"challenge\" - Guiding through problem-solving via questions\n",
    "5. \"mcq_active\" - When user is answering an MCQ\n",
    "6. \"mcq_request\" - When user wants an MCQ or needs assessment\n",
    "\n",
    "Current state:\n",
    "- Difficulty: {difficulty_level}\n",
    "- Topic: {topic}\n",
    "- Sub-topic: {sub_topic}\n",
    "- Struggle count: {user_struggle_count}\n",
    "- MCQ active: {mcq_active}\n",
    "- Current mode: {interaction_mode}\n",
    "\n",
    "User message indicators:\n",
    "- Code blocks/snippets → \"code_review\"\n",
    "- \"explain\", \"what is\", \"how does\" → \"concept_exploration\"  \n",
    "- \"challenge\", \"problem\", \"exercise\" → \"challenge\"\n",
    "- \"quiz\", \"test\", \"MCQ\" → \"mcq_request\"\n",
    "- A/B/C/D answers when MCQ active → \"mcq_active\"\n",
    "- General questions → \"general\"\n",
    "\n",
    "Respond with just the mode name.\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", supervisor_system_prompt),\n",
    "    (\"user\", \"User message: {user_message}\")\n",
    "])\n",
    "\n",
    "def supervisor_node(state: SocraticAgentState):\n",
    "    \"\"\"Determines the interaction mode for Socratic guidance.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    user_message = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
    "    \n",
    "    # Quick checks for specific patterns\n",
    "    if state.get(\"mcq_active\") and user_message.strip().upper() in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return {\"interaction_mode\": \"mcq_active\"}\n",
    "    \n",
    "    if any(keyword in user_message.lower() for keyword in [\"```\", \"def \", \"class \", \"import \", \"print(\"]):\n",
    "        return {\"interaction_mode\": \"code_review\", \"context_data\": {\"code\": user_message}}\n",
    "    \n",
    "    # Use supervisor LLM for more complex decisions\n",
    "    response = supervisor_llm.invoke(supervisor_prompt.format(\n",
    "        user_message=user_message,\n",
    "        difficulty_level=state.get(\"difficulty_level\", \"beginner\"),\n",
    "        topic=state.get(\"topic\", \"\"),\n",
    "        sub_topic=state.get(\"sub_topic\", \"\"),\n",
    "        user_struggle_count=state.get(\"user_struggle_count\", 0),\n",
    "        mcq_active=state.get(\"mcq_active\", False),\n",
    "        interaction_mode=state.get(\"interaction_mode\", \"general\")\n",
    "    ))\n",
    "    \n",
    "    mode = response.content.strip().lower()\n",
    "    \n",
    "    # Extract context based on mode\n",
    "    context_data = {}\n",
    "    if mode == \"code_review\":\n",
    "        context_data = {\"code\": user_message}\n",
    "    elif mode == \"concept_exploration\":\n",
    "        # Extract concept from message\n",
    "        concept_keywords = [\"variables\", \"functions\", \"classes\", \"loops\", \"conditionals\", \"decorators\", \"generators\"]\n",
    "        for keyword in concept_keywords:\n",
    "            if keyword in user_message.lower():\n",
    "                context_data = {\"concept\": keyword}\n",
    "                break\n",
    "        if not context_data:\n",
    "            context_data = {\"concept\": state.get(\"topic\", \"programming\")}\n",
    "    elif mode == \"challenge\":\n",
    "        context_data = {\"topic\": state.get(\"topic\", \"general\")}\n",
    "    \n",
    "    return {\"interaction_mode\": mode, \"context_data\": context_data}\n",
    "\n",
    "# --- 5. Unified Socratic Node ---\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your CORE PRINCIPLE is to NEVER give direct answers. \n",
    "Instead, guide students to discover answers through thoughtful questioning.\n",
    "\n",
    "INTERACTION MODES:\n",
    "1. GENERAL: Explore topics through open-ended questions\n",
    "2. CODE_REVIEW: Guide code improvement through questions about structure, logic, and best practices\n",
    "3. CONCEPT_EXPLORATION: Deep dive into concepts by asking about understanding, applications, and connections\n",
    "4. CHALLENGE: Guide problem-solving by breaking down problems into smaller questions\n",
    "5. MCQ_ACTIVE: Process MCQ answers and provide Socratic feedback\n",
    "6. MCQ_REQUEST: Generate MCQ and ask guiding questions\n",
    "\n",
    "SOCRATIC PRINCIPLES:\n",
    "- Always ask questions, never state facts directly\n",
    "- Build on student's existing knowledge\n",
    "- Guide discovery through smaller questions\n",
    "- Encourage experimentation and thinking\n",
    "- Adapt question complexity to student's understanding\n",
    "- Use \"What do you think...?\", \"How might...?\", \"Can you explain...?\" patterns\n",
    "\n",
    "Current Context:\n",
    "- Mode: {interaction_mode}\n",
    "- Topic: {topic}\n",
    "- Difficulty: {difficulty_level}\n",
    "- Struggle count: {user_struggle_count}\n",
    "- MCQ active: {mcq_active}\n",
    "- Context data: {context_data}\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Always start with \"Thought: [your reasoning]\" then provide your Socratic question or response.\n",
    "If you need tool information, call the appropriate tool first, then ask questions based on the results.\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", socratic_system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Tools available to the Socratic agent\n",
    "tools = [extract_code_context, analyze_concept_depth, generate_mcq_data, create_challenge_context]\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "def socratic_agent_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Unified Socratic agent that handles all interactions through questioning.\n",
    "    \"\"\"\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"interaction_mode\": state.get(\"interaction_mode\", \"general\"),\n",
    "        \"topic\": state.get(\"topic\", \"\"),\n",
    "        \"difficulty_level\": state.get(\"difficulty_level\", \"beginner\"),\n",
    "        \"user_struggle_count\": state.get(\"user_struggle_count\", 0),\n",
    "        \"mcq_active\": state.get(\"mcq_active\", False),\n",
    "        \"context_data\": state.get(\"context_data\", {})\n",
    "    })\n",
    "    \n",
    "    # Extract thought and content\n",
    "    thought = \"\"\n",
    "    display_content = response.content\n",
    "    \n",
    "    if response.content and response.content.startswith(\"Thought:\"):\n",
    "        parts = response.content.split(\"Thought:\", 1)\n",
    "        if len(parts) > 1:\n",
    "            thought = parts[1].strip().split('\\n', 1)[0]\n",
    "            if '\\n' in parts[1]:\n",
    "                display_content = parts[1].split('\\n', 1)[1].strip()\n",
    "            else:\n",
    "                display_content = \"\"\n",
    "    \n",
    "    # Handle tool calls if any\n",
    "    if response.tool_calls:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=display_content, tool_calls=response.tool_calls)],\n",
    "            \"agent_thought\": thought\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=display_content)],\n",
    "            \"agent_thought\": thought\n",
    "        }\n",
    "\n",
    "# --- 6. Tool Execution Node ---\n",
    "def execute_tools(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Executes tools and continues with Socratic questioning based on results.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if not (isinstance(last_message, AIMessage) and last_message.tool_calls):\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    tool_mapping = {\n",
    "        \"extract_code_context\": extract_code_context,\n",
    "        \"analyze_concept_depth\": analyze_concept_depth,\n",
    "        \"generate_mcq_data\": generate_mcq_data,\n",
    "        \"create_challenge_context\": create_challenge_context\n",
    "    }\n",
    "    \n",
    "    tool_messages = []\n",
    "    state_updates = {}\n",
    "    \n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        if tool_name in tool_mapping:\n",
    "            try:\n",
    "                result = tool_mapping[tool_name].invoke(tool_args)\n",
    "                tool_messages.append(ToolMessage(content=result, tool_call_id=tool_call[\"id\"]))\n",
    "                \n",
    "                # Handle specific tool results\n",
    "                if tool_name == \"generate_mcq_data\":\n",
    "                    try:\n",
    "                        mcq_data = json.loads(result)\n",
    "                        if \"error\" not in mcq_data:\n",
    "                            state_updates.update({\n",
    "                                \"mcq_active\": True,\n",
    "                                \"mcq_question\": mcq_data.get(\"question\", \"\"),\n",
    "                                \"mcq_options\": mcq_data.get(\"options\", []),\n",
    "                                \"mcq_correct_answer\": mcq_data.get(\"correct_answer\", \"\")\n",
    "                            })\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                        \n",
    "            except Exception as e:\n",
    "                tool_messages.append(ToolMessage(content=f\"Error: {str(e)}\", tool_call_id=tool_call[\"id\"]))\n",
    "    \n",
    "    return {\"messages\": tool_messages, **state_updates}\n",
    "\n",
    "# --- 7. MCQ Processing Node ---\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Processes MCQ answers in a Socratic manner.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    user_answer = last_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    \n",
    "    is_correct = user_answer == correct_answer\n",
    "    \n",
    "    # Socratic feedback based on answer\n",
    "    if is_correct:\n",
    "        feedback = \"Excellent! You got it right. Now, can you explain why the other options were incorrect? What made you choose this answer?\"\n",
    "        new_struggle_count = 0\n",
    "    else:\n",
    "        feedback = f\"I see you chose {user_answer}. Let's think about this together. What do you think the correct answer might be and why? Can you walk me through your reasoning?\"\n",
    "        new_struggle_count = state.get(\"user_struggle_count\", 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=feedback)],\n",
    "        \"user_struggle_count\": new_struggle_count,\n",
    "        \"mcq_active\": False,\n",
    "        \"mcq_question\": \"\",\n",
    "        \"mcq_options\": [],\n",
    "        \"mcq_correct_answer\": \"\",\n",
    "        \"interaction_mode\": \"general\"\n",
    "    }\n",
    "\n",
    "# --- 8. Graph Construction ---\n",
    "def should_continue_to_tools(state: SocraticAgentState):\n",
    "    \"\"\"Check if we need to execute tools.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"execute_tools\"\n",
    "    return \"end\"\n",
    "\n",
    "def route_from_supervisor(state: SocraticAgentState):\n",
    "    \"\"\"Route based on interaction mode.\"\"\"\n",
    "    mode = state.get(\"interaction_mode\", \"general\")\n",
    "    if mode == \"mcq_active\":\n",
    "        return \"process_mcq\"\n",
    "    return \"socratic_agent\"\n",
    "\n",
    "def build_enhanced_socratic_graph():\n",
    "    \"\"\"Build the graph with supervisor and unified Socratic approach.\"\"\"\n",
    "    workflow = StateGraph(SocraticAgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"supervisor\", supervisor_node)\n",
    "    workflow.add_node(\"socratic_agent\", socratic_agent_node)\n",
    "    workflow.add_node(\"execute_tools\", execute_tools)\n",
    "    workflow.add_node(\"process_mcq\", process_mcq_answer)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        route_from_supervisor,\n",
    "        {\n",
    "            \"socratic_agent\": \"socratic_agent\",\n",
    "            \"process_mcq\": \"process_mcq\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"socratic_agent\",\n",
    "        should_continue_to_tools,\n",
    "        {\n",
    "            \"execute_tools\": \"execute_tools\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"execute_tools\", \"socratic_agent\")\n",
    "    workflow.add_edge(\"process_mcq\", END)\n",
    "    \n",
    "    # Add memory\n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "# --- 9. Initialize System ---\n",
    "enhanced_socratic_graph = build_enhanced_socratic_graph()\n",
    "enhanced_socratic_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d226d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fully Socratic Python Tutor ===\n",
      "🎯 Everything is Socratic - no direct answers, only guiding questions!\n",
      "📚 Supports: Code Review, Concept Exploration, Challenges, MCQs\n",
      "🧠 Memory-enabled for persistent learning\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Test 1: I want to learn about variable... ---\n",
      "Socratic Response: What do you think a variable is in programming?\n",
      "\n",
      "--- Test 2: def add(a, b):\n",
      "    return a + ... ---\n",
      "Socratic Response: Given the code `def add(a, b):\\n    return a + b`, what aspects come to mind when you consider if it's \"correct\"? What do you think this function is intended to do?\n",
      "\n",
      "--- Test 3: Explain what a function is... ---\n",
      "Error: 'list' object has no attribute 'startswith'\n",
      "Socratic Response: I'm having trouble processing that. Can you tell me what you'd like to learn about in Python?\n",
      "\n",
      "--- Test 4: Give me a coding challenge... ---\n",
      "Socratic Response: What kind of topic are you interested in, and what difficulty level would you prefer (beginner, intermediate, advanced)?\n",
      "\n",
      "--- Test 5: Can I have an MCQ on loops?... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 24\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socratic Response: Here's a multiple-choice question for you:\n",
      "\n",
      "**What will happen if you don't include a break statement in a `while` loop with a condition that never becomes `False`?**\n",
      "\n",
      "A) The program will end normally\n",
      "B) An error will occur\n",
      "C) The loop will run indefinitely\n",
      "D) Python will automatically break the loop\n",
      "\n",
      "What do you think is the correct answer, and more importantly, can you explain *why* you think that's the case?\n",
      "\n",
      "==================================================\n",
      "✅ Fully Socratic Python Tutor Ready!\n",
      "Key Features:\n",
      "🔍 Supervisor-driven interaction routing\n",
      "❓ Pure Socratic method - only questions\n",
      "🧠 Memory persistence across sessions\n",
      "🔧 Context-aware tool usage\n",
      "📊 Adaptive difficulty and struggle tracking\n"
     ]
    }
   ],
   "source": [
    "def create_initial_state():\n",
    "    \"\"\"Create initial state for new conversations.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [],\n",
    "        \"difficulty_level\": \"beginner\",\n",
    "        \"user_struggle_count\": 0,\n",
    "        \"topic\": \"\",\n",
    "        \"sub_topic\": \"\",\n",
    "        \"mcq_active\": False,\n",
    "        \"mcq_question\": \"\",\n",
    "        \"mcq_options\": [],\n",
    "        \"mcq_correct_answer\": \"\",\n",
    "        \"agent_thought\": \"\",\n",
    "        \"interaction_mode\": \"general\",\n",
    "        \"context_data\": {}\n",
    "    }\n",
    "\n",
    "def run_socratic_conversation(user_input: str, thread_id: str = \"default\"):\n",
    "    \"\"\"\n",
    "    Run a fully Socratic conversation.\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    state = create_initial_state()\n",
    "    state[\"messages\"] = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    try:\n",
    "        result = enhanced_socratic_graph.invoke(state, config=config)\n",
    "        \n",
    "        # Return the last AI message\n",
    "        for message in reversed(result[\"messages\"]):\n",
    "            if isinstance(message, AIMessage):\n",
    "                return message.content\n",
    "        \n",
    "        return \"What Python topic would you like to explore today? What interests you most about programming?\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"I'm having trouble processing that. Can you tell me what you'd like to learn about in Python?\"\n",
    "\n",
    "# --- 10. Test the System ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Fully Socratic Python Tutor ===\")\n",
    "    print(\"🎯 Everything is Socratic - no direct answers, only guiding questions!\")\n",
    "    print(\"📚 Supports: Code Review, Concept Exploration, Challenges, MCQs\")\n",
    "    print(\"🧠 Memory-enabled for persistent learning\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_scenarios = [\n",
    "        \"I want to learn about variables\",\n",
    "        \"def add(a, b):\\n    return a + b\\n\\nIs this code correct?\",\n",
    "        \"Explain what a function is\",\n",
    "        \"Give me a coding challenge\",\n",
    "        \"Can I have an MCQ on loops?\",\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\n--- Test {i}: {scenario[:30]}... ---\")\n",
    "        response = run_socratic_conversation(scenario, f\"test_{i}\")\n",
    "        print(f\"Socratic Response: {response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"✅ Fully Socratic Python Tutor Ready!\")\n",
    "    print(\"Key Features:\")\n",
    "    print(\"🔍 Supervisor-driven interaction routing\")\n",
    "    print(\"❓ Pure Socratic method - only questions\")\n",
    "    print(\"🧠 Memory persistence across sessions\")\n",
    "    print(\"🔧 Context-aware tool usage\")\n",
    "    print(\"📊 Adaptive difficulty and struggle tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97b88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. Helper Functions ---\n",
    "def create_initial_state():\n",
    "    \"\"\"Creates initial state for new conversations.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [],\n",
    "        \"difficulty_level\": \"beginner\",\n",
    "        \"user_struggle_count\": 0,\n",
    "        \"topic\": \"\",\n",
    "        \"sub_topic\": \"\",\n",
    "        \"mcq_active\": False,\n",
    "        \"mcq_question\": \"\",\n",
    "        \"mcq_options\": [],\n",
    "        \"mcq_correct_answer\": \"\",\n",
    "        \"agent_thought\": \"\",\n",
    "        \"next_action\": \"\"\n",
    "    }\n",
    "\n",
    "def run_conversation(user_input: str, thread_id: str = \"default\"):\n",
    "    \"\"\"\n",
    "    Runs a conversation turn with the enhanced Socratic bot.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's message\n",
    "        thread_id: Unique identifier for the conversation thread\n",
    "    \n",
    "    Returns:\n",
    "        The bot's response\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    # Create initial state if needed\n",
    "    state = create_initial_state()\n",
    "    state[\"messages\"] = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    try:\n",
    "        result = enhanced_socratic_graph.invoke(state, config=config)\n",
    "        \n",
    "        # Return the last AI message content\n",
    "        for message in reversed(result[\"messages\"]):\n",
    "            if isinstance(message, AIMessage):\n",
    "                return message.content\n",
    "        \n",
    "        return \"I'm here to help you learn Python! What would you like to explore?\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversation: {e}\")\n",
    "        return \"I encountered an error. Let's try again!\"\n",
    "\n",
    "# --- 11. Test the Enhanced System ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Enhanced Socratic Bot with Supervisor and Memory ===\")\n",
    "    print(\"Features:\")\n",
    "    print(\"- Supervisor node for intelligent routing\")\n",
    "    print(\"- Memory saver for conversation persistence\")\n",
    "    print(\"- Enhanced tool routing and state management\")\n",
    "    print(\"- Multi-turn conversation support\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Test conversation\n",
    "    test_thread = \"test_conversation_1\"\n",
    "    \n",
    "    print(\"Bot: Hello! I'm your Socratic Python tutor. What would you like to learn today?\")\n",
    "    \n",
    "    # Simulate a conversation\n",
    "    responses = [\n",
    "        \"I want to learn about variables\",\n",
    "        \"What are variables used for?\",\n",
    "        \"Give me an MCQ on variables\",\n",
    "        \"C\",\n",
    "        \"Can you give me a coding challenge about functions?\"\n",
    "    ]\n",
    "    \n",
    "    for user_msg in responses:\n",
    "        print(f\"\\nUser: {user_msg}\")\n",
    "        bot_response = run_conversation(user_msg, test_thread)\n",
    "        print(f\"Bot: {bot_response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Enhanced Socratic Bot is ready!\")\n",
    "    print(\"Key improvements:\")\n",
    "    print(\"✓ Supervisor node for intelligent routing\")\n",
    "    print(\"✓ Memory persistence across conversations\")\n",
    "    print(\"✓ Better state management\")\n",
    "    print(\"✓ Modular node architecture\")\n",
    "    print(\"✓ Enhanced error handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# socratic_bot_logic_enhanced.py\n",
    "import os\n",
    "from typing import List, TypedDict, Annotated, Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import json\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. Define the Agent State ---\n",
    "class SocraticAgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    difficulty_level: str\n",
    "    user_struggle_count: int\n",
    "    topic: str\n",
    "    sub_topic: str\n",
    "    mcq_active: bool\n",
    "    mcq_question: str\n",
    "    mcq_options: List[str]\n",
    "    mcq_correct_answer: str\n",
    "    agent_thought: str\n",
    "    interaction_mode: str  # 'general', 'code_review', 'concept_exploration', 'challenge', 'mcq'\n",
    "    context_data: dict  # Store relevant context like code, concept, etc.\n",
    "\n",
    "# --- 2. Initialize the LLMs ---\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API\")\n",
    "llm = ChatGroq(model = \"llama3-8b-8192\", temperature=0.7)\n",
    "mcq_generation_llm = ChatGroq(model = \"llama3-70b-8192\", temperature=0.5)\n",
    "supervisor_llm = ChatGroq(model = \"llama3-70b-8192\", temperature=0.3)\n",
    "\n",
    "# --- 3. Socratic Tools (Information Gathering Only) ---\n",
    "@tool\n",
    "def extract_code_context(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts key information about the provided code for Socratic questioning.\n",
    "    This tool analyzes code structure, patterns, and potential issues to inform questions.\n",
    "    \"\"\"\n",
    "    context = {\n",
    "        \"code_length\": len(code.split('\\n')),\n",
    "        \"has_functions\": \"def \" in code,\n",
    "        \"has_classes\": \"class \" in code,\n",
    "        \"has_loops\": any(loop in code for loop in [\"for \", \"while \"]),\n",
    "        \"has_conditionals\": any(cond in code for cond in [\"if \", \"elif \", \"else\"]),\n",
    "        \"imports\": [line.strip() for line in code.split('\\n') if line.strip().startswith('import') or line.strip().startswith('from')],\n",
    "        \"potential_issues\": []\n",
    "    }\n",
    "    \n",
    "    # Check for common issues\n",
    "    if code.count('(') != code.count(')'):\n",
    "        context[\"potential_issues\"].append(\"parentheses_mismatch\")\n",
    "    if \"print(\" in code and code.count(\"print(\") > 3:\n",
    "        context[\"potential_issues\"].append(\"excessive_prints\")\n",
    "    if \"global \" in code:\n",
    "        context[\"potential_issues\"].append(\"global_variables\")\n",
    "    \n",
    "    return json.dumps(context)\n",
    "\n",
    "@tool\n",
    "def analyze_concept_depth(concept: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the depth and complexity of a programming concept for appropriate Socratic questioning.\n",
    "    \"\"\"\n",
    "    concept_mapping = {\n",
    "        \"variables\": {\"level\": \"beginner\", \"prerequisites\": [], \"subtopics\": [\"assignment\", \"naming\", \"types\", \"scope\"]},\n",
    "        \"functions\": {\"level\": \"beginner\", \"prerequisites\": [\"variables\"], \"subtopics\": [\"definition\", \"parameters\", \"return\", \"scope\"]},\n",
    "        \"classes\": {\"level\": \"intermediate\", \"prerequisites\": [\"functions\", \"variables\"], \"subtopics\": [\"attributes\", \"methods\", \"inheritance\", \"encapsulation\"]},\n",
    "        \"loops\": {\"level\": \"beginner\", \"prerequisites\": [\"variables\", \"conditionals\"], \"subtopics\": [\"for\", \"while\", \"iteration\", \"break\", \"continue\"]},\n",
    "        \"conditionals\": {\"level\": \"beginner\", \"prerequisites\": [\"variables\", \"comparisons\"], \"subtopics\": [\"if\", \"elif\", \"else\", \"boolean\", \"logical_operators\"]},\n",
    "        \"decorators\": {\"level\": \"advanced\", \"prerequisites\": [\"functions\", \"closures\"], \"subtopics\": [\"syntax\", \"parameters\", \"multiple\", \"built_in\"]},\n",
    "        \"generators\": {\"level\": \"advanced\", \"prerequisites\": [\"functions\", \"loops\"], \"subtopics\": [\"yield\", \"iterator\", \"memory\", \"lazy_evaluation\"]}\n",
    "    }\n",
    "    \n",
    "    concept_info = concept_mapping.get(concept.lower(), {\n",
    "        \"level\": \"intermediate\", \n",
    "        \"prerequisites\": [\"basic_python\"], \n",
    "        \"subtopics\": [\"definition\", \"usage\", \"examples\"]\n",
    "    })\n",
    "    \n",
    "    return json.dumps(concept_info)\n",
    "\n",
    "@tool\n",
    "def generate_mcq_data(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates MCQ data for Socratic assessment. First tries predefined, then generates new ones.\n",
    "    \"\"\"\n",
    "    mcqs_raw = {\n",
    "        \"variables\": {\n",
    "            \"question\": \"Which of the following data types is mutable in Python?\",\n",
    "            \"options\": [\"A) Tuple\", \"B) String\", \"C) List\", \"D) Integer\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"question\": \"Which keyword is used to define a function in Python?\",\n",
    "            \"options\": [\"A) func\", \"B) define\", \"C) def\", \"D) function\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"question\": \"In Python, what is the primary purpose of the `__init__` method in a class?\",\n",
    "            \"options\": [\n",
    "                \"A) To destroy an object when it's no longer needed\",\n",
    "                \"B) To define static methods\",\n",
    "                \"C) To initialize the attributes of an object when it's created\",\n",
    "                \"D) To define the string representation of an object\"\n",
    "            ],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"loops\": {\n",
    "            \"question\": \"What will happen if you don't include a break statement in a while loop with a condition that never becomes False?\",\n",
    "            \"options\": [\"A) The program will end normally\", \"B) An error will occur\", \"C) The loop will run indefinitely\", \"D) Python will automatically break the loop\"],\n",
    "            \"correct_answer\": \"C\"\n",
    "        },\n",
    "        \"conditionals\": {\n",
    "            \"question\": \"Which Python keyword is used to start an 'if' statement?\",\n",
    "            \"options\": [\"A) then\", \"B) if\", \"C) when\", \"D) check\"],\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    selected_mcq = mcqs_raw.get(topic.lower())\n",
    "    \n",
    "    if selected_mcq:\n",
    "        formatted_question = f\"**{selected_mcq['question']}**\\n\\n\" + \"\\n\".join(selected_mcq['options'])\n",
    "        return json.dumps({\n",
    "            \"question\": formatted_question,\n",
    "            \"options\": selected_mcq['options'],\n",
    "            \"correct_answer\": selected_mcq['correct_answer']\n",
    "        })\n",
    "    else:\n",
    "        # Generate using LLM\n",
    "        mcq_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Generate a Python MCQ in JSON format with 'question', 'options' (array of 4 strings A-D), and 'correct_answer' (A/B/C/D).\"\"\"),\n",
    "            (\"user\", f\"Topic: {topic}, Difficulty: {difficulty}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = mcq_generation_llm.invoke(mcq_prompt.format())\n",
    "            mcq_data = json.loads(response.content)\n",
    "            \n",
    "            formatted_question = f\"**{mcq_data['question']}**\\n\\n\" + \"\\n\".join(mcq_data['options'])\n",
    "            return json.dumps({\n",
    "                \"question\": formatted_question,\n",
    "                \"options\": mcq_data['options'],\n",
    "                \"correct_answer\": mcq_data['correct_answer']\n",
    "            })\n",
    "        except:\n",
    "            return json.dumps({\"error\": \"Could not generate MCQ\"})\n",
    "\n",
    "@tool\n",
    "def create_challenge_context(topic: str, difficulty: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates challenge context for Socratic guidance through problem-solving.\n",
    "    \"\"\"\n",
    "    challenges = {\n",
    "        \"variables\": {\n",
    "            \"problem\": \"Swapping two variables without using a third variable\",\n",
    "            \"key_concepts\": [\"assignment\", \"arithmetic\", \"temporary storage\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"What methods do you know for swapping values?\",\n",
    "                \"How might mathematical operations help?\",\n",
    "                \"What happens when you do a = a + b?\"\n",
    "            ]\n",
    "        },\n",
    "        \"functions\": {\n",
    "            \"problem\": \"Calculating factorial of a number\",\n",
    "            \"key_concepts\": [\"recursion\", \"iteration\", \"base case\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"What is a factorial mathematically?\",\n",
    "                \"How would you break this down into smaller problems?\",\n",
    "                \"What happens when the number is 0 or 1?\"\n",
    "            ]\n",
    "        },\n",
    "        \"loops\": {\n",
    "            \"problem\": \"Generating Fibonacci sequence\",\n",
    "            \"key_concepts\": [\"iteration\", \"sequence\", \"previous values\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"How does each Fibonacci number relate to previous ones?\",\n",
    "                \"What values do you need to track?\",\n",
    "                \"How would you generate the next number?\"\n",
    "            ]\n",
    "        },\n",
    "        \"classes\": {\n",
    "            \"problem\": \"Creating a simple bank account class\",\n",
    "            \"key_concepts\": [\"encapsulation\", \"methods\", \"attributes\"],\n",
    "            \"guiding_questions\": [\n",
    "                \"What data should a bank account store?\",\n",
    "                \"What operations can you perform on an account?\",\n",
    "                \"How would you ensure the balance can't be negative?\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return json.dumps(challenges.get(topic.lower(), {\n",
    "        \"problem\": f\"Implementing a solution for {topic}\",\n",
    "        \"key_concepts\": [\"problem_solving\", \"implementation\"],\n",
    "        \"guiding_questions\": [f\"How would you approach solving {topic}?\"]\n",
    "    }))\n",
    "\n",
    "# --- 4. Supervisor Node ---\n",
    "supervisor_system_prompt = \"\"\"\n",
    "You are a supervisor for a Socratic Python tutoring system. Your role is to determine the interaction mode \n",
    "based on the user's message and current state. Remember: EVERYTHING must remain Socratic - we never give direct answers.\n",
    "\n",
    "Available interaction modes:\n",
    "1. \"general\" - General Socratic questioning and topic exploration\n",
    "2. \"code_review\" - Socratic code review through guided questions\n",
    "3. \"concept_exploration\" - Deep dive into concepts through questioning\n",
    "4. \"challenge\" - Guiding through problem-solving via questions\n",
    "5. \"mcq_active\" - When user is answering an MCQ\n",
    "6. \"mcq_request\" - When user wants an MCQ or needs assessment\n",
    "7. \"evaluate_understanding\" - When the user gives a short, affirmative response (e.g., \"Got it\", \"Ok\", \"Yes\") and the tutor needs to determine if it signifies true understanding or requires further probing.\n",
    "\n",
    "Current state:\n",
    "- Difficulty: {difficulty_level}\n",
    "- Topic: {topic}\n",
    "- Sub-topic: {sub_topic}\n",
    "- Struggle count: {user_struggle_count}\n",
    "- MCQ active: {mcq_active}\n",
    "- Current mode: {interaction_mode}\n",
    "\n",
    "User message indicators:\n",
    "- Code blocks/snippets → \"code_review\"\n",
    "- \"explain\", \"what is\", \"how does\" → \"concept_exploration\"  \n",
    "- \"challenge\", \"problem\", \"exercise\" → \"challenge\"\n",
    "- \"quiz\", \"test\", \"MCQ\" → \"mcq_request\"\n",
    "- A/B/C/D answers when MCQ active → \"mcq_active\"\n",
    "- Short affirmative responses like \"Got it\", \"Ok\", \"Yes\", \"I understand\" → \"evaluate_understanding\"\n",
    "- General questions or detailed responses → \"general\"\n",
    "\n",
    "Respond with just the mode name.\n",
    "\"\"\"\n",
    "\n",
    "supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", supervisor_system_prompt),\n",
    "    (\"user\", \"User message: {user_message}\")\n",
    "])\n",
    "\n",
    "def supervisor_node(state: SocraticAgentState):\n",
    "    \"\"\"Determines the interaction mode for Socratic guidance.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    user_message = last_message.content if hasattr(last_message, 'content') else str(last_message)\n",
    "    \n",
    "    # Quick checks for specific patterns\n",
    "    if state.get(\"mcq_active\") and user_message.strip().upper() in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return {\"interaction_mode\": \"mcq_active\"}\n",
    "    \n",
    "    if any(keyword in user_message.lower() for keyword in [\"```\", \"def \", \"class \", \"import \", \"print(\"]):\n",
    "        return {\"interaction_mode\": \"code_review\", \"context_data\": {\"code\": user_message}}\n",
    "    \n",
    "    # Check for short affirmative responses to trigger 'evaluate_understanding'\n",
    "    short_affirmative_keywords = [\"got it\", \"ok\", \"yes\", \"i understand\", \"understood\", \"ahh i see\", \"i get it now\"]\n",
    "    if any(keyword in user_message.lower() for keyword in short_affirmative_keywords) and len(user_message.split()) <= 5:\n",
    "        # If it's a short affirmative, force evaluation\n",
    "        return {\"interaction_mode\": \"evaluate_understanding\"}\n",
    "\n",
    "    # Use supervisor LLM for more complex decisions\n",
    "    response = supervisor_llm.invoke(supervisor_prompt.format(\n",
    "        user_message=user_message,\n",
    "        difficulty_level=state.get(\"difficulty_level\", \"beginner\"),\n",
    "        topic=state.get(\"topic\", \"\"),\n",
    "        sub_topic=state.get(\"sub_topic\", \"\"),\n",
    "        user_struggle_count=state.get(\"user_struggle_count\", 0),\n",
    "        mcq_active=state.get(\"mcq_active\", False),\n",
    "        interaction_mode=state.get(\"interaction_mode\", \"general\")\n",
    "    ))\n",
    "    \n",
    "    mode = response.content.strip().lower()\n",
    "    \n",
    "    # Extract context based on mode\n",
    "    context_data = {}\n",
    "    if mode == \"code_review\":\n",
    "        context_data = {\"code\": user_message}\n",
    "    elif mode == \"concept_exploration\":\n",
    "        # Extract concept from message\n",
    "        concept_keywords = [\"variables\", \"functions\", \"classes\", \"loops\", \"conditionals\", \"decorators\", \"generators\"]\n",
    "        found_concept = None\n",
    "        for keyword in concept_keywords:\n",
    "            if keyword in user_message.lower():\n",
    "                found_concept = keyword\n",
    "                break\n",
    "        if found_concept:\n",
    "            context_data = {\"concept\": found_concept}\n",
    "        else:\n",
    "            context_data = {\"concept\": state.get(\"topic\", \"programming\")} # Fallback to current topic\n",
    "    elif mode == \"challenge\":\n",
    "        context_data = {\"topic\": state.get(\"topic\", \"general\")}\n",
    "    \n",
    "    return {\"interaction_mode\": mode, \"context_data\": context_data}\n",
    "\n",
    "# --- 5. Unified Socratic Node ---\n",
    "socratic_system_prompt = \"\"\"\n",
    "You are a Socratic Python programming tutor. Your CORE PRINCIPLE is to NEVER give direct answers. \n",
    "Instead, guide students to discover answers through thoughtful questioning.\n",
    "\n",
    "INTERACTION MODES:\n",
    "1. GENERAL: Explore topics through open-ended questions\n",
    "2. CODE_REVIEW: Guide code improvement through questions about structure, logic, and best practices\n",
    "3. CONCEPT_EXPLORATION: Deep dive into concepts by asking about understanding, applications, and connections\n",
    "4. CHALLENGE: Guide problem-solving by breaking down problems into smaller questions\n",
    "5. MCQ_ACTIVE: Process MCQ answers and provide Socratic feedback\n",
    "6. MCQ_REQUEST: Generate MCQ and ask guiding questions\n",
    "7. EVALUATE_UNDERSTANDING: The user has given a short affirmative response (e.g., \"Got it\"). Your task is to ask a probing, Socratic question to verify their understanding. Do NOT simply acknowledge their \"Got it\". Instead, ask them to elaborate, apply the concept, or explain it in their own words. If user_struggle_count is high, consider rephrasing or simplifying.\n",
    "\n",
    "SOCRATIC PRINCIPLES:\n",
    "- Always ask questions, never state facts directly\n",
    "- Build on student's existing knowledge\n",
    "- Guide discovery through smaller questions\n",
    "- Encourage experimentation and thinking\n",
    "- Adapt question complexity to student's understanding\n",
    "- Use \"What do you think...?\", \"How might...?\", \"Can you explain...?\" patterns\n",
    "\n",
    "Current Context:\n",
    "- Mode: {interaction_mode}\n",
    "- Topic: {topic}\n",
    "- Difficulty: {difficulty_level}\n",
    "- Struggle count: {user_struggle_count}\n",
    "- MCQ active: {mcq_active}\n",
    "- Context data: {context_data}\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Always start with \"Thought: [your reasoning]\" then provide your Socratic question or response.\n",
    "If you need tool information, call the appropriate tool first, then ask questions based on the results.\n",
    "\"\"\"\n",
    "\n",
    "socratic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", socratic_system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])\n",
    "\n",
    "# Tools available to the Socratic agent\n",
    "tools = [extract_code_context, analyze_concept_depth, generate_mcq_data, create_challenge_context]\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"auto\")\n",
    "socratic_agent_runnable = socratic_prompt | llm_with_tools\n",
    "\n",
    "def socratic_agent_node(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Unified Socratic agent that handles all interactions through questioning.\n",
    "    \"\"\"\n",
    "    response = socratic_agent_runnable.invoke({\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"interaction_mode\": state.get(\"interaction_mode\", \"general\"),\n",
    "        \"topic\": state.get(\"topic\", \"\"),\n",
    "        \"difficulty_level\": state.get(\"difficulty_level\", \"beginner\"),\n",
    "        \"user_struggle_count\": state.get(\"user_struggle_count\", 0),\n",
    "        \"mcq_active\": state.get(\"mcq_active\", False),\n",
    "        \"context_data\": state.get(\"context_data\", {})\n",
    "    })\n",
    "    \n",
    "    # Extract thought and content more robustly\n",
    "    thought = \"\"\n",
    "    actual_content = response.content # Store the original content\n",
    "    display_content = \"\"\n",
    "\n",
    "    if actual_content:\n",
    "        if actual_content.startswith(\"Thought:\"):\n",
    "            parts = actual_content.split(\"Thought:\", 1)\n",
    "            if len(parts) > 1:\n",
    "                thought_and_rest = parts[1].strip()\n",
    "                thought_lines = thought_and_rest.split('\\n', 1)\n",
    "                thought = thought_lines[0]\n",
    "                \n",
    "                if len(thought_lines) > 1:\n",
    "                    display_content = thought_lines[1].strip()\n",
    "                else:\n",
    "                    # If no newline after thought, use the thought_and_rest as display_content\n",
    "                    # This ensures something is displayed even if the LLM only gave a thought.\n",
    "                    display_content = thought_and_rest \n",
    "            else:\n",
    "                # If \"Thought:\" is there but nothing after it, display original content\n",
    "                display_content = actual_content\n",
    "        else:\n",
    "            # If it doesn't start with \"Thought:\", display as is\n",
    "            display_content = actual_content\n",
    "    \n",
    "    # Handle tool calls if any\n",
    "    if response.tool_calls:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=display_content, tool_calls=response.tool_calls)],\n",
    "            \"agent_thought\": thought\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=display_content)],\n",
    "            \"agent_thought\": thought\n",
    "        }\n",
    "\n",
    "# --- 6. Tool Execution Node ---\n",
    "def execute_tools(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Executes tools and continues with Socratic questioning based on results.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if not (isinstance(last_message, AIMessage) and last_message.tool_calls):\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    tool_mapping = {\n",
    "        \"extract_code_context\": extract_code_context,\n",
    "        \"analyze_concept_depth\": analyze_concept_depth,\n",
    "        \"generate_mcq_data\": generate_mcq_data,\n",
    "        \"create_challenge_context\": create_challenge_context\n",
    "    }\n",
    "    \n",
    "    tool_messages = []\n",
    "    state_updates = {}\n",
    "    \n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        \n",
    "        if tool_name in tool_mapping:\n",
    "            try:\n",
    "                result = tool_mapping[tool_name].invoke(tool_args)\n",
    "                tool_messages.append(ToolMessage(content=result, tool_call_id=tool_call[\"id\"]))\n",
    "                \n",
    "                # Handle specific tool results\n",
    "                if tool_name == \"generate_mcq_data\":\n",
    "                    try:\n",
    "                        mcq_data = json.loads(result)\n",
    "                        if \"error\" not in mcq_data:\n",
    "                            state_updates.update({\n",
    "                                \"mcq_active\": True,\n",
    "                                \"mcq_question\": mcq_data.get(\"question\", \"\"),\n",
    "                                \"mcq_options\": mcq_data.get(\"options\", []),\n",
    "                                \"mcq_correct_answer\": mcq_data.get(\"correct_answer\", \"\")\n",
    "                            })\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                        \n",
    "            except Exception as e:\n",
    "                tool_messages.append(ToolMessage(content=f\"Error: {str(e)}\", tool_call_id=tool_call[\"id\"]))\n",
    "    \n",
    "    return {\"messages\": tool_messages, **state_updates}\n",
    "\n",
    "# --- 7. MCQ Processing Node ---\n",
    "def process_mcq_answer(state: SocraticAgentState):\n",
    "    \"\"\"\n",
    "    Processes MCQ answers in a Socratic manner.\n",
    "    If the answer is incorrect, it increments struggle count and routes back to supervisor.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    user_answer = last_message.content.strip().upper()\n",
    "    correct_answer = state.get(\"mcq_correct_answer\", \"\")\n",
    "    \n",
    "    # Extract just the letter from the user's response, e.g., \"A)\" -> \"A\"\n",
    "    if len(user_answer) > 0 and user_answer[0] in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        user_answer_letter = user_answer[0]\n",
    "    else:\n",
    "        user_answer_letter = \"\" # Invalid answer format\n",
    "\n",
    "    is_correct = user_answer_letter == correct_answer\n",
    "    \n",
    "    new_struggle_count = state.get(\"user_struggle_count\", 0)\n",
    "    feedback_message = \"\"\n",
    "    next_route = \"continue_to_supervisor\" # Default to route back to supervisor\n",
    "\n",
    "    if is_correct:\n",
    "        feedback_message = \"Excellent! You got it right. Now, can you explain why the other options were incorrect? What made you choose this answer?\"\n",
    "        new_struggle_count = 0 # Reset struggle count on correct answer\n",
    "        next_route = \"continue_to_supervisor\" # Still route to supervisor to decide next general step\n",
    "    else:\n",
    "        feedback_message = f\"I see you chose {user_answer_letter}. Let's think about this together. What do you think the correct answer might be and why? Can you walk me through your reasoning?\"\n",
    "        new_struggle_count += 1\n",
    "        # If struggle count exceeds a threshold, we might want to change topic or difficulty\n",
    "        # For now, just routing back to supervisor will allow it to re-evaluate based on the new struggle count.\n",
    "        next_route = \"continue_to_supervisor\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=feedback_message)],\n",
    "        \"user_struggle_count\": new_struggle_count,\n",
    "        \"mcq_active\": False, # Always set MCQ to inactive after processing an answer\n",
    "        \"mcq_question\": \"\",\n",
    "        \"mcq_options\": [],\n",
    "        \"mcq_correct_answer\": \"\",\n",
    "        \"interaction_mode\": \"general\", # Set to general, supervisor will re-evaluate\n",
    "        \"next_route\": next_route # Custom key for routing\n",
    "    }\n",
    "\n",
    "# --- 8. Graph Construction ---\n",
    "def should_continue_to_tools(state: SocraticAgentState):\n",
    "    \"\"\"Check if we need to execute tools.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return \"execute_tools\"\n",
    "    return \"end\"\n",
    "\n",
    "def route_from_supervisor(state: SocraticAgentState):\n",
    "    \"\"\"Route based on interaction mode.\"\"\"\n",
    "    mode = state.get(\"interaction_mode\", \"general\")\n",
    "    if mode == \"mcq_active\":\n",
    "        return \"process_mcq\"\n",
    "    elif mode == \"evaluate_understanding\":\n",
    "        # If the supervisor decided to evaluate understanding, route to socratic_agent\n",
    "        # The socratic_agent's prompt will handle the probing question\n",
    "        return \"socratic_agent\"\n",
    "    return \"socratic_agent\"\n",
    "\n",
    "# New routing logic for after MCQ processing (no longer needed as process_mcq routes directly to supervisor)\n",
    "# def route_after_mcq_processing(state: SocraticAgentState):\n",
    "#     \"\"\"Route after MCQ processing, typically back to supervisor.\"\"\"\n",
    "#     # The 'next_route' key is set in process_mcq_answer\n",
    "#     return state.get(\"next_route\", \"supervisor\")\n",
    "\n",
    "\n",
    "def build_enhanced_socratic_graph():\n",
    "    \"\"\"Build the graph with supervisor and unified Socratic approach.\"\"\"\n",
    "    workflow = StateGraph(SocraticAgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"supervisor\", supervisor_node)\n",
    "    workflow.add_node(\"socratic_agent\", socratic_agent_node)\n",
    "    workflow.add_node(\"execute_tools\", execute_tools)\n",
    "    workflow.add_node(\"process_mcq\", process_mcq_answer)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        route_from_supervisor,\n",
    "        {\n",
    "            \"socratic_agent\": \"socratic_agent\",\n",
    "            \"process_mcq\": \"process_mcq\",\n",
    "            # No explicit route for evaluate_understanding here, as it routes to socratic_agent\n",
    "            # \"evaluate_understanding\": \"socratic_agent\" # This is handled by the route_from_supervisor logic\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"socratic_agent\",\n",
    "        should_continue_to_tools,\n",
    "        {\n",
    "            \"execute_tools\": \"execute_tools\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"execute_tools\", \"socratic_agent\")\n",
    "    \n",
    "    # Modified edge: process_mcq now routes back to supervisor\n",
    "    workflow.add_edge(\"process_mcq\", \"supervisor\") \n",
    "    \n",
    "    # Add memory\n",
    "    memory = MemorySaver()\n",
    "    return workflow.compile(checkpointer=memory)\n",
    "\n",
    "# --- 9. Initialize System ---\n",
    "enhanced_socratic_graph = build_enhanced_socratic_graph()\n",
    "# The memory_saver instance needs to be accessible from main.py\n",
    "memory_saver = MemorySaver() # Define memory_saver here for import in main.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socra-bot-streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
